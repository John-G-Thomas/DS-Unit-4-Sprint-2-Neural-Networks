{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "41TS0Sa0rDNx"
   },
   "source": [
    "Lambda School Data Science\n",
    "\n",
    "*Unit 4, Sprint 2, Module 4*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OomlVN7zIILb"
   },
   "source": [
    "# Neural Networks & GPUs (Prepare)\n",
    "*aka Hyperparameter Tuning*\n",
    "\n",
    "*aka Big Servers for Big Problems*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R4L97Zu5IILb"
   },
   "source": [
    "## Learning Objectives\n",
    "* <a href=\"#p1\">Part 1</a>: Describe the major hyperparemeters to tune\n",
    "* <a href=\"#p2\">Part 2</a>: Implement an experiment tracking framework\n",
    "* <a href=\"#p3\">Part 3</a>: Search the hyperparameter space using RandomSearch (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "toTqYhQvIILf"
   },
   "source": [
    "# Hyperparameter Options (Learn)\n",
    "<a id=\"p1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fI3U8thwIILg"
   },
   "source": [
    "## Overview\n",
    "\n",
    "Hyperparameter tuning is much more important with neural networks than it has been with any other models that we have considered up to this point. Other supervised learning models might have a couple of parameters, but neural networks can have dozens. These can substantially affect the accuracy of our models and although it can be a time consuming process is a necessary step when working with neural networks.\n",
    "â€‹\n",
    "Hyperparameter tuning comes with a challenge. How can we compare models specified with different hyperparameters if our model's final error metric can vary somewhat erratically? How do we avoid just getting unlucky and selecting the wrong hyperparameter? This is a problem that to a certain degree we just have to live with as we test and test again. However, we can minimize it somewhat by pairing our experiments with Cross Validation to reduce the variance of our final accuracy values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0s0o2pqBs88q"
   },
   "source": [
    "### Load MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KMcNwZDoIILh"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pm7zow5IvaTt"
   },
   "source": [
    "### Normalizing Input Data\n",
    "\n",
    "It's not 100% necessary to normalize/scale your input data before feeding it to a neural network, the network can learn the appropriate weights to deal with data of as long as it is numerically represented,  but it is recommended as it can help **make training faster** and **reduces the chances that gradient descent might get stuck in a local optimum**.\n",
    "\n",
    "<https://stackoverflow.com/questions/4674623/why-do-we-have-to-normalize-the-input-for-an-artificial-neural-network>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6vnuh5O3IILk"
   },
   "outputs": [],
   "source": [
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sYJ8t_ezHP4W"
   },
   "source": [
    "### Hyperparameter Tuning Approaches:\n",
    "\n",
    "#### 1) Babysitting AKA \"Grad Student Descent\".\n",
    "\n",
    "If you fiddled with any hyperparameters yesterday, this is basically what you did. This approach is 100% manual and is pretty common among researchers where finding that 1 exact specification that jumps your model to a level of accuracy never seen before is the difference between publishing and not publishing a paper. Of course the professors don't do this themselves, that's grunt work. This is also known as the fiddle with hyperparameters until you run out of time method.\n",
    "\n",
    "#### 2) Grid Search\n",
    "\n",
    "Grid Search is the Grad Student galaxy brain realization of: why don't I just specify all the experiments I want to run and let the computer try every possible combination of them while I go and grab lunch. This has a specific downside in that if I specify 5 hyperparameters with 5 options each then I've just created 5^5 combinations of hyperparameters to check. Which means that I have to train 3125 different versions of my model Then if I use 5-fold Cross Validation on that then my model has to run 15,525 times. This is the brute-force method of hyperparameter tuning, but it can be very profitable if done wisely. \n",
    "\n",
    "When using Grid Search here's what I suggest: don't use it to test combinations of different hyperparameters, only use it to test different specifications of **a single** hyperparameter. It's rare that combinations between different hyperparameters lead to big performance gains. You'll get 90-95% of the way there if you just Grid Search one parameter and take the best result, then retain that best result while you test another, and then retain the best specification from that while you train another. This at least makes the situation much more manageable and leads to pretty good results. \n",
    "\n",
    "#### 3) Random Search\n",
    "\n",
    "Do Grid Search for a couple of hours and you'll say to yourself - \"There's got to be a better way.\" Enter Random Search. For Random search you specify a hyperparameter space and it picks specifications from that randomly, tries them out, gives you the best results and says - That's going to have to be good enough, go home and spend time with your family. \n",
    "\n",
    "Grid Search treats every parameter as if it was equally important, but this just isn't the case, some are known to move the needle a lot more than others (we'll talk about that in a minute). Random Search allows searching to be specified along the most important parameter and experiments less along the dimensions of less important hyperparameters. The downside of Random search is that it won't find the absolute best hyperparameters, but it is much less costly to perform than Grid Search. \n",
    "\n",
    "#### 4) Bayesian Methods\n",
    "\n",
    "One thing that can make more manual methods like babysitting and gridsearch effective is that as the experimenter sees results he can then make updates to his future searches taking into account the results of past specifications. If only we could hyperparameter tune our hyperparameter tuning. Well, we kind of can. Enter Bayesian Optimization. Neural Networks are like an optimization problem within an optimization problem, and Bayesian Optimization is a search strategy that tries to take into account the results of past searches in order to improve future ones. Check out the new library `keras-tuner` for easy implementations of Bayesian methods. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HfQ7D043OMMn"
   },
   "source": [
    "## What Hyperparameters are there to test?\n",
    "\n",
    "- batch_size\n",
    "- training epochs\n",
    "- optimization algorithms\n",
    "- learning rate\n",
    "- momentum\n",
    "- activation functions\n",
    "- dropout regularization\n",
    "- number of neurons in the hidden layer\n",
    "\n",
    "There are more, but these are the most important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u844csK5IILp"
   },
   "source": [
    "## Follow Along"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mri5-kXzVKAa"
   },
   "source": [
    "## Batch Size\n",
    "\n",
    "Batch size determines how many observations the model is shown before it calculates loss/error and updates the model weights via gradient descent. You're looking for a sweet spot here where you're showing it enough observations that you have enough information to updates the weights, but not such a large batch size that you don't get a lot of weight update iterations performed in a given epoch. Feed-forward Neural Networks aren't as sensitive to bach_size as other networks, but it is still an important hyperparameter to tune. Smaller batch sizes will also take longer to train. \n",
    "\n",
    "Traditionally, batch size is set in powers of 2 starting at 32 up to 512. Keras defaults to a batch size of 32 if you do not specify it. Yann LeCun famously Twitted: \n",
    "\n",
    "> Training with large minibatches is bad for your health.\n",
    "More importantly, it's bad for your test error.\n",
    "Friends dont let friends use minibatches larger than 32.\n",
    "\n",
    "Check out this paper for more reference on his tweet. https://arxiv.org/abs/1804.07612. \n",
    "\n",
    "Check out this SO question on why batch size is typically set in powers of two: https://datascience.stackexchange.com/questions/20179/what-is-the-advantage-of-keeping-batch-size-a-power-of-2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2smXfriNAGn7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1875/1875 [==============================] - 1s 760us/step - loss: 0.3593 - accuracy: 0.8991\n",
      "Epoch 2/20\n",
      "1875/1875 [==============================] - 1s 757us/step - loss: 0.1922 - accuracy: 0.9445\n",
      "Epoch 3/20\n",
      "1875/1875 [==============================] - 1s 775us/step - loss: 0.1515 - accuracy: 0.9553\n",
      "Epoch 4/20\n",
      "1875/1875 [==============================] - 2s 809us/step - loss: 0.1265 - accuracy: 0.9632\n",
      "Epoch 5/20\n",
      "1875/1875 [==============================] - 1s 777us/step - loss: 0.1099 - accuracy: 0.9671\n",
      "Epoch 6/20\n",
      "1875/1875 [==============================] - 1s 787us/step - loss: 0.0967 - accuracy: 0.9715\n",
      "Epoch 7/20\n",
      "1875/1875 [==============================] - 2s 803us/step - loss: 0.0883 - accuracy: 0.9736\n",
      "Epoch 8/20\n",
      "1875/1875 [==============================] - 1s 798us/step - loss: 0.0812 - accuracy: 0.9758\n",
      "Epoch 9/20\n",
      "1875/1875 [==============================] - 1s 773us/step - loss: 0.0743 - accuracy: 0.9776\n",
      "Epoch 10/20\n",
      "1875/1875 [==============================] - 1s 799us/step - loss: 0.0691 - accuracy: 0.9790\n",
      "Epoch 11/20\n",
      "1875/1875 [==============================] - 1s 779us/step - loss: 0.0650 - accuracy: 0.9801\n",
      "Epoch 12/20\n",
      "1875/1875 [==============================] - 1s 760us/step - loss: 0.0600 - accuracy: 0.9807\n",
      "Epoch 13/20\n",
      "1875/1875 [==============================] - 1s 784us/step - loss: 0.0564 - accuracy: 0.9826\n",
      "Epoch 14/20\n",
      "1875/1875 [==============================] - 1s 789us/step - loss: 0.0527 - accuracy: 0.9840\n",
      "Epoch 15/20\n",
      "1875/1875 [==============================] - 1s 783us/step - loss: 0.0496 - accuracy: 0.9844\n",
      "Epoch 16/20\n",
      "1875/1875 [==============================] - 1s 773us/step - loss: 0.0477 - accuracy: 0.9853\n",
      "Epoch 17/20\n",
      "1875/1875 [==============================] - 1s 772us/step - loss: 0.0448 - accuracy: 0.9858\n",
      "Epoch 18/20\n",
      "1875/1875 [==============================] - 1s 778us/step - loss: 0.0420 - accuracy: 0.9870\n",
      "Epoch 19/20\n",
      "1875/1875 [==============================] - 1s 777us/step - loss: 0.0397 - accuracy: 0.9880\n",
      "Epoch 20/20\n",
      "1875/1875 [==============================] - 1s 774us/step - loss: 0.0377 - accuracy: 0.9880\n",
      "Best: 0.9633333325386048 using {'batch_size': 32, 'epochs': 20, 'units': 32}\n",
      "Means: 0.9633333325386048, Stdev: 0.0032710730949084767 with: {'batch_size': 32, 'epochs': 20, 'units': 32}\n",
      "Means: 0.9621166706085205, Stdev: 0.0018261899022375965 with: {'batch_size': 64, 'epochs': 20, 'units': 32}\n",
      "Means: 0.9551166772842408, Stdev: 0.0038472980352534138 with: {'batch_size': 512, 'epochs': 20, 'units': 32}\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(units=32):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units, input_dim=784, activation='relu'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=1)\n",
    "\n",
    "# define the grid search parameters\n",
    "# batch_size = [10, 20, 40, 60, 80, 100]\n",
    "# param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "# define the grid search parameters\n",
    "param_grid = {'batch_size': [32,64,512],\n",
    "              'epochs': [20],\n",
    "              'units': [32],\n",
    "              # TODO add more params\n",
    "              }\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EKcuY6OiaLfz"
   },
   "source": [
    "## Optimizer\n",
    "\n",
    "Remember that there's a different optimizers [optimizers](https://keras.io/optimizers/). At some point, take some time to read up on them a little bit. \"adam\" usually gives the best results. The thing to know about choosing an optimizer is that different optimizers have different hyperparameters like learning rate, momentum, etc. So based on the optimizer you choose you might also have to tune the learning rate and momentum of those optimizers after that. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DG3wq5iOaLig"
   },
   "source": [
    "## Learning Rate\n",
    "\n",
    "Remember that the Learning Rate is a hyperparameter that is specific to your gradient-descent based optimizer selection. A learning rate that is too high will cause divergent behavior, but a Learning Rate that is too low will fail to converge, again, you're looking for the sweet spot. I would start out tuning learning rates by orders of magnitude: [.001, .01, .1, .2, .3, .5] etc. I wouldn't go above .5, but you can try it and see what the behavior is like. \n",
    "\n",
    "Once you have narrowed it down, make the window even smaller and try it again. If after running the above specification your model reports that .1 is the best optimizer, then you should probably try things like [.05, .08, .1, .12, .15] to try and narrow it down. \n",
    "\n",
    "It can also be good to tune the number of epochs in combination with the learning rate since the number of iterations that you allow the learning rate to reach the minimum can determine if you have let it run long enough to converge to the minimum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fr2wRCQfIILu"
   },
   "outputs": [],
   "source": [
    "learning_rates = [.001, .01, .1, .2, .3, .5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gNTBUWd1aLlA"
   },
   "source": [
    "## Momentum\n",
    "\n",
    "Momentum is a hyperparameter that is more commonly associated with Stochastic Gradient Descent. SGD is a common optimizer because it's what people understand and know, but I doubt it will get you the best results, you can try hyperparameter tuning its attributes and see if you can beat the performance from adam. Momentum is a property that decides the willingness of an optimizer to overshoot the minimum. Imagine a ball rolling down one side of a bowl and then up the opposite side a little bit before settling back to the bottom. The purpose of momentum is to try and escale local minima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xnEG-bCJaLnZ"
   },
   "source": [
    "## Activation Functions\n",
    "\n",
    "We've talked about this a little bit, typically you'l want to use ReLU for hidden layers and either Sigmoid, or Softmax for output layers of binary and multi-class classification implementations respectively, but try other activation functions and see if you can get any better results with sigmoid or tanh or something. There are a lot of activation functions that we haven't really talked about. Maybe you'll get good results with them. Maybe you won't. :) <https://keras.io/activations/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oul9sPq-dU-h"
   },
   "source": [
    "## Network Weight Initialization\n",
    "\n",
    "You saw how big of an effect the way that we initialize our network's weights can have on our results. There are **a lot** of what are called initialization modes. I don't understand all of them, but they can have a big affect on your model's initial accuracy. Your model will get further with less epochs if you initialize it with weights that are well suited to the problem you're trying to solve.\n",
    "\n",
    "`init_mode = ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bqtEuxeQaLqE"
   },
   "source": [
    "## Dropout Regularization and the Weight Constraint\n",
    "\n",
    "the Dropout Regularization value is a percentage of neurons that you want to be randomly deactivated during training. The weight constraint is a second regularization parameter that works in tandem with dropout regularization. You should tune these two values at the same time. \n",
    "\n",
    "Using dropout on visible vs hidden layers might have a different effect. Using dropout on hidden layers might not have any effect while using dropout on hidden layers might have a substantial effect. You don't necessarily need to turn use dropout unless you see that your model has overfitting and generalizability problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P2c5Cv6oaLtO"
   },
   "source": [
    "## Neurons in Hidden Layer \n",
    "\n",
    "Remember that when we only had a single perceptron our model was only able to fit to linearly separable data, but as we have added layers and nodes to those layers our network has become a powerhouse of fitting nonlinearity in data. The larger the network and the more nodes generally the stronger the network's capacity to fit nonlinear patterns in data. The more nodes and layers the longer it will take to train a network, and higher the probability of overfitting. The larger your network gets the more you'll need dropout regularization or other regularization techniques to keep it in check. \n",
    "\n",
    "Typically depth (more layers) is more important than width (more nodes) for neural networks. This is part of why Deep Learning is so highly touted. Certain deep learning architectures have truly been huge breakthroughs for certain machine learning tasks. \n",
    "\n",
    "You might borrow ideas from other network architectures. For example if I was doing image recognition and I wasn't taking cues from state of the art architectures like resnet, alexnet, googlenet, etc. Then I'm probably going to have to do a lot more experimentation on my own before I find something that works.\n",
    "\n",
    "There are some heuristics, but I am highly skeptical of them. I think you're better off experimenting on your own and forming your own intuition for these kinds of problems. \n",
    "\n",
    "- https://machinelearningmastery.com/how-to-configure-the-number-of-layers-and-nodes-in-a-neural-network/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YrcXQxBUIILy"
   },
   "source": [
    "## Challenge\n",
    "You will be expected to tune several hyperparameters in today's module project. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UIRKRf5WIILy"
   },
   "source": [
    "# Experiment Tracking Framework (Learn)\n",
    "<a id=\"p2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YTHOpGLpIILz"
   },
   "source": [
    "## Overview\n",
    "\n",
    "You will notice quickly that managing the results of all the experiments you are running becomes challenging. Which set of parameters did the best? Are my results today different than my results yesterday? Although we use Ipython Notebooks to work, the format is not well suited to logging experimental results. Enter experiment tracking frameworks like [Comet.ml](https://comet.ml) and [Weights and Biases](https://wandb.ai/), and TensorBoard's Hyperparameter Dashboard. \n",
    "\n",
    "Those tools will help you track your experiments, store the results, and the code associated with those experiments. Experimental results can also be readily visualized to see changes in performance across any metric you care about. Data is sent to the tool as each epoch is completed, so you can also see if your model is converging. Let's check out TensorBoard today. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L-Yhf0eCIILz"
   },
   "source": [
    "## Follow Along"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ncbmt-nAIILz"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XDfQi_1SPGVQ"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ieDkdH3kPR75"
   },
   "source": [
    "### 1. Create Experiment Configuration\n",
    "We are going to experiment with: \n",
    "* Number of units in the first dense layer\n",
    "* Learning Rate\n",
    "* Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m9DCVHodPkgH"
   },
   "outputs": [],
   "source": [
    "HP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([16,32]))\n",
    "HP_LEARNING_RATE = hp.HParam('learning_rate', hp.RealInterval(0.001,.01))\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd']))\n",
    "\n",
    "METRIC_ACCURACY = 'accuracy'\n",
    "\n",
    "with tf.summary.create_file_writer('logs/hparam_tuning').as_default():\n",
    "  hp.hparams_config(\n",
    "      hparams=[HP_NUM_UNITS, HP_LEARNING_RATE, HP_OPTIMIZER],\n",
    "      metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')]\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2rhRMwfLQuGJ"
   },
   "source": [
    "### 2. Adapt Model Function with HParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nrjKyNiMQ0Dn"
   },
   "outputs": [],
   "source": [
    "def train_test_model(hparams):\n",
    "  \n",
    "  model = tf.keras.Sequential(\n",
    "      [tf.keras.layers.Dense(hparams[HP_NUM_UNITS], activation='relu'),\n",
    "       tf.keras.layers.Dense(10, activation='softmax')      \n",
    "  ])\n",
    "\n",
    "  opt_name = hparams[HP_OPTIMIZER]\n",
    "  lr = hparams[HP_LEARNING_RATE]\n",
    "\n",
    "  if opt_name == 'adam':\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "  elif opt_name == 'sgd':\n",
    "    opt = tf.keras.optimizers.SGD(learning_rate=lr)\n",
    "  else:\n",
    "    raise ValueError(f'Unexpected optimizer: {opt_name}')\n",
    "\n",
    "  model.compile(\n",
    "      optimizer=opt,\n",
    "      loss='sparse_categorical_crossentropy',\n",
    "      metrics=['accuracy']\n",
    "  )\n",
    "\n",
    "  model.fit(X_train, y_train, epochs=5)\n",
    "  _, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "  # Python convention: if a variable doesn't need a name, give it _\n",
    "  # ten_ones = [1 for _ in range(10)]\n",
    "\n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6bixdT4BSFvx"
   },
   "source": [
    "For each run, log an hparams summary with the hyperparameters and final accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ajJRJSCDTLOA"
   },
   "outputs": [],
   "source": [
    "def run(run_dir, hparams):\n",
    "  with tf.summary.create_file_writer(run_dir).as_default():\n",
    "    hp.hparams(hparams)  # record the values used in this trial\n",
    "    accuracy = train_test_model(hparams)\n",
    "    tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qWC0FMCjTQk1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting trial: run-0\n",
      "{'num_units': 16, 'learning_rate': 0.001, 'optimizer': 'adam'}\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 1s 752us/step - loss: 0.4448 - accuracy: 0.8763\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 1s 721us/step - loss: 0.2603 - accuracy: 0.9251\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 1s 787us/step - loss: 0.2249 - accuracy: 0.9362\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 1s 738us/step - loss: 0.2022 - accuracy: 0.9416\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 1s 755us/step - loss: 0.1869 - accuracy: 0.9461\n",
      "313/313 [==============================] - 0s 562us/step - loss: 0.1916 - accuracy: 0.9432\n",
      "--- Starting trial: run-1\n",
      "{'num_units': 16, 'learning_rate': 0.001, 'optimizer': 'sgd'}\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 1s 659us/step - loss: 1.8407 - accuracy: 0.4470\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 1s 660us/step - loss: 1.1526 - accuracy: 0.7381\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 1s 627us/step - loss: 0.8219 - accuracy: 0.8130\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 1s 628us/step - loss: 0.6599 - accuracy: 0.8389\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 1s 634us/step - loss: 0.5711 - accuracy: 0.8547\n",
      "313/313 [==============================] - 0s 620us/step - loss: 0.5132 - accuracy: 0.8700\n",
      "--- Starting trial: run-2\n",
      "{'num_units': 16, 'learning_rate': 0.01, 'optimizer': 'adam'}\n",
      "Epoch 1/5\n",
      "   1/1875 [..............................] - ETA: 0s - loss: 2.3563 - accuracy: 0.0312WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0010s). Check your callbacks.\n",
      "1875/1875 [==============================] - 1s 674us/step - loss: 0.3284 - accuracy: 0.9051\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 1s 666us/step - loss: 0.2348 - accuracy: 0.9334\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 1s 673us/step - loss: 0.2134 - accuracy: 0.9397\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 1s 663us/step - loss: 0.2056 - accuracy: 0.9413\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 1s 687us/step - loss: 0.1951 - accuracy: 0.9442\n",
      "313/313 [==============================] - 0s 492us/step - loss: 0.2465 - accuracy: 0.9321\n",
      "--- Starting trial: run-3\n",
      "{'num_units': 16, 'learning_rate': 0.01, 'optimizer': 'sgd'}\n",
      "Epoch 1/5\n",
      "   1/1875 [..............................] - ETA: 0s - loss: 2.3413 - accuracy: 0.1875WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0010s). Check your callbacks.\n",
      "1875/1875 [==============================] - 1s 595us/step - loss: 0.8382 - accuracy: 0.7683\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 1s 588us/step - loss: 0.3702 - accuracy: 0.8964\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 1s 585us/step - loss: 0.3184 - accuracy: 0.9096\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 1s 575us/step - loss: 0.2925 - accuracy: 0.9162\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 1s 575us/step - loss: 0.2748 - accuracy: 0.9215\n",
      "313/313 [==============================] - 0s 518us/step - loss: 0.2618 - accuracy: 0.9242\n",
      "--- Starting trial: run-4\n",
      "{'num_units': 32, 'learning_rate': 0.001, 'optimizer': 'adam'}\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 1s 715us/step - loss: 0.3623 - accuracy: 0.9005\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 1s 676us/step - loss: 0.1946 - accuracy: 0.9442\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 1s 661us/step - loss: 0.1538 - accuracy: 0.9546\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 1s 656us/step - loss: 0.1306 - accuracy: 0.9615\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 1s 661us/step - loss: 0.1149 - accuracy: 0.9664\n",
      "313/313 [==============================] - 0s 479us/step - loss: 0.1324 - accuracy: 0.9608\n",
      "--- Starting trial: run-5\n",
      "{'num_units': 32, 'learning_rate': 0.001, 'optimizer': 'sgd'}\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 1s 592us/step - loss: 1.7934 - accuracy: 0.4845\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 1s 601us/step - loss: 1.0793 - accuracy: 0.7631\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 1s 648us/step - loss: 0.7765 - accuracy: 0.8260\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 1s 586us/step - loss: 0.6307 - accuracy: 0.8494\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 1s 585us/step - loss: 0.5497 - accuracy: 0.8642\n",
      "  1/313 [..............................] - ETA: 0s - loss: 0.4146 - accuracy: 0.9375WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_end` time: 0.0010s). Check your callbacks.\n",
      "313/313 [==============================] - 0s 473us/step - loss: 0.4978 - accuracy: 0.8747\n",
      "--- Starting trial: run-6\n",
      "{'num_units': 32, 'learning_rate': 0.01, 'optimizer': 'adam'}\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 1s 728us/step - loss: 0.2737 - accuracy: 0.9189\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 2s 901us/step - loss: 0.1855 - accuracy: 0.9464\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 1s 788us/step - loss: 0.1660 - accuracy: 0.9528\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 2s 805us/step - loss: 0.1551 - accuracy: 0.9567\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 2s 865us/step - loss: 0.1456 - accuracy: 0.9592\n",
      "  1/313 [..............................] - ETA: 0s - loss: 0.0457 - accuracy: 0.9688WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_end` time: 0.0010s). Check your callbacks.\n",
      "313/313 [==============================] - 0s 623us/step - loss: 0.1992 - accuracy: 0.9537\n",
      "--- Starting trial: run-7\n",
      "{'num_units': 32, 'learning_rate': 0.01, 'optimizer': 'sgd'}\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 1s 745us/step - loss: 0.7113 - accuracy: 0.8152\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 1s 619us/step - loss: 0.3587 - accuracy: 0.9000\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 1s 597us/step - loss: 0.3121 - accuracy: 0.9110\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 1s 594us/step - loss: 0.2866 - accuracy: 0.9184\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 1s 651us/step - loss: 0.2682 - accuracy: 0.9243\n",
      "313/313 [==============================] - 0s 527us/step - loss: 0.2482 - accuracy: 0.9302\n"
     ]
    }
   ],
   "source": [
    "session_num = 0\n",
    "\n",
    "# Basically a grid search\n",
    "for num_units in HP_NUM_UNITS.domain.values:\n",
    "  for learning_rate in (HP_LEARNING_RATE.domain.min_value,\n",
    "                        HP_LEARNING_RATE.domain.max_value):\n",
    "    for optimizer in HP_OPTIMIZER.domain.values:\n",
    "      hparams = {\n",
    "          HP_NUM_UNITS: num_units,\n",
    "          HP_LEARNING_RATE: learning_rate,\n",
    "          HP_OPTIMIZER: optimizer\n",
    "      }\n",
    "\n",
    "      run_name = f'run-{session_num}'\n",
    "      print(f'--- Starting trial: {run_name}')\n",
    "      print({param.name: hparams[param] for param in hparams})\n",
    "      run('logs/hparams_tuning/' + run_name, hparams)\n",
    "      session_num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jdWHqy70YVFY"
   },
   "source": [
    "### 4. Visualize the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sRX1kUChYUgX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Timed out waiting for TensorBoard to start. It may still be running as pid 18796."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/hparams_tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dFovWNBmIIL4"
   },
   "source": [
    "### Your Turn\n",
    "\n",
    "Pick a few hyparameters that we *have not* tuned. Using the same code above, try changing a few parameters you're interested in and submitting the results to weights & biases. :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7584VUQTYst8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dFwQuQOGIIL6"
   },
   "source": [
    "## Challenge\n",
    "\n",
    "You will be expected to use Weights & Biases to try to tune your model during your module assignment today. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TUGXzf6TIIL7"
   },
   "source": [
    "# Hyperparameters with RandomSearchCV (Learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sBRu7-zZIIL7"
   },
   "source": [
    "## Overview\n",
    "\n",
    "Basically `GridSearchCV` takes forever. You'll want to adopt a slightly more sophiscated strategy.\n",
    "\n",
    "Let's also take a look at an alternative with Keras-Tuner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v-tDdeWYIIL7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras-tuner\n",
      "  Downloading keras-tuner-1.0.1.tar.gz (54 kB)\n",
      "Requirement already satisfied: future in c:\\users\\johnt\\anaconda3\\envs\\u4-s1-nlp\\lib\\site-packages (from keras-tuner) (0.18.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\johnt\\anaconda3\\envs\\u4-s1-nlp\\lib\\site-packages (from keras-tuner) (1.19.1)\n",
      "Collecting tabulate\n",
      "  Downloading tabulate-0.8.7-py3-none-any.whl (24 kB)\n",
      "Collecting terminaltables\n",
      "  Downloading terminaltables-3.1.0.tar.gz (12 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\johnt\\anaconda3\\envs\\u4-s1-nlp\\lib\\site-packages (from keras-tuner) (0.4.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\johnt\\anaconda3\\envs\\u4-s1-nlp\\lib\\site-packages (from keras-tuner) (4.48.2)\n",
      "Requirement already satisfied: requests in c:\\users\\johnt\\anaconda3\\envs\\u4-s1-nlp\\lib\\site-packages (from keras-tuner) (2.24.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\johnt\\anaconda3\\envs\\u4-s1-nlp\\lib\\site-packages (from keras-tuner) (1.4.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\johnt\\anaconda3\\envs\\u4-s1-nlp\\lib\\site-packages (from keras-tuner) (0.22.2)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\johnt\\anaconda3\\envs\\u4-s1-nlp\\lib\\site-packages (from requests->keras-tuner) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\johnt\\anaconda3\\envs\\u4-s1-nlp\\lib\\site-packages (from requests->keras-tuner) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\johnt\\anaconda3\\envs\\u4-s1-nlp\\lib\\site-packages (from requests->keras-tuner) (1.25.10)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\johnt\\anaconda3\\envs\\u4-s1-nlp\\lib\\site-packages (from requests->keras-tuner) (2.10)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\johnt\\anaconda3\\envs\\u4-s1-nlp\\lib\\site-packages (from scikit-learn->keras-tuner) (0.16.0)\n",
      "Building wheels for collected packages: keras-tuner, terminaltables\n",
      "  Building wheel for keras-tuner (setup.py): started\n",
      "  Building wheel for keras-tuner (setup.py): finished with status 'done'\n",
      "  Created wheel for keras-tuner: filename=keras_tuner-1.0.1-py3-none-any.whl size=73206 sha256=5255fc058d1944a5978ef4b89276dfa5e0668994a5511cd6a8e9cdf228d0145c\n",
      "  Stored in directory: c:\\users\\johnt\\appdata\\local\\pip\\cache\\wheels\\0b\\cf\\2f\\1a1749d3a3650fac3305a8d7f9237b6de7c41068e2f8520ca2\n",
      "  Building wheel for terminaltables (setup.py): started\n",
      "  Building wheel for terminaltables (setup.py): finished with status 'done'\n",
      "  Created wheel for terminaltables: filename=terminaltables-3.1.0-py3-none-any.whl size=15359 sha256=3205bc317ff56d9fe251c22112d82e33721e8b7abeddd4ad676d227a2bb94618\n",
      "  Stored in directory: c:\\users\\johnt\\appdata\\local\\pip\\cache\\wheels\\ba\\ad\\c8\\2d98360791161cd3db6daf6b5e730f34021fc9367d5879f497\n",
      "Successfully built keras-tuner terminaltables\n",
      "Installing collected packages: tabulate, terminaltables, keras-tuner\n",
      "Successfully installed keras-tuner-1.0.1 tabulate-0.8.7 terminaltables-3.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NO5PEiGcIIL9"
   },
   "source": [
    "## Follow Along"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YKxI2vx4IIL9"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from kerastuner.tuners import RandomSearch\n",
    "\n",
    "\"\"\"\n",
    "This model Tunes:\n",
    "- Number of Neurons in the Hidden Layer\n",
    "- Learning Rate in Adam\n",
    "\n",
    "\"\"\"\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from kerastuner.tuners import RandomSearch\n",
    "\n",
    "\"\"\"\n",
    "This model Tunes:\n",
    "- Number of Neurons in the Hidden Layer\n",
    "- Learning Rate in Adam\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Dense(units=hp.Int('units',\n",
    "                                        min_value=32,\n",
    "                                        max_value=512,\n",
    "                                        step=32),\n",
    "                           activation='relu'))\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(\n",
    "            hp.Choice('learning_rate',\n",
    "                      values=[1e-2, 1e-3, 1e-4])),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jBCfWIweIIL_"
   },
   "outputs": [],
   "source": [
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=5,\n",
    "    executions_per_trial=3,\n",
    "    directory='./keras-tuner-trial',\n",
    "    project_name='helloworld')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tpGTo9bpIIMB"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Search space summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Default search space size: 2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">units (Int)</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-default: None</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-max_value: 512</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-min_value: 32</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-sampling: None</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-step: 32</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">learning_rate (Choice)</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-default: 0.01</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-ordered: True</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-values: [0.01, 0.001, 0.0001]</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SghuI4TqIIME"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 2.3412 - accuracy: 0.12 - ETA: 1s - loss: 1.2837 - accuracy: 0.65 - ETA: 1s - loss: 0.9161 - accuracy: 0.75 - ETA: 1s - loss: 0.7483 - accuracy: 0.79 - ETA: 1s - loss: 0.6674 - accuracy: 0.81 - ETA: 1s - loss: 0.6150 - accuracy: 0.83 - ETA: 1s - loss: 0.5808 - accuracy: 0.83 - ETA: 1s - loss: 0.5382 - accuracy: 0.84 - ETA: 1s - loss: 0.5040 - accuracy: 0.85 - ETA: 1s - loss: 0.4739 - accuracy: 0.86 - ETA: 1s - loss: 0.4523 - accuracy: 0.87 - ETA: 1s - loss: 0.4374 - accuracy: 0.87 - ETA: 1s - loss: 0.4211 - accuracy: 0.88 - ETA: 1s - loss: 0.4119 - accuracy: 0.88 - ETA: 1s - loss: 0.4033 - accuracy: 0.88 - ETA: 1s - loss: 0.3949 - accuracy: 0.88 - ETA: 1s - loss: 0.3840 - accuracy: 0.89 - ETA: 1s - loss: 0.3738 - accuracy: 0.89 - ETA: 0s - loss: 0.3642 - accuracy: 0.89 - ETA: 0s - loss: 0.3588 - accuracy: 0.89 - ETA: 0s - loss: 0.3532 - accuracy: 0.90 - ETA: 0s - loss: 0.3450 - accuracy: 0.90 - ETA: 0s - loss: 0.3384 - accuracy: 0.90 - ETA: 0s - loss: 0.3329 - accuracy: 0.90 - ETA: 0s - loss: 0.3264 - accuracy: 0.90 - ETA: 0s - loss: 0.3227 - accuracy: 0.90 - ETA: 0s - loss: 0.3166 - accuracy: 0.91 - ETA: 0s - loss: 0.3121 - accuracy: 0.91 - ETA: 0s - loss: 0.3072 - accuracy: 0.91 - ETA: 0s - loss: 0.3023 - accuracy: 0.91 - ETA: 0s - loss: 0.2979 - accuracy: 0.91 - ETA: 0s - loss: 0.2938 - accuracy: 0.91 - ETA: 0s - loss: 0.2892 - accuracy: 0.91 - ETA: 0s - loss: 0.2857 - accuracy: 0.91 - ETA: 0s - loss: 0.2825 - accuracy: 0.91 - ETA: 0s - loss: 0.2783 - accuracy: 0.92 - 2s 1ms/step - loss: 0.2752 - accuracy: 0.9216 - val_loss: 0.1451 - val_accuracy: 0.9578\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.1032 - accuracy: 0.96 - ETA: 1s - loss: 0.1458 - accuracy: 0.95 - ETA: 1s - loss: 0.1471 - accuracy: 0.95 - ETA: 1s - loss: 0.1455 - accuracy: 0.95 - ETA: 1s - loss: 0.1422 - accuracy: 0.95 - ETA: 1s - loss: 0.1452 - accuracy: 0.95 - ETA: 1s - loss: 0.1465 - accuracy: 0.95 - ETA: 1s - loss: 0.1467 - accuracy: 0.95 - ETA: 1s - loss: 0.1450 - accuracy: 0.95 - ETA: 1s - loss: 0.1411 - accuracy: 0.96 - ETA: 1s - loss: 0.1384 - accuracy: 0.96 - ETA: 1s - loss: 0.1393 - accuracy: 0.96 - ETA: 1s - loss: 0.1401 - accuracy: 0.96 - ETA: 1s - loss: 0.1388 - accuracy: 0.96 - ETA: 0s - loss: 0.1386 - accuracy: 0.96 - ETA: 0s - loss: 0.1383 - accuracy: 0.96 - ETA: 0s - loss: 0.1373 - accuracy: 0.96 - ETA: 0s - loss: 0.1370 - accuracy: 0.96 - ETA: 0s - loss: 0.1359 - accuracy: 0.96 - ETA: 0s - loss: 0.1355 - accuracy: 0.96 - ETA: 0s - loss: 0.1349 - accuracy: 0.96 - ETA: 0s - loss: 0.1331 - accuracy: 0.96 - ETA: 0s - loss: 0.1320 - accuracy: 0.96 - ETA: 0s - loss: 0.1316 - accuracy: 0.96 - ETA: 0s - loss: 0.1310 - accuracy: 0.96 - ETA: 0s - loss: 0.1303 - accuracy: 0.96 - ETA: 0s - loss: 0.1295 - accuracy: 0.96 - ETA: 0s - loss: 0.1290 - accuracy: 0.96 - ETA: 0s - loss: 0.1281 - accuracy: 0.96 - ETA: 0s - loss: 0.1273 - accuracy: 0.96 - ETA: 0s - loss: 0.1269 - accuracy: 0.96 - ETA: 0s - loss: 0.1264 - accuracy: 0.96 - ETA: 0s - loss: 0.1267 - accuracy: 0.96 - ETA: 0s - loss: 0.1261 - accuracy: 0.96 - 2s 1ms/step - loss: 0.1262 - accuracy: 0.9636 - val_loss: 0.1121 - val_accuracy: 0.9649\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.1754 - accuracy: 0.87 - ETA: 1s - loss: 0.0809 - accuracy: 0.97 - ETA: 1s - loss: 0.0895 - accuracy: 0.97 - ETA: 1s - loss: 0.0932 - accuracy: 0.97 - ETA: 1s - loss: 0.0882 - accuracy: 0.97 - ETA: 1s - loss: 0.0878 - accuracy: 0.97 - ETA: 1s - loss: 0.0880 - accuracy: 0.97 - ETA: 1s - loss: 0.0871 - accuracy: 0.97 - ETA: 1s - loss: 0.0876 - accuracy: 0.97 - ETA: 1s - loss: 0.0877 - accuracy: 0.97 - ETA: 1s - loss: 0.0890 - accuracy: 0.97 - ETA: 1s - loss: 0.0874 - accuracy: 0.97 - ETA: 1s - loss: 0.0868 - accuracy: 0.97 - ETA: 1s - loss: 0.0861 - accuracy: 0.97 - ETA: 1s - loss: 0.0847 - accuracy: 0.97 - ETA: 0s - loss: 0.0837 - accuracy: 0.97 - ETA: 0s - loss: 0.0844 - accuracy: 0.97 - ETA: 0s - loss: 0.0843 - accuracy: 0.97 - ETA: 0s - loss: 0.0840 - accuracy: 0.97 - ETA: 0s - loss: 0.0842 - accuracy: 0.97 - ETA: 0s - loss: 0.0848 - accuracy: 0.97 - ETA: 0s - loss: 0.0860 - accuracy: 0.97 - ETA: 0s - loss: 0.0862 - accuracy: 0.97 - ETA: 0s - loss: 0.0872 - accuracy: 0.97 - ETA: 0s - loss: 0.0879 - accuracy: 0.97 - ETA: 0s - loss: 0.0871 - accuracy: 0.97 - ETA: 0s - loss: 0.0876 - accuracy: 0.97 - ETA: 0s - loss: 0.0876 - accuracy: 0.97 - ETA: 0s - loss: 0.0875 - accuracy: 0.97 - ETA: 0s - loss: 0.0879 - accuracy: 0.97 - ETA: 0s - loss: 0.0873 - accuracy: 0.97 - ETA: 0s - loss: 0.0874 - accuracy: 0.97 - ETA: 0s - loss: 0.0881 - accuracy: 0.97 - ETA: 0s - loss: 0.0881 - accuracy: 0.97 - ETA: 0s - loss: 0.0874 - accuracy: 0.97 - ETA: 0s - loss: 0.0873 - accuracy: 0.97 - ETA: 0s - loss: 0.0874 - accuracy: 0.97 - 2s 1ms/step - loss: 0.0876 - accuracy: 0.9741 - val_loss: 0.0920 - val_accuracy: 0.9727\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.1151 - accuracy: 0.93 - ETA: 1s - loss: 0.0714 - accuracy: 0.97 - ETA: 1s - loss: 0.0647 - accuracy: 0.97 - ETA: 1s - loss: 0.0651 - accuracy: 0.97 - ETA: 1s - loss: 0.0655 - accuracy: 0.98 - ETA: 1s - loss: 0.0634 - accuracy: 0.98 - ETA: 1s - loss: 0.0634 - accuracy: 0.98 - ETA: 1s - loss: 0.0627 - accuracy: 0.98 - ETA: 1s - loss: 0.0637 - accuracy: 0.98 - ETA: 1s - loss: 0.0650 - accuracy: 0.98 - ETA: 1s - loss: 0.0652 - accuracy: 0.98 - ETA: 1s - loss: 0.0652 - accuracy: 0.98 - ETA: 1s - loss: 0.0661 - accuracy: 0.98 - ETA: 1s - loss: 0.0651 - accuracy: 0.98 - ETA: 0s - loss: 0.0653 - accuracy: 0.98 - ETA: 0s - loss: 0.0659 - accuracy: 0.98 - ETA: 0s - loss: 0.0655 - accuracy: 0.98 - ETA: 0s - loss: 0.0661 - accuracy: 0.98 - ETA: 0s - loss: 0.0661 - accuracy: 0.98 - ETA: 0s - loss: 0.0655 - accuracy: 0.98 - ETA: 0s - loss: 0.0664 - accuracy: 0.98 - ETA: 0s - loss: 0.0667 - accuracy: 0.97 - ETA: 0s - loss: 0.0670 - accuracy: 0.97 - ETA: 0s - loss: 0.0675 - accuracy: 0.97 - ETA: 0s - loss: 0.0673 - accuracy: 0.97 - ETA: 0s - loss: 0.0673 - accuracy: 0.97 - ETA: 0s - loss: 0.0670 - accuracy: 0.97 - ETA: 0s - loss: 0.0667 - accuracy: 0.97 - ETA: 0s - loss: 0.0668 - accuracy: 0.97 - ETA: 0s - loss: 0.0676 - accuracy: 0.97 - ETA: 0s - loss: 0.0683 - accuracy: 0.97 - ETA: 0s - loss: 0.0680 - accuracy: 0.97 - ETA: 0s - loss: 0.0676 - accuracy: 0.97 - ETA: 0s - loss: 0.0673 - accuracy: 0.97 - ETA: 0s - loss: 0.0674 - accuracy: 0.97 - 2s 1ms/step - loss: 0.0674 - accuracy: 0.9796 - val_loss: 0.0820 - val_accuracy: 0.9739\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0124 - accuracy: 1.00 - ETA: 1s - loss: 0.0444 - accuracy: 0.98 - ETA: 1s - loss: 0.0450 - accuracy: 0.98 - ETA: 1s - loss: 0.0458 - accuracy: 0.98 - ETA: 1s - loss: 0.0480 - accuracy: 0.98 - ETA: 1s - loss: 0.0469 - accuracy: 0.98 - ETA: 1s - loss: 0.0496 - accuracy: 0.98 - ETA: 1s - loss: 0.0479 - accuracy: 0.98 - ETA: 1s - loss: 0.0476 - accuracy: 0.98 - ETA: 1s - loss: 0.0486 - accuracy: 0.98 - ETA: 1s - loss: 0.0489 - accuracy: 0.98 - ETA: 1s - loss: 0.0496 - accuracy: 0.98 - ETA: 1s - loss: 0.0494 - accuracy: 0.98 - ETA: 1s - loss: 0.0499 - accuracy: 0.98 - ETA: 0s - loss: 0.0501 - accuracy: 0.98 - ETA: 0s - loss: 0.0500 - accuracy: 0.98 - ETA: 0s - loss: 0.0504 - accuracy: 0.98 - ETA: 0s - loss: 0.0511 - accuracy: 0.98 - ETA: 0s - loss: 0.0523 - accuracy: 0.98 - ETA: 0s - loss: 0.0524 - accuracy: 0.98 - ETA: 0s - loss: 0.0527 - accuracy: 0.98 - ETA: 0s - loss: 0.0525 - accuracy: 0.98 - ETA: 0s - loss: 0.0524 - accuracy: 0.98 - ETA: 0s - loss: 0.0527 - accuracy: 0.98 - ETA: 0s - loss: 0.0524 - accuracy: 0.98 - ETA: 0s - loss: 0.0519 - accuracy: 0.98 - ETA: 0s - loss: 0.0525 - accuracy: 0.98 - ETA: 0s - loss: 0.0529 - accuracy: 0.98 - ETA: 0s - loss: 0.0529 - accuracy: 0.98 - ETA: 0s - loss: 0.0529 - accuracy: 0.98 - ETA: 0s - loss: 0.0529 - accuracy: 0.98 - ETA: 0s - loss: 0.0530 - accuracy: 0.98 - ETA: 0s - loss: 0.0527 - accuracy: 0.98 - ETA: 0s - loss: 0.0527 - accuracy: 0.98 - ETA: 0s - loss: 0.0528 - accuracy: 0.98 - 2s 1ms/step - loss: 0.0533 - accuracy: 0.9842 - val_loss: 0.0862 - val_accuracy: 0.9724\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 2.4020 - accuracy: 0.03 - ETA: 2s - loss: 1.5249 - accuracy: 0.58 - ETA: 2s - loss: 1.1399 - accuracy: 0.68 - ETA: 2s - loss: 0.8993 - accuracy: 0.75 - ETA: 2s - loss: 0.7638 - accuracy: 0.78 - ETA: 2s - loss: 0.6946 - accuracy: 0.80 - ETA: 2s - loss: 0.6496 - accuracy: 0.82 - ETA: 2s - loss: 0.6135 - accuracy: 0.83 - ETA: 2s - loss: 0.5748 - accuracy: 0.83 - ETA: 2s - loss: 0.5305 - accuracy: 0.85 - ETA: 1s - loss: 0.5081 - accuracy: 0.85 - ETA: 1s - loss: 0.4897 - accuracy: 0.86 - ETA: 1s - loss: 0.4706 - accuracy: 0.86 - ETA: 1s - loss: 0.4527 - accuracy: 0.87 - ETA: 1s - loss: 0.4373 - accuracy: 0.87 - ETA: 1s - loss: 0.4249 - accuracy: 0.88 - ETA: 1s - loss: 0.4126 - accuracy: 0.88 - ETA: 1s - loss: 0.4024 - accuracy: 0.88 - ETA: 1s - loss: 0.3938 - accuracy: 0.88 - ETA: 1s - loss: 0.3866 - accuracy: 0.89 - ETA: 1s - loss: 0.3778 - accuracy: 0.89 - ETA: 1s - loss: 0.3688 - accuracy: 0.89 - ETA: 1s - loss: 0.3613 - accuracy: 0.89 - ETA: 0s - loss: 0.3541 - accuracy: 0.89 - ETA: 0s - loss: 0.3476 - accuracy: 0.90 - ETA: 0s - loss: 0.3430 - accuracy: 0.90 - ETA: 0s - loss: 0.3369 - accuracy: 0.90 - ETA: 0s - loss: 0.3345 - accuracy: 0.90 - ETA: 0s - loss: 0.3316 - accuracy: 0.90 - ETA: 0s - loss: 0.3286 - accuracy: 0.90 - ETA: 0s - loss: 0.3234 - accuracy: 0.90 - ETA: 0s - loss: 0.3212 - accuracy: 0.90 - ETA: 0s - loss: 0.3172 - accuracy: 0.90 - ETA: 0s - loss: 0.3123 - accuracy: 0.91 - ETA: 0s - loss: 0.3062 - accuracy: 0.91 - ETA: 0s - loss: 0.3012 - accuracy: 0.91 - ETA: 0s - loss: 0.2977 - accuracy: 0.91 - ETA: 0s - loss: 0.2941 - accuracy: 0.91 - ETA: 0s - loss: 0.2910 - accuracy: 0.91 - ETA: 0s - loss: 0.2865 - accuracy: 0.91 - ETA: 0s - loss: 0.2824 - accuracy: 0.91 - ETA: 0s - loss: 0.2790 - accuracy: 0.92 - 2s 1ms/step - loss: 0.2764 - accuracy: 0.9210 - val_loss: 0.1480 - val_accuracy: 0.9563\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.1412 - accuracy: 0.96 - ETA: 1s - loss: 0.1132 - accuracy: 0.96 - ETA: 1s - loss: 0.1267 - accuracy: 0.96 - ETA: 1s - loss: 0.1302 - accuracy: 0.96 - ETA: 1s - loss: 0.1379 - accuracy: 0.95 - ETA: 1s - loss: 0.1394 - accuracy: 0.95 - ETA: 1s - loss: 0.1385 - accuracy: 0.95 - ETA: 1s - loss: 0.1399 - accuracy: 0.95 - ETA: 1s - loss: 0.1380 - accuracy: 0.95 - ETA: 1s - loss: 0.1373 - accuracy: 0.95 - ETA: 1s - loss: 0.1338 - accuracy: 0.96 - ETA: 1s - loss: 0.1338 - accuracy: 0.96 - ETA: 1s - loss: 0.1333 - accuracy: 0.96 - ETA: 1s - loss: 0.1318 - accuracy: 0.96 - ETA: 1s - loss: 0.1333 - accuracy: 0.96 - ETA: 1s - loss: 0.1339 - accuracy: 0.96 - ETA: 1s - loss: 0.1342 - accuracy: 0.96 - ETA: 1s - loss: 0.1354 - accuracy: 0.96 - ETA: 1s - loss: 0.1341 - accuracy: 0.96 - ETA: 1s - loss: 0.1326 - accuracy: 0.96 - ETA: 0s - loss: 0.1322 - accuracy: 0.96 - ETA: 0s - loss: 0.1315 - accuracy: 0.96 - ETA: 0s - loss: 0.1299 - accuracy: 0.96 - ETA: 0s - loss: 0.1292 - accuracy: 0.96 - ETA: 0s - loss: 0.1291 - accuracy: 0.96 - ETA: 0s - loss: 0.1293 - accuracy: 0.96 - ETA: 0s - loss: 0.1290 - accuracy: 0.96 - ETA: 0s - loss: 0.1287 - accuracy: 0.96 - ETA: 0s - loss: 0.1280 - accuracy: 0.96 - ETA: 0s - loss: 0.1280 - accuracy: 0.96 - ETA: 0s - loss: 0.1270 - accuracy: 0.96 - ETA: 0s - loss: 0.1258 - accuracy: 0.96 - ETA: 0s - loss: 0.1251 - accuracy: 0.96 - ETA: 0s - loss: 0.1256 - accuracy: 0.96 - ETA: 0s - loss: 0.1253 - accuracy: 0.96 - ETA: 0s - loss: 0.1256 - accuracy: 0.96 - ETA: 0s - loss: 0.1253 - accuracy: 0.96 - ETA: 0s - loss: 0.1245 - accuracy: 0.96 - 2s 1ms/step - loss: 0.1245 - accuracy: 0.9636 - val_loss: 0.1054 - val_accuracy: 0.9691\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0855 - accuracy: 0.96 - ETA: 1s - loss: 0.0996 - accuracy: 0.97 - ETA: 1s - loss: 0.1011 - accuracy: 0.97 - ETA: 1s - loss: 0.0981 - accuracy: 0.97 - ETA: 1s - loss: 0.0941 - accuracy: 0.97 - ETA: 1s - loss: 0.0909 - accuracy: 0.97 - ETA: 1s - loss: 0.0893 - accuracy: 0.97 - ETA: 1s - loss: 0.0895 - accuracy: 0.97 - ETA: 1s - loss: 0.0904 - accuracy: 0.97 - ETA: 1s - loss: 0.0912 - accuracy: 0.97 - ETA: 1s - loss: 0.0929 - accuracy: 0.97 - ETA: 1s - loss: 0.0927 - accuracy: 0.97 - ETA: 1s - loss: 0.0937 - accuracy: 0.97 - ETA: 1s - loss: 0.0926 - accuracy: 0.97 - ETA: 1s - loss: 0.0934 - accuracy: 0.97 - ETA: 1s - loss: 0.0931 - accuracy: 0.97 - ETA: 1s - loss: 0.0923 - accuracy: 0.97 - ETA: 1s - loss: 0.0920 - accuracy: 0.97 - ETA: 0s - loss: 0.0922 - accuracy: 0.97 - ETA: 0s - loss: 0.0919 - accuracy: 0.97 - ETA: 0s - loss: 0.0918 - accuracy: 0.97 - ETA: 0s - loss: 0.0910 - accuracy: 0.97 - ETA: 0s - loss: 0.0916 - accuracy: 0.97 - ETA: 0s - loss: 0.0916 - accuracy: 0.97 - ETA: 0s - loss: 0.0914 - accuracy: 0.97 - ETA: 0s - loss: 0.0908 - accuracy: 0.97 - ETA: 0s - loss: 0.0905 - accuracy: 0.97 - ETA: 0s - loss: 0.0904 - accuracy: 0.97 - ETA: 0s - loss: 0.0893 - accuracy: 0.97 - ETA: 0s - loss: 0.0896 - accuracy: 0.97 - ETA: 0s - loss: 0.0904 - accuracy: 0.97 - ETA: 0s - loss: 0.0897 - accuracy: 0.97 - ETA: 0s - loss: 0.0895 - accuracy: 0.97 - ETA: 0s - loss: 0.0892 - accuracy: 0.97 - ETA: 0s - loss: 0.0885 - accuracy: 0.97 - ETA: 0s - loss: 0.0887 - accuracy: 0.97 - ETA: 0s - loss: 0.0886 - accuracy: 0.97 - ETA: 0s - loss: 0.0881 - accuracy: 0.97 - ETA: 0s - loss: 0.0880 - accuracy: 0.97 - 2s 1ms/step - loss: 0.0881 - accuracy: 0.9733 - val_loss: 0.0965 - val_accuracy: 0.9711\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0381 - accuracy: 1.00 - ETA: 1s - loss: 0.0515 - accuracy: 0.98 - ETA: 1s - loss: 0.0528 - accuracy: 0.98 - ETA: 1s - loss: 0.0637 - accuracy: 0.98 - ETA: 1s - loss: 0.0642 - accuracy: 0.98 - ETA: 1s - loss: 0.0653 - accuracy: 0.98 - ETA: 1s - loss: 0.0679 - accuracy: 0.98 - ETA: 1s - loss: 0.0676 - accuracy: 0.97 - ETA: 1s - loss: 0.0676 - accuracy: 0.97 - ETA: 1s - loss: 0.0665 - accuracy: 0.97 - ETA: 1s - loss: 0.0680 - accuracy: 0.97 - ETA: 1s - loss: 0.0677 - accuracy: 0.97 - ETA: 1s - loss: 0.0677 - accuracy: 0.97 - ETA: 1s - loss: 0.0668 - accuracy: 0.97 - ETA: 1s - loss: 0.0662 - accuracy: 0.98 - ETA: 1s - loss: 0.0666 - accuracy: 0.97 - ETA: 1s - loss: 0.0673 - accuracy: 0.97 - ETA: 1s - loss: 0.0678 - accuracy: 0.97 - ETA: 0s - loss: 0.0677 - accuracy: 0.97 - ETA: 0s - loss: 0.0682 - accuracy: 0.97 - ETA: 0s - loss: 0.0673 - accuracy: 0.97 - ETA: 0s - loss: 0.0672 - accuracy: 0.97 - ETA: 0s - loss: 0.0677 - accuracy: 0.97 - ETA: 0s - loss: 0.0677 - accuracy: 0.97 - ETA: 0s - loss: 0.0672 - accuracy: 0.97 - ETA: 0s - loss: 0.0683 - accuracy: 0.97 - ETA: 0s - loss: 0.0687 - accuracy: 0.97 - ETA: 0s - loss: 0.0689 - accuracy: 0.97 - ETA: 0s - loss: 0.0690 - accuracy: 0.97 - ETA: 0s - loss: 0.0686 - accuracy: 0.97 - ETA: 0s - loss: 0.0685 - accuracy: 0.97 - ETA: 0s - loss: 0.0683 - accuracy: 0.97 - ETA: 0s - loss: 0.0686 - accuracy: 0.97 - ETA: 0s - loss: 0.0689 - accuracy: 0.97 - ETA: 0s - loss: 0.0688 - accuracy: 0.97 - ETA: 0s - loss: 0.0690 - accuracy: 0.97 - ETA: 0s - loss: 0.0691 - accuracy: 0.97 - 2s 1ms/step - loss: 0.0692 - accuracy: 0.9788 - val_loss: 0.0885 - val_accuracy: 0.9746\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0592 - accuracy: 1.00 - ETA: 1s - loss: 0.0550 - accuracy: 0.98 - ETA: 1s - loss: 0.0545 - accuracy: 0.98 - ETA: 1s - loss: 0.0588 - accuracy: 0.98 - ETA: 1s - loss: 0.0550 - accuracy: 0.98 - ETA: 1s - loss: 0.0569 - accuracy: 0.98 - ETA: 1s - loss: 0.0537 - accuracy: 0.98 - ETA: 1s - loss: 0.0548 - accuracy: 0.98 - ETA: 1s - loss: 0.0534 - accuracy: 0.98 - ETA: 1s - loss: 0.0525 - accuracy: 0.98 - ETA: 1s - loss: 0.0518 - accuracy: 0.98 - ETA: 1s - loss: 0.0525 - accuracy: 0.98 - ETA: 1s - loss: 0.0530 - accuracy: 0.98 - ETA: 1s - loss: 0.0536 - accuracy: 0.98 - ETA: 1s - loss: 0.0537 - accuracy: 0.98 - ETA: 1s - loss: 0.0530 - accuracy: 0.98 - ETA: 0s - loss: 0.0534 - accuracy: 0.98 - ETA: 0s - loss: 0.0532 - accuracy: 0.98 - ETA: 0s - loss: 0.0532 - accuracy: 0.98 - ETA: 0s - loss: 0.0531 - accuracy: 0.98 - ETA: 0s - loss: 0.0525 - accuracy: 0.98 - ETA: 0s - loss: 0.0524 - accuracy: 0.98 - ETA: 0s - loss: 0.0521 - accuracy: 0.98 - ETA: 0s - loss: 0.0526 - accuracy: 0.98 - ETA: 0s - loss: 0.0530 - accuracy: 0.98 - ETA: 0s - loss: 0.0536 - accuracy: 0.98 - ETA: 0s - loss: 0.0534 - accuracy: 0.98 - ETA: 0s - loss: 0.0535 - accuracy: 0.98 - ETA: 0s - loss: 0.0539 - accuracy: 0.98 - ETA: 0s - loss: 0.0538 - accuracy: 0.98 - ETA: 0s - loss: 0.0541 - accuracy: 0.98 - ETA: 0s - loss: 0.0539 - accuracy: 0.98 - ETA: 0s - loss: 0.0538 - accuracy: 0.98 - ETA: 0s - loss: 0.0540 - accuracy: 0.98 - ETA: 0s - loss: 0.0538 - accuracy: 0.98 - ETA: 0s - loss: 0.0538 - accuracy: 0.98 - ETA: 0s - loss: 0.0542 - accuracy: 0.98 - 2s 1ms/step - loss: 0.0541 - accuracy: 0.9839 - val_loss: 0.0803 - val_accuracy: 0.9745\n",
      "Epoch 1/5\n",
      "1861/1875 [============================>.] - ETA: 0s - loss: 2.3027 - accuracy: 0.06 - ETA: 1s - loss: 1.2493 - accuracy: 0.66 - ETA: 1s - loss: 0.8997 - accuracy: 0.75 - ETA: 1s - loss: 0.7599 - accuracy: 0.79 - ETA: 1s - loss: 0.6631 - accuracy: 0.81 - ETA: 1s - loss: 0.6005 - accuracy: 0.83 - ETA: 1s - loss: 0.5525 - accuracy: 0.84 - ETA: 1s - loss: 0.5150 - accuracy: 0.85 - ETA: 1s - loss: 0.4851 - accuracy: 0.86 - ETA: 1s - loss: 0.4625 - accuracy: 0.87 - ETA: 1s - loss: 0.4419 - accuracy: 0.87 - ETA: 1s - loss: 0.4268 - accuracy: 0.88 - ETA: 1s - loss: 0.4134 - accuracy: 0.88 - ETA: 1s - loss: 0.4002 - accuracy: 0.88 - ETA: 1s - loss: 0.3912 - accuracy: 0.88 - ETA: 1s - loss: 0.3819 - accuracy: 0.89 - ETA: 0s - loss: 0.3731 - accuracy: 0.89 - ETA: 0s - loss: 0.3649 - accuracy: 0.89 - ETA: 0s - loss: 0.3561 - accuracy: 0.89 - ETA: 0s - loss: 0.3492 - accuracy: 0.90 - ETA: 0s - loss: 0.3423 - accuracy: 0.90 - ETA: 0s - loss: 0.3376 - accuracy: 0.90 - ETA: 0s - loss: 0.3315 - accuracy: 0.90 - ETA: 0s - loss: 0.3253 - accuracy: 0.90 - ETA: 0s - loss: 0.3211 - accuracy: 0.90 - ETA: 0s - loss: 0.3167 - accuracy: 0.91 - ETA: 0s - loss: 0.3117 - accuracy: 0.91 - ETA: 0s - loss: 0.3068 - accuracy: 0.91 - ETA: 0s - loss: 0.3029 - accuracy: 0.91 - ETA: 0s - loss: 0.2985 - accuracy: 0.91 - ETA: 0s - loss: 0.2949 - accuracy: 0.91 - ETA: 0s - loss: 0.2908 - accuracy: 0.91 - ETA: 0s - loss: 0.2869 - accuracy: 0.91 - ETA: 0s - loss: 0.2833 - accuracy: 0.91 - ETA: 0s - loss: 0.2799 - accuracy: 0.92 - ETA: 0s - loss: 0.2771 - accuracy: 0.92 - ETA: 0s - loss: 0.2756 - accuracy: 0.9211WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_end` time: 0.0010s). Check your callbacks.\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.2748 - accuracy: 0.9213 - val_loss: 0.1382 - val_accuracy: 0.9580\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0489 - accuracy: 1.00 - ETA: 1s - loss: 0.1554 - accuracy: 0.95 - ETA: 1s - loss: 0.1413 - accuracy: 0.95 - ETA: 1s - loss: 0.1397 - accuracy: 0.95 - ETA: 1s - loss: 0.1391 - accuracy: 0.95 - ETA: 1s - loss: 0.1379 - accuracy: 0.95 - ETA: 1s - loss: 0.1352 - accuracy: 0.95 - ETA: 1s - loss: 0.1351 - accuracy: 0.95 - ETA: 1s - loss: 0.1347 - accuracy: 0.96 - ETA: 1s - loss: 0.1344 - accuracy: 0.96 - ETA: 1s - loss: 0.1362 - accuracy: 0.95 - ETA: 1s - loss: 0.1380 - accuracy: 0.95 - ETA: 1s - loss: 0.1383 - accuracy: 0.95 - ETA: 1s - loss: 0.1370 - accuracy: 0.96 - ETA: 1s - loss: 0.1363 - accuracy: 0.96 - ETA: 1s - loss: 0.1358 - accuracy: 0.96 - ETA: 0s - loss: 0.1336 - accuracy: 0.96 - ETA: 0s - loss: 0.1338 - accuracy: 0.96 - ETA: 0s - loss: 0.1325 - accuracy: 0.96 - ETA: 0s - loss: 0.1329 - accuracy: 0.96 - ETA: 0s - loss: 0.1338 - accuracy: 0.96 - ETA: 0s - loss: 0.1334 - accuracy: 0.96 - ETA: 0s - loss: 0.1334 - accuracy: 0.96 - ETA: 0s - loss: 0.1324 - accuracy: 0.96 - ETA: 0s - loss: 0.1311 - accuracy: 0.96 - ETA: 0s - loss: 0.1310 - accuracy: 0.96 - ETA: 0s - loss: 0.1314 - accuracy: 0.96 - ETA: 0s - loss: 0.1314 - accuracy: 0.96 - ETA: 0s - loss: 0.1304 - accuracy: 0.96 - ETA: 0s - loss: 0.1295 - accuracy: 0.96 - ETA: 0s - loss: 0.1295 - accuracy: 0.96 - ETA: 0s - loss: 0.1284 - accuracy: 0.96 - ETA: 0s - loss: 0.1279 - accuracy: 0.96 - ETA: 0s - loss: 0.1272 - accuracy: 0.96 - ETA: 0s - loss: 0.1266 - accuracy: 0.96 - ETA: 0s - loss: 0.1265 - accuracy: 0.96 - ETA: 0s - loss: 0.1262 - accuracy: 0.96 - 2s 1ms/step - loss: 0.1258 - accuracy: 0.9627 - val_loss: 0.1128 - val_accuracy: 0.9656\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - ETA: 1s - loss: 0.0220 - accuracy: 1.00 - ETA: 1s - loss: 0.0985 - accuracy: 0.97 - ETA: 1s - loss: 0.0926 - accuracy: 0.97 - ETA: 1s - loss: 0.0961 - accuracy: 0.97 - ETA: 1s - loss: 0.0948 - accuracy: 0.97 - ETA: 1s - loss: 0.0979 - accuracy: 0.97 - ETA: 1s - loss: 0.0958 - accuracy: 0.97 - ETA: 1s - loss: 0.0966 - accuracy: 0.97 - ETA: 1s - loss: 0.0972 - accuracy: 0.97 - ETA: 1s - loss: 0.0991 - accuracy: 0.97 - ETA: 1s - loss: 0.0977 - accuracy: 0.97 - ETA: 1s - loss: 0.0972 - accuracy: 0.97 - ETA: 1s - loss: 0.0980 - accuracy: 0.97 - ETA: 1s - loss: 0.0986 - accuracy: 0.97 - ETA: 1s - loss: 0.0965 - accuracy: 0.97 - ETA: 1s - loss: 0.0965 - accuracy: 0.97 - ETA: 0s - loss: 0.0952 - accuracy: 0.97 - ETA: 0s - loss: 0.0933 - accuracy: 0.97 - ETA: 0s - loss: 0.0932 - accuracy: 0.97 - ETA: 0s - loss: 0.0926 - accuracy: 0.97 - ETA: 0s - loss: 0.0922 - accuracy: 0.97 - ETA: 0s - loss: 0.0920 - accuracy: 0.97 - ETA: 0s - loss: 0.0913 - accuracy: 0.97 - ETA: 0s - loss: 0.0914 - accuracy: 0.97 - ETA: 0s - loss: 0.0918 - accuracy: 0.97 - ETA: 0s - loss: 0.0920 - accuracy: 0.97 - ETA: 0s - loss: 0.0919 - accuracy: 0.97 - ETA: 0s - loss: 0.0918 - accuracy: 0.97 - ETA: 0s - loss: 0.0920 - accuracy: 0.97 - ETA: 0s - loss: 0.0911 - accuracy: 0.97 - ETA: 0s - loss: 0.0913 - accuracy: 0.97 - ETA: 0s - loss: 0.0912 - accuracy: 0.97 - ETA: 0s - loss: 0.0908 - accuracy: 0.97 - ETA: 0s - loss: 0.0903 - accuracy: 0.97 - ETA: 0s - loss: 0.0903 - accuracy: 0.97 - ETA: 0s - loss: 0.0904 - accuracy: 0.97 - ETA: 0s - loss: 0.0897 - accuracy: 0.97 - ETA: 0s - loss: 0.0895 - accuracy: 0.97 - 2s 1ms/step - loss: 0.0895 - accuracy: 0.9735 - val_loss: 0.0946 - val_accuracy: 0.9694\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0801 - accuracy: 0.96 - ETA: 1s - loss: 0.0517 - accuracy: 0.98 - ETA: 1s - loss: 0.0634 - accuracy: 0.97 - ETA: 1s - loss: 0.0620 - accuracy: 0.97 - ETA: 1s - loss: 0.0654 - accuracy: 0.97 - ETA: 1s - loss: 0.0652 - accuracy: 0.97 - ETA: 1s - loss: 0.0653 - accuracy: 0.97 - ETA: 1s - loss: 0.0671 - accuracy: 0.97 - ETA: 1s - loss: 0.0685 - accuracy: 0.97 - ETA: 1s - loss: 0.0694 - accuracy: 0.97 - ETA: 1s - loss: 0.0686 - accuracy: 0.97 - ETA: 1s - loss: 0.0680 - accuracy: 0.97 - ETA: 1s - loss: 0.0682 - accuracy: 0.97 - ETA: 1s - loss: 0.0689 - accuracy: 0.97 - ETA: 1s - loss: 0.0690 - accuracy: 0.97 - ETA: 1s - loss: 0.0700 - accuracy: 0.97 - ETA: 1s - loss: 0.0697 - accuracy: 0.97 - ETA: 1s - loss: 0.0699 - accuracy: 0.97 - ETA: 0s - loss: 0.0696 - accuracy: 0.97 - ETA: 0s - loss: 0.0687 - accuracy: 0.97 - ETA: 0s - loss: 0.0691 - accuracy: 0.97 - ETA: 0s - loss: 0.0689 - accuracy: 0.97 - ETA: 0s - loss: 0.0688 - accuracy: 0.97 - ETA: 0s - loss: 0.0683 - accuracy: 0.97 - ETA: 0s - loss: 0.0682 - accuracy: 0.97 - ETA: 0s - loss: 0.0688 - accuracy: 0.97 - ETA: 0s - loss: 0.0692 - accuracy: 0.97 - ETA: 0s - loss: 0.0695 - accuracy: 0.97 - ETA: 0s - loss: 0.0694 - accuracy: 0.97 - ETA: 0s - loss: 0.0686 - accuracy: 0.97 - ETA: 0s - loss: 0.0683 - accuracy: 0.97 - ETA: 0s - loss: 0.0684 - accuracy: 0.97 - ETA: 0s - loss: 0.0682 - accuracy: 0.97 - ETA: 0s - loss: 0.0682 - accuracy: 0.97 - ETA: 0s - loss: 0.0684 - accuracy: 0.97 - ETA: 0s - loss: 0.0682 - accuracy: 0.97 - ETA: 0s - loss: 0.0680 - accuracy: 0.97 - ETA: 0s - loss: 0.0687 - accuracy: 0.97 - 2s 1ms/step - loss: 0.0686 - accuracy: 0.9793 - val_loss: 0.0827 - val_accuracy: 0.9726\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0027 - accuracy: 1.00 - ETA: 1s - loss: 0.0474 - accuracy: 0.98 - ETA: 1s - loss: 0.0439 - accuracy: 0.98 - ETA: 1s - loss: 0.0407 - accuracy: 0.98 - ETA: 1s - loss: 0.0424 - accuracy: 0.98 - ETA: 1s - loss: 0.0440 - accuracy: 0.98 - ETA: 1s - loss: 0.0473 - accuracy: 0.98 - ETA: 1s - loss: 0.0475 - accuracy: 0.98 - ETA: 1s - loss: 0.0482 - accuracy: 0.98 - ETA: 1s - loss: 0.0479 - accuracy: 0.98 - ETA: 1s - loss: 0.0491 - accuracy: 0.98 - ETA: 1s - loss: 0.0486 - accuracy: 0.98 - ETA: 1s - loss: 0.0499 - accuracy: 0.98 - ETA: 1s - loss: 0.0505 - accuracy: 0.98 - ETA: 1s - loss: 0.0504 - accuracy: 0.98 - ETA: 1s - loss: 0.0508 - accuracy: 0.98 - ETA: 1s - loss: 0.0501 - accuracy: 0.98 - ETA: 0s - loss: 0.0513 - accuracy: 0.98 - ETA: 0s - loss: 0.0512 - accuracy: 0.98 - ETA: 0s - loss: 0.0508 - accuracy: 0.98 - ETA: 0s - loss: 0.0505 - accuracy: 0.98 - ETA: 0s - loss: 0.0507 - accuracy: 0.98 - ETA: 0s - loss: 0.0506 - accuracy: 0.98 - ETA: 0s - loss: 0.0513 - accuracy: 0.98 - ETA: 0s - loss: 0.0513 - accuracy: 0.98 - ETA: 0s - loss: 0.0511 - accuracy: 0.98 - ETA: 0s - loss: 0.0515 - accuracy: 0.98 - ETA: 0s - loss: 0.0523 - accuracy: 0.98 - ETA: 0s - loss: 0.0519 - accuracy: 0.98 - ETA: 0s - loss: 0.0526 - accuracy: 0.98 - ETA: 0s - loss: 0.0529 - accuracy: 0.98 - ETA: 0s - loss: 0.0527 - accuracy: 0.98 - ETA: 0s - loss: 0.0529 - accuracy: 0.98 - ETA: 0s - loss: 0.0528 - accuracy: 0.98 - ETA: 0s - loss: 0.0527 - accuracy: 0.98 - ETA: 0s - loss: 0.0526 - accuracy: 0.98 - ETA: 0s - loss: 0.0532 - accuracy: 0.98 - ETA: 0s - loss: 0.0538 - accuracy: 0.98 - 2s 1ms/step - loss: 0.0541 - accuracy: 0.9838 - val_loss: 0.0905 - val_accuracy: 0.9731\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: a72f5264f8ccab2796623fc42cd077e7</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.9738666812578837</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-learning_rate: 0.001</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units: 96</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 2.3896 - accuracy: 0.21 - ETA: 1s - loss: 0.8292 - accuracy: 0.73 - ETA: 1s - loss: 0.6079 - accuracy: 0.81 - ETA: 1s - loss: 0.5481 - accuracy: 0.83 - ETA: 1s - loss: 0.4919 - accuracy: 0.84 - ETA: 1s - loss: 0.4499 - accuracy: 0.86 - ETA: 1s - loss: 0.4185 - accuracy: 0.87 - ETA: 1s - loss: 0.3949 - accuracy: 0.87 - ETA: 1s - loss: 0.3725 - accuracy: 0.88 - ETA: 1s - loss: 0.3563 - accuracy: 0.89 - ETA: 1s - loss: 0.3441 - accuracy: 0.89 - ETA: 1s - loss: 0.3397 - accuracy: 0.89 - ETA: 1s - loss: 0.3316 - accuracy: 0.89 - ETA: 1s - loss: 0.3229 - accuracy: 0.90 - ETA: 1s - loss: 0.3178 - accuracy: 0.90 - ETA: 1s - loss: 0.3156 - accuracy: 0.90 - ETA: 1s - loss: 0.3091 - accuracy: 0.90 - ETA: 1s - loss: 0.3037 - accuracy: 0.90 - ETA: 0s - loss: 0.2970 - accuracy: 0.91 - ETA: 0s - loss: 0.2941 - accuracy: 0.91 - ETA: 0s - loss: 0.2899 - accuracy: 0.91 - ETA: 0s - loss: 0.2850 - accuracy: 0.91 - ETA: 0s - loss: 0.2800 - accuracy: 0.91 - ETA: 0s - loss: 0.2782 - accuracy: 0.91 - ETA: 0s - loss: 0.2746 - accuracy: 0.91 - ETA: 0s - loss: 0.2695 - accuracy: 0.91 - ETA: 0s - loss: 0.2664 - accuracy: 0.92 - ETA: 0s - loss: 0.2641 - accuracy: 0.92 - ETA: 0s - loss: 0.2611 - accuracy: 0.92 - ETA: 0s - loss: 0.2597 - accuracy: 0.92 - ETA: 0s - loss: 0.2566 - accuracy: 0.92 - ETA: 0s - loss: 0.2524 - accuracy: 0.92 - ETA: 0s - loss: 0.2514 - accuracy: 0.92 - ETA: 0s - loss: 0.2486 - accuracy: 0.92 - ETA: 0s - loss: 0.2461 - accuracy: 0.92 - ETA: 0s - loss: 0.2452 - accuracy: 0.92 - ETA: 0s - loss: 0.2441 - accuracy: 0.92 - 2s 1ms/step - loss: 0.2423 - accuracy: 0.9288 - val_loss: 0.1649 - val_accuracy: 0.9513\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0450 - accuracy: 1.00 - ETA: 1s - loss: 0.1426 - accuracy: 0.95 - ETA: 1s - loss: 0.1384 - accuracy: 0.95 - ETA: 1s - loss: 0.1310 - accuracy: 0.96 - ETA: 1s - loss: 0.1450 - accuracy: 0.95 - ETA: 1s - loss: 0.1502 - accuracy: 0.95 - ETA: 1s - loss: 0.1526 - accuracy: 0.95 - ETA: 1s - loss: 0.1505 - accuracy: 0.95 - ETA: 1s - loss: 0.1499 - accuracy: 0.95 - ETA: 1s - loss: 0.1524 - accuracy: 0.95 - ETA: 1s - loss: 0.1588 - accuracy: 0.95 - ETA: 1s - loss: 0.1610 - accuracy: 0.95 - ETA: 1s - loss: 0.1626 - accuracy: 0.95 - ETA: 1s - loss: 0.1620 - accuracy: 0.95 - ETA: 1s - loss: 0.1634 - accuracy: 0.95 - ETA: 1s - loss: 0.1625 - accuracy: 0.95 - ETA: 1s - loss: 0.1614 - accuracy: 0.95 - ETA: 0s - loss: 0.1629 - accuracy: 0.95 - ETA: 0s - loss: 0.1628 - accuracy: 0.95 - ETA: 0s - loss: 0.1619 - accuracy: 0.95 - ETA: 0s - loss: 0.1634 - accuracy: 0.95 - ETA: 0s - loss: 0.1629 - accuracy: 0.95 - ETA: 0s - loss: 0.1621 - accuracy: 0.95 - ETA: 0s - loss: 0.1611 - accuracy: 0.95 - ETA: 0s - loss: 0.1597 - accuracy: 0.95 - ETA: 0s - loss: 0.1592 - accuracy: 0.95 - ETA: 0s - loss: 0.1588 - accuracy: 0.95 - ETA: 0s - loss: 0.1593 - accuracy: 0.95 - ETA: 0s - loss: 0.1586 - accuracy: 0.95 - ETA: 0s - loss: 0.1586 - accuracy: 0.95 - ETA: 0s - loss: 0.1592 - accuracy: 0.95 - ETA: 0s - loss: 0.1603 - accuracy: 0.95 - ETA: 0s - loss: 0.1620 - accuracy: 0.95 - ETA: 0s - loss: 0.1629 - accuracy: 0.95 - ETA: 0s - loss: 0.1615 - accuracy: 0.95 - ETA: 0s - loss: 0.1609 - accuracy: 0.95 - ETA: 0s - loss: 0.1609 - accuracy: 0.95 - ETA: 0s - loss: 0.1628 - accuracy: 0.95 - 2s 1ms/step - loss: 0.1631 - accuracy: 0.9546 - val_loss: 0.1655 - val_accuracy: 0.9540\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0235 - accuracy: 1.00 - ETA: 1s - loss: 0.0994 - accuracy: 0.97 - ETA: 1s - loss: 0.1138 - accuracy: 0.96 - ETA: 1s - loss: 0.1132 - accuracy: 0.96 - ETA: 1s - loss: 0.1134 - accuracy: 0.96 - ETA: 1s - loss: 0.1163 - accuracy: 0.96 - ETA: 1s - loss: 0.1139 - accuracy: 0.96 - ETA: 1s - loss: 0.1216 - accuracy: 0.96 - ETA: 1s - loss: 0.1257 - accuracy: 0.96 - ETA: 1s - loss: 0.1270 - accuracy: 0.96 - ETA: 1s - loss: 0.1290 - accuracy: 0.96 - ETA: 1s - loss: 0.1316 - accuracy: 0.96 - ETA: 1s - loss: 0.1342 - accuracy: 0.96 - ETA: 1s - loss: 0.1356 - accuracy: 0.96 - ETA: 1s - loss: 0.1360 - accuracy: 0.96 - ETA: 1s - loss: 0.1368 - accuracy: 0.96 - ETA: 0s - loss: 0.1379 - accuracy: 0.96 - ETA: 0s - loss: 0.1395 - accuracy: 0.96 - ETA: 0s - loss: 0.1385 - accuracy: 0.96 - ETA: 0s - loss: 0.1395 - accuracy: 0.96 - ETA: 0s - loss: 0.1398 - accuracy: 0.96 - ETA: 0s - loss: 0.1379 - accuracy: 0.96 - ETA: 0s - loss: 0.1366 - accuracy: 0.96 - ETA: 0s - loss: 0.1365 - accuracy: 0.96 - ETA: 0s - loss: 0.1379 - accuracy: 0.96 - ETA: 0s - loss: 0.1382 - accuracy: 0.96 - ETA: 0s - loss: 0.1394 - accuracy: 0.96 - ETA: 0s - loss: 0.1413 - accuracy: 0.96 - ETA: 0s - loss: 0.1417 - accuracy: 0.96 - ETA: 0s - loss: 0.1426 - accuracy: 0.96 - ETA: 0s - loss: 0.1429 - accuracy: 0.96 - ETA: 0s - loss: 0.1425 - accuracy: 0.96 - ETA: 0s - loss: 0.1417 - accuracy: 0.96 - ETA: 0s - loss: 0.1414 - accuracy: 0.96 - ETA: 0s - loss: 0.1409 - accuracy: 0.96 - ETA: 0s - loss: 0.1406 - accuracy: 0.96 - ETA: 0s - loss: 0.1409 - accuracy: 0.96 - 2s 1ms/step - loss: 0.1413 - accuracy: 0.9607 - val_loss: 0.1663 - val_accuracy: 0.9558\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.3280 - accuracy: 0.93 - ETA: 1s - loss: 0.1388 - accuracy: 0.95 - ETA: 1s - loss: 0.1281 - accuracy: 0.96 - ETA: 1s - loss: 0.1259 - accuracy: 0.96 - ETA: 1s - loss: 0.1219 - accuracy: 0.96 - ETA: 1s - loss: 0.1222 - accuracy: 0.96 - ETA: 1s - loss: 0.1217 - accuracy: 0.96 - ETA: 1s - loss: 0.1253 - accuracy: 0.96 - ETA: 1s - loss: 0.1215 - accuracy: 0.96 - ETA: 1s - loss: 0.1190 - accuracy: 0.96 - ETA: 1s - loss: 0.1171 - accuracy: 0.96 - ETA: 1s - loss: 0.1161 - accuracy: 0.96 - ETA: 1s - loss: 0.1191 - accuracy: 0.96 - ETA: 1s - loss: 0.1193 - accuracy: 0.96 - ETA: 1s - loss: 0.1188 - accuracy: 0.96 - ETA: 1s - loss: 0.1198 - accuracy: 0.96 - ETA: 1s - loss: 0.1217 - accuracy: 0.96 - ETA: 1s - loss: 0.1208 - accuracy: 0.96 - ETA: 0s - loss: 0.1198 - accuracy: 0.96 - ETA: 0s - loss: 0.1194 - accuracy: 0.96 - ETA: 0s - loss: 0.1220 - accuracy: 0.96 - ETA: 0s - loss: 0.1213 - accuracy: 0.96 - ETA: 0s - loss: 0.1216 - accuracy: 0.96 - ETA: 0s - loss: 0.1238 - accuracy: 0.96 - ETA: 0s - loss: 0.1241 - accuracy: 0.96 - ETA: 0s - loss: 0.1273 - accuracy: 0.96 - ETA: 0s - loss: 0.1275 - accuracy: 0.96 - ETA: 0s - loss: 0.1281 - accuracy: 0.96 - ETA: 0s - loss: 0.1298 - accuracy: 0.96 - ETA: 0s - loss: 0.1304 - accuracy: 0.96 - ETA: 0s - loss: 0.1311 - accuracy: 0.96 - ETA: 0s - loss: 0.1316 - accuracy: 0.96 - ETA: 0s - loss: 0.1336 - accuracy: 0.96 - ETA: 0s - loss: 0.1350 - accuracy: 0.96 - ETA: 0s - loss: 0.1357 - accuracy: 0.96 - ETA: 0s - loss: 0.1352 - accuracy: 0.96 - ETA: 0s - loss: 0.1343 - accuracy: 0.96 - ETA: 0s - loss: 0.1339 - accuracy: 0.96 - 2s 1ms/step - loss: 0.1331 - accuracy: 0.9637 - val_loss: 0.1749 - val_accuracy: 0.9607\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0278 - accuracy: 1.00 - ETA: 1s - loss: 0.0945 - accuracy: 0.97 - ETA: 1s - loss: 0.0885 - accuracy: 0.97 - ETA: 1s - loss: 0.1019 - accuracy: 0.96 - ETA: 1s - loss: 0.0961 - accuracy: 0.97 - ETA: 1s - loss: 0.1066 - accuracy: 0.96 - ETA: 1s - loss: 0.1073 - accuracy: 0.96 - ETA: 1s - loss: 0.1061 - accuracy: 0.97 - ETA: 1s - loss: 0.1086 - accuracy: 0.97 - ETA: 1s - loss: 0.1066 - accuracy: 0.97 - ETA: 1s - loss: 0.1068 - accuracy: 0.97 - ETA: 1s - loss: 0.1067 - accuracy: 0.97 - ETA: 1s - loss: 0.1091 - accuracy: 0.97 - ETA: 1s - loss: 0.1075 - accuracy: 0.97 - ETA: 1s - loss: 0.1076 - accuracy: 0.97 - ETA: 1s - loss: 0.1101 - accuracy: 0.96 - ETA: 1s - loss: 0.1139 - accuracy: 0.96 - ETA: 1s - loss: 0.1166 - accuracy: 0.96 - ETA: 1s - loss: 0.1166 - accuracy: 0.96 - ETA: 1s - loss: 0.1177 - accuracy: 0.96 - ETA: 0s - loss: 0.1186 - accuracy: 0.96 - ETA: 0s - loss: 0.1176 - accuracy: 0.96 - ETA: 0s - loss: 0.1189 - accuracy: 0.96 - ETA: 0s - loss: 0.1180 - accuracy: 0.96 - ETA: 0s - loss: 0.1176 - accuracy: 0.96 - ETA: 0s - loss: 0.1182 - accuracy: 0.96 - ETA: 0s - loss: 0.1187 - accuracy: 0.96 - ETA: 0s - loss: 0.1194 - accuracy: 0.96 - ETA: 0s - loss: 0.1197 - accuracy: 0.96 - ETA: 0s - loss: 0.1203 - accuracy: 0.96 - ETA: 0s - loss: 0.1197 - accuracy: 0.96 - ETA: 0s - loss: 0.1193 - accuracy: 0.96 - ETA: 0s - loss: 0.1198 - accuracy: 0.96 - ETA: 0s - loss: 0.1198 - accuracy: 0.96 - ETA: 0s - loss: 0.1205 - accuracy: 0.96 - ETA: 0s - loss: 0.1204 - accuracy: 0.96 - ETA: 0s - loss: 0.1225 - accuracy: 0.96 - ETA: 0s - loss: 0.1247 - accuracy: 0.96 - ETA: 0s - loss: 0.1248 - accuracy: 0.96 - 2s 1ms/step - loss: 0.1247 - accuracy: 0.9662 - val_loss: 0.1880 - val_accuracy: 0.9584\n",
      "Epoch 1/5\n",
      "   1/1875 [..............................] - ETA: 0s - loss: 2.3641 - accuracy: 0.1562WARNING:tensorflow:Callbacks method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_begin` time: 0.0010s). Check your callbacks.\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0010s). Check your callbacks.\n",
      "1875/1875 [==============================] - ETA: 1s - loss: 0.8013 - accuracy: 0.74 - ETA: 1s - loss: 0.5963 - accuracy: 0.81 - ETA: 1s - loss: 0.5028 - accuracy: 0.84 - ETA: 1s - loss: 0.4545 - accuracy: 0.85 - ETA: 1s - loss: 0.4305 - accuracy: 0.86 - ETA: 1s - loss: 0.4090 - accuracy: 0.87 - ETA: 1s - loss: 0.3847 - accuracy: 0.88 - ETA: 1s - loss: 0.3642 - accuracy: 0.88 - ETA: 1s - loss: 0.3560 - accuracy: 0.89 - ETA: 1s - loss: 0.3390 - accuracy: 0.89 - ETA: 1s - loss: 0.3268 - accuracy: 0.89 - ETA: 1s - loss: 0.3182 - accuracy: 0.90 - ETA: 1s - loss: 0.3088 - accuracy: 0.90 - ETA: 1s - loss: 0.3004 - accuracy: 0.90 - ETA: 1s - loss: 0.2926 - accuracy: 0.91 - ETA: 0s - loss: 0.2886 - accuracy: 0.91 - ETA: 0s - loss: 0.2810 - accuracy: 0.91 - ETA: 0s - loss: 0.2777 - accuracy: 0.91 - ETA: 0s - loss: 0.2744 - accuracy: 0.91 - ETA: 0s - loss: 0.2684 - accuracy: 0.91 - ETA: 0s - loss: 0.2677 - accuracy: 0.91 - ETA: 0s - loss: 0.2652 - accuracy: 0.92 - ETA: 0s - loss: 0.2628 - accuracy: 0.92 - ETA: 0s - loss: 0.2588 - accuracy: 0.92 - ETA: 0s - loss: 0.2556 - accuracy: 0.92 - ETA: 0s - loss: 0.2554 - accuracy: 0.92 - ETA: 0s - loss: 0.2545 - accuracy: 0.92 - ETA: 0s - loss: 0.2507 - accuracy: 0.92 - ETA: 0s - loss: 0.2483 - accuracy: 0.92 - ETA: 0s - loss: 0.2461 - accuracy: 0.92 - ETA: 0s - loss: 0.2432 - accuracy: 0.92 - ETA: 0s - loss: 0.2423 - accuracy: 0.92 - ETA: 0s - loss: 0.2402 - accuracy: 0.92 - ETA: 0s - loss: 0.2381 - accuracy: 0.92 - ETA: 0s - loss: 0.2370 - accuracy: 0.92 - 2s 1ms/step - loss: 0.2366 - accuracy: 0.9298 - val_loss: 0.1534 - val_accuracy: 0.9544\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0388 - accuracy: 1.00 - ETA: 1s - loss: 0.1365 - accuracy: 0.95 - ETA: 1s - loss: 0.1577 - accuracy: 0.95 - ETA: 1s - loss: 0.1723 - accuracy: 0.94 - ETA: 1s - loss: 0.1694 - accuracy: 0.94 - ETA: 1s - loss: 0.1601 - accuracy: 0.95 - ETA: 1s - loss: 0.1617 - accuracy: 0.95 - ETA: 1s - loss: 0.1616 - accuracy: 0.95 - ETA: 1s - loss: 0.1582 - accuracy: 0.95 - ETA: 1s - loss: 0.1545 - accuracy: 0.95 - ETA: 1s - loss: 0.1557 - accuracy: 0.95 - ETA: 1s - loss: 0.1533 - accuracy: 0.95 - ETA: 1s - loss: 0.1523 - accuracy: 0.95 - ETA: 1s - loss: 0.1517 - accuracy: 0.95 - ETA: 1s - loss: 0.1532 - accuracy: 0.95 - ETA: 1s - loss: 0.1525 - accuracy: 0.95 - ETA: 0s - loss: 0.1521 - accuracy: 0.95 - ETA: 0s - loss: 0.1519 - accuracy: 0.95 - ETA: 0s - loss: 0.1542 - accuracy: 0.95 - ETA: 0s - loss: 0.1544 - accuracy: 0.95 - ETA: 0s - loss: 0.1570 - accuracy: 0.95 - ETA: 0s - loss: 0.1569 - accuracy: 0.95 - ETA: 0s - loss: 0.1564 - accuracy: 0.95 - ETA: 0s - loss: 0.1548 - accuracy: 0.95 - ETA: 0s - loss: 0.1551 - accuracy: 0.95 - ETA: 0s - loss: 0.1548 - accuracy: 0.95 - ETA: 0s - loss: 0.1550 - accuracy: 0.95 - ETA: 0s - loss: 0.1551 - accuracy: 0.95 - ETA: 0s - loss: 0.1552 - accuracy: 0.95 - ETA: 0s - loss: 0.1569 - accuracy: 0.95 - ETA: 0s - loss: 0.1591 - accuracy: 0.95 - ETA: 0s - loss: 0.1579 - accuracy: 0.95 - ETA: 0s - loss: 0.1579 - accuracy: 0.95 - ETA: 0s - loss: 0.1585 - accuracy: 0.95 - ETA: 0s - loss: 0.1580 - accuracy: 0.95 - ETA: 0s - loss: 0.1583 - accuracy: 0.95 - ETA: 0s - loss: 0.1584 - accuracy: 0.95 - 2s 1ms/step - loss: 0.1580 - accuracy: 0.9559 - val_loss: 0.1575 - val_accuracy: 0.9562\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.1873 - accuracy: 0.93 - ETA: 1s - loss: 0.1489 - accuracy: 0.95 - ETA: 1s - loss: 0.1396 - accuracy: 0.96 - ETA: 1s - loss: 0.1492 - accuracy: 0.95 - ETA: 1s - loss: 0.1423 - accuracy: 0.96 - ETA: 1s - loss: 0.1416 - accuracy: 0.96 - ETA: 1s - loss: 0.1366 - accuracy: 0.96 - ETA: 1s - loss: 0.1345 - accuracy: 0.96 - ETA: 1s - loss: 0.1358 - accuracy: 0.96 - ETA: 1s - loss: 0.1313 - accuracy: 0.96 - ETA: 1s - loss: 0.1309 - accuracy: 0.96 - ETA: 1s - loss: 0.1295 - accuracy: 0.96 - ETA: 1s - loss: 0.1298 - accuracy: 0.96 - ETA: 1s - loss: 0.1313 - accuracy: 0.96 - ETA: 1s - loss: 0.1309 - accuracy: 0.96 - ETA: 1s - loss: 0.1296 - accuracy: 0.96 - ETA: 0s - loss: 0.1292 - accuracy: 0.96 - ETA: 0s - loss: 0.1309 - accuracy: 0.96 - ETA: 0s - loss: 0.1321 - accuracy: 0.96 - ETA: 0s - loss: 0.1312 - accuracy: 0.96 - ETA: 0s - loss: 0.1314 - accuracy: 0.96 - ETA: 0s - loss: 0.1314 - accuracy: 0.96 - ETA: 0s - loss: 0.1314 - accuracy: 0.96 - ETA: 0s - loss: 0.1328 - accuracy: 0.96 - ETA: 0s - loss: 0.1336 - accuracy: 0.96 - ETA: 0s - loss: 0.1336 - accuracy: 0.96 - ETA: 0s - loss: 0.1334 - accuracy: 0.96 - ETA: 0s - loss: 0.1326 - accuracy: 0.96 - ETA: 0s - loss: 0.1334 - accuracy: 0.96 - ETA: 0s - loss: 0.1351 - accuracy: 0.96 - ETA: 0s - loss: 0.1351 - accuracy: 0.96 - ETA: 0s - loss: 0.1349 - accuracy: 0.96 - ETA: 0s - loss: 0.1363 - accuracy: 0.96 - ETA: 0s - loss: 0.1380 - accuracy: 0.96 - ETA: 0s - loss: 0.1387 - accuracy: 0.96 - ETA: 0s - loss: 0.1392 - accuracy: 0.96 - 2s 1ms/step - loss: 0.1395 - accuracy: 0.9617 - val_loss: 0.2075 - val_accuracy: 0.9531\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.4422 - accuracy: 0.84 - ETA: 1s - loss: 0.1567 - accuracy: 0.96 - ETA: 1s - loss: 0.1508 - accuracy: 0.96 - ETA: 1s - loss: 0.1288 - accuracy: 0.96 - ETA: 1s - loss: 0.1240 - accuracy: 0.96 - ETA: 1s - loss: 0.1207 - accuracy: 0.96 - ETA: 1s - loss: 0.1161 - accuracy: 0.96 - ETA: 1s - loss: 0.1156 - accuracy: 0.96 - ETA: 1s - loss: 0.1232 - accuracy: 0.96 - ETA: 1s - loss: 0.1235 - accuracy: 0.96 - ETA: 1s - loss: 0.1301 - accuracy: 0.96 - ETA: 1s - loss: 0.1304 - accuracy: 0.96 - ETA: 1s - loss: 0.1305 - accuracy: 0.96 - ETA: 1s - loss: 0.1309 - accuracy: 0.96 - ETA: 1s - loss: 0.1302 - accuracy: 0.96 - ETA: 1s - loss: 0.1293 - accuracy: 0.96 - ETA: 1s - loss: 0.1309 - accuracy: 0.96 - ETA: 0s - loss: 0.1343 - accuracy: 0.96 - ETA: 0s - loss: 0.1335 - accuracy: 0.96 - ETA: 0s - loss: 0.1347 - accuracy: 0.96 - ETA: 0s - loss: 0.1339 - accuracy: 0.96 - ETA: 0s - loss: 0.1329 - accuracy: 0.96 - ETA: 0s - loss: 0.1326 - accuracy: 0.96 - ETA: 0s - loss: 0.1317 - accuracy: 0.96 - ETA: 0s - loss: 0.1301 - accuracy: 0.96 - ETA: 0s - loss: 0.1294 - accuracy: 0.96 - ETA: 0s - loss: 0.1303 - accuracy: 0.96 - ETA: 0s - loss: 0.1319 - accuracy: 0.96 - ETA: 0s - loss: 0.1319 - accuracy: 0.96 - ETA: 0s - loss: 0.1321 - accuracy: 0.96 - ETA: 0s - loss: 0.1318 - accuracy: 0.96 - ETA: 0s - loss: 0.1317 - accuracy: 0.96 - ETA: 0s - loss: 0.1314 - accuracy: 0.96 - ETA: 0s - loss: 0.1319 - accuracy: 0.96 - ETA: 0s - loss: 0.1311 - accuracy: 0.96 - ETA: 0s - loss: 0.1307 - accuracy: 0.96 - 2s 1ms/step - loss: 0.1317 - accuracy: 0.9652 - val_loss: 0.1786 - val_accuracy: 0.9574\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0617 - accuracy: 0.96 - ETA: 1s - loss: 0.1249 - accuracy: 0.96 - ETA: 1s - loss: 0.1196 - accuracy: 0.96 - ETA: 1s - loss: 0.1154 - accuracy: 0.96 - ETA: 1s - loss: 0.1088 - accuracy: 0.96 - ETA: 1s - loss: 0.1099 - accuracy: 0.97 - ETA: 1s - loss: 0.1169 - accuracy: 0.96 - ETA: 1s - loss: 0.1149 - accuracy: 0.96 - ETA: 1s - loss: 0.1157 - accuracy: 0.96 - ETA: 1s - loss: 0.1130 - accuracy: 0.96 - ETA: 1s - loss: 0.1119 - accuracy: 0.96 - ETA: 1s - loss: 0.1132 - accuracy: 0.96 - ETA: 1s - loss: 0.1137 - accuracy: 0.96 - ETA: 1s - loss: 0.1138 - accuracy: 0.96 - ETA: 1s - loss: 0.1153 - accuracy: 0.96 - ETA: 0s - loss: 0.1161 - accuracy: 0.96 - ETA: 0s - loss: 0.1158 - accuracy: 0.96 - ETA: 0s - loss: 0.1142 - accuracy: 0.96 - ETA: 0s - loss: 0.1144 - accuracy: 0.96 - ETA: 0s - loss: 0.1170 - accuracy: 0.96 - ETA: 0s - loss: 0.1169 - accuracy: 0.96 - ETA: 0s - loss: 0.1176 - accuracy: 0.96 - ETA: 0s - loss: 0.1184 - accuracy: 0.96 - ETA: 0s - loss: 0.1183 - accuracy: 0.96 - ETA: 0s - loss: 0.1178 - accuracy: 0.96 - ETA: 0s - loss: 0.1182 - accuracy: 0.96 - ETA: 0s - loss: 0.1189 - accuracy: 0.96 - ETA: 0s - loss: 0.1191 - accuracy: 0.96 - ETA: 0s - loss: 0.1217 - accuracy: 0.96 - ETA: 0s - loss: 0.1217 - accuracy: 0.96 - ETA: 0s - loss: 0.1220 - accuracy: 0.96 - ETA: 0s - loss: 0.1230 - accuracy: 0.96 - ETA: 0s - loss: 0.1230 - accuracy: 0.96 - ETA: 0s - loss: 0.1240 - accuracy: 0.96 - ETA: 0s - loss: 0.1245 - accuracy: 0.96 - 2s 1ms/step - loss: 0.1250 - accuracy: 0.9669 - val_loss: 0.1893 - val_accuracy: 0.9567\n",
      "Epoch 1/5\n",
      "   1/1875 [..............................] - ETA: 0s - loss: 2.4672 - accuracy: 0.0312WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0010s). Check your callbacks.\n",
      "1875/1875 [==============================] - ETA: 1s - loss: 0.7280 - accuracy: 0.76 - ETA: 1s - loss: 0.5817 - accuracy: 0.82 - ETA: 1s - loss: 0.5068 - accuracy: 0.84 - ETA: 1s - loss: 0.4527 - accuracy: 0.86 - ETA: 1s - loss: 0.4187 - accuracy: 0.87 - ETA: 1s - loss: 0.3990 - accuracy: 0.87 - ETA: 1s - loss: 0.3766 - accuracy: 0.88 - ETA: 1s - loss: 0.3613 - accuracy: 0.89 - ETA: 1s - loss: 0.3462 - accuracy: 0.89 - ETA: 1s - loss: 0.3347 - accuracy: 0.89 - ETA: 1s - loss: 0.3247 - accuracy: 0.90 - ETA: 1s - loss: 0.3200 - accuracy: 0.90 - ETA: 1s - loss: 0.3109 - accuracy: 0.90 - ETA: 1s - loss: 0.3016 - accuracy: 0.90 - ETA: 1s - loss: 0.2944 - accuracy: 0.91 - ETA: 1s - loss: 0.2918 - accuracy: 0.91 - ETA: 1s - loss: 0.2857 - accuracy: 0.91 - ETA: 1s - loss: 0.2796 - accuracy: 0.91 - ETA: 0s - loss: 0.2761 - accuracy: 0.91 - ETA: 0s - loss: 0.2719 - accuracy: 0.91 - ETA: 0s - loss: 0.2693 - accuracy: 0.91 - ETA: 0s - loss: 0.2664 - accuracy: 0.91 - ETA: 0s - loss: 0.2647 - accuracy: 0.92 - ETA: 0s - loss: 0.2622 - accuracy: 0.92 - ETA: 0s - loss: 0.2600 - accuracy: 0.92 - ETA: 0s - loss: 0.2590 - accuracy: 0.92 - ETA: 0s - loss: 0.2565 - accuracy: 0.92 - ETA: 0s - loss: 0.2553 - accuracy: 0.92 - ETA: 0s - loss: 0.2540 - accuracy: 0.92 - ETA: 0s - loss: 0.2521 - accuracy: 0.92 - ETA: 0s - loss: 0.2499 - accuracy: 0.92 - ETA: 0s - loss: 0.2466 - accuracy: 0.92 - ETA: 0s - loss: 0.2449 - accuracy: 0.92 - ETA: 0s - loss: 0.2434 - accuracy: 0.92 - ETA: 0s - loss: 0.2414 - accuracy: 0.92 - ETA: 0s - loss: 0.2395 - accuracy: 0.92 - ETA: 0s - loss: 0.2378 - accuracy: 0.92 - 2s 1ms/step - loss: 0.2373 - accuracy: 0.9297 - val_loss: 0.1928 - val_accuracy: 0.9455\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.1033 - accuracy: 0.96 - ETA: 1s - loss: 0.1727 - accuracy: 0.94 - ETA: 1s - loss: 0.1519 - accuracy: 0.95 - ETA: 1s - loss: 0.1495 - accuracy: 0.95 - ETA: 1s - loss: 0.1578 - accuracy: 0.95 - ETA: 1s - loss: 0.1553 - accuracy: 0.95 - ETA: 1s - loss: 0.1557 - accuracy: 0.95 - ETA: 1s - loss: 0.1576 - accuracy: 0.95 - ETA: 1s - loss: 0.1600 - accuracy: 0.95 - ETA: 1s - loss: 0.1593 - accuracy: 0.95 - ETA: 1s - loss: 0.1624 - accuracy: 0.95 - ETA: 1s - loss: 0.1635 - accuracy: 0.95 - ETA: 1s - loss: 0.1622 - accuracy: 0.95 - ETA: 1s - loss: 0.1621 - accuracy: 0.95 - ETA: 1s - loss: 0.1643 - accuracy: 0.95 - ETA: 0s - loss: 0.1639 - accuracy: 0.95 - ETA: 0s - loss: 0.1624 - accuracy: 0.95 - ETA: 0s - loss: 0.1627 - accuracy: 0.95 - ETA: 0s - loss: 0.1631 - accuracy: 0.95 - ETA: 0s - loss: 0.1627 - accuracy: 0.95 - ETA: 0s - loss: 0.1638 - accuracy: 0.95 - ETA: 0s - loss: 0.1644 - accuracy: 0.95 - ETA: 0s - loss: 0.1634 - accuracy: 0.95 - ETA: 0s - loss: 0.1635 - accuracy: 0.95 - ETA: 0s - loss: 0.1618 - accuracy: 0.95 - ETA: 0s - loss: 0.1635 - accuracy: 0.95 - ETA: 0s - loss: 0.1631 - accuracy: 0.95 - ETA: 0s - loss: 0.1642 - accuracy: 0.95 - ETA: 0s - loss: 0.1637 - accuracy: 0.95 - ETA: 0s - loss: 0.1647 - accuracy: 0.95 - ETA: 0s - loss: 0.1647 - accuracy: 0.95 - ETA: 0s - loss: 0.1658 - accuracy: 0.95 - ETA: 0s - loss: 0.1664 - accuracy: 0.95 - ETA: 0s - loss: 0.1665 - accuracy: 0.95 - ETA: 0s - loss: 0.1655 - accuracy: 0.95 - 2s 1ms/step - loss: 0.1656 - accuracy: 0.9533 - val_loss: 0.1639 - val_accuracy: 0.9584\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.2114 - accuracy: 0.93 - ETA: 1s - loss: 0.1105 - accuracy: 0.96 - ETA: 1s - loss: 0.1075 - accuracy: 0.96 - ETA: 1s - loss: 0.1127 - accuracy: 0.96 - ETA: 1s - loss: 0.1103 - accuracy: 0.96 - ETA: 1s - loss: 0.1096 - accuracy: 0.96 - ETA: 1s - loss: 0.1136 - accuracy: 0.96 - ETA: 1s - loss: 0.1175 - accuracy: 0.96 - ETA: 1s - loss: 0.1200 - accuracy: 0.96 - ETA: 1s - loss: 0.1241 - accuracy: 0.96 - ETA: 1s - loss: 0.1256 - accuracy: 0.96 - ETA: 1s - loss: 0.1262 - accuracy: 0.96 - ETA: 1s - loss: 0.1296 - accuracy: 0.96 - ETA: 1s - loss: 0.1310 - accuracy: 0.96 - ETA: 1s - loss: 0.1319 - accuracy: 0.96 - ETA: 0s - loss: 0.1312 - accuracy: 0.96 - ETA: 0s - loss: 0.1316 - accuracy: 0.96 - ETA: 0s - loss: 0.1339 - accuracy: 0.96 - ETA: 0s - loss: 0.1355 - accuracy: 0.96 - ETA: 0s - loss: 0.1374 - accuracy: 0.96 - ETA: 0s - loss: 0.1369 - accuracy: 0.96 - ETA: 0s - loss: 0.1367 - accuracy: 0.96 - ETA: 0s - loss: 0.1371 - accuracy: 0.96 - ETA: 0s - loss: 0.1374 - accuracy: 0.96 - ETA: 0s - loss: 0.1377 - accuracy: 0.96 - ETA: 0s - loss: 0.1372 - accuracy: 0.96 - ETA: 0s - loss: 0.1380 - accuracy: 0.96 - ETA: 0s - loss: 0.1377 - accuracy: 0.96 - ETA: 0s - loss: 0.1380 - accuracy: 0.96 - ETA: 0s - loss: 0.1385 - accuracy: 0.96 - ETA: 0s - loss: 0.1390 - accuracy: 0.96 - ETA: 0s - loss: 0.1394 - accuracy: 0.96 - ETA: 0s - loss: 0.1402 - accuracy: 0.96 - ETA: 0s - loss: 0.1409 - accuracy: 0.96 - ETA: 0s - loss: 0.1401 - accuracy: 0.96 - 2s 1ms/step - loss: 0.1402 - accuracy: 0.9611 - val_loss: 0.1580 - val_accuracy: 0.9608\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0465 - accuracy: 0.96 - ETA: 1s - loss: 0.1178 - accuracy: 0.96 - ETA: 1s - loss: 0.1188 - accuracy: 0.96 - ETA: 1s - loss: 0.1108 - accuracy: 0.96 - ETA: 1s - loss: 0.1218 - accuracy: 0.96 - ETA: 1s - loss: 0.1204 - accuracy: 0.96 - ETA: 1s - loss: 0.1171 - accuracy: 0.96 - ETA: 1s - loss: 0.1157 - accuracy: 0.96 - ETA: 1s - loss: 0.1167 - accuracy: 0.96 - ETA: 1s - loss: 0.1164 - accuracy: 0.96 - ETA: 1s - loss: 0.1181 - accuracy: 0.96 - ETA: 1s - loss: 0.1220 - accuracy: 0.96 - ETA: 1s - loss: 0.1232 - accuracy: 0.96 - ETA: 1s - loss: 0.1235 - accuracy: 0.96 - ETA: 1s - loss: 0.1247 - accuracy: 0.96 - ETA: 0s - loss: 0.1242 - accuracy: 0.96 - ETA: 0s - loss: 0.1292 - accuracy: 0.96 - ETA: 0s - loss: 0.1307 - accuracy: 0.96 - ETA: 0s - loss: 0.1315 - accuracy: 0.96 - ETA: 0s - loss: 0.1316 - accuracy: 0.96 - ETA: 0s - loss: 0.1320 - accuracy: 0.96 - ETA: 0s - loss: 0.1320 - accuracy: 0.96 - ETA: 0s - loss: 0.1313 - accuracy: 0.96 - ETA: 0s - loss: 0.1312 - accuracy: 0.96 - ETA: 0s - loss: 0.1314 - accuracy: 0.96 - ETA: 0s - loss: 0.1316 - accuracy: 0.96 - ETA: 0s - loss: 0.1319 - accuracy: 0.96 - ETA: 0s - loss: 0.1322 - accuracy: 0.96 - ETA: 0s - loss: 0.1328 - accuracy: 0.96 - ETA: 0s - loss: 0.1342 - accuracy: 0.96 - ETA: 0s - loss: 0.1352 - accuracy: 0.96 - ETA: 0s - loss: 0.1357 - accuracy: 0.96 - ETA: 0s - loss: 0.1359 - accuracy: 0.96 - ETA: 0s - loss: 0.1361 - accuracy: 0.96 - ETA: 0s - loss: 0.1367 - accuracy: 0.96 - 2s 1ms/step - loss: 0.1364 - accuracy: 0.9640 - val_loss: 0.1618 - val_accuracy: 0.9602\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0902 - accuracy: 0.96 - ETA: 1s - loss: 0.1177 - accuracy: 0.96 - ETA: 1s - loss: 0.1024 - accuracy: 0.96 - ETA: 1s - loss: 0.1016 - accuracy: 0.97 - ETA: 1s - loss: 0.1024 - accuracy: 0.97 - ETA: 1s - loss: 0.1006 - accuracy: 0.97 - ETA: 1s - loss: 0.1025 - accuracy: 0.97 - ETA: 1s - loss: 0.1048 - accuracy: 0.97 - ETA: 1s - loss: 0.1101 - accuracy: 0.96 - ETA: 1s - loss: 0.1130 - accuracy: 0.96 - ETA: 1s - loss: 0.1123 - accuracy: 0.96 - ETA: 1s - loss: 0.1117 - accuracy: 0.96 - ETA: 1s - loss: 0.1102 - accuracy: 0.96 - ETA: 1s - loss: 0.1101 - accuracy: 0.96 - ETA: 1s - loss: 0.1109 - accuracy: 0.96 - ETA: 0s - loss: 0.1122 - accuracy: 0.96 - ETA: 0s - loss: 0.1130 - accuracy: 0.96 - ETA: 0s - loss: 0.1141 - accuracy: 0.96 - ETA: 0s - loss: 0.1135 - accuracy: 0.96 - ETA: 0s - loss: 0.1148 - accuracy: 0.96 - ETA: 0s - loss: 0.1147 - accuracy: 0.96 - ETA: 0s - loss: 0.1159 - accuracy: 0.96 - ETA: 0s - loss: 0.1157 - accuracy: 0.96 - ETA: 0s - loss: 0.1161 - accuracy: 0.96 - ETA: 0s - loss: 0.1185 - accuracy: 0.96 - ETA: 0s - loss: 0.1195 - accuracy: 0.96 - ETA: 0s - loss: 0.1194 - accuracy: 0.96 - ETA: 0s - loss: 0.1202 - accuracy: 0.96 - ETA: 0s - loss: 0.1194 - accuracy: 0.96 - ETA: 0s - loss: 0.1205 - accuracy: 0.96 - ETA: 0s - loss: 0.1199 - accuracy: 0.96 - ETA: 0s - loss: 0.1209 - accuracy: 0.96 - ETA: 0s - loss: 0.1211 - accuracy: 0.96 - ETA: 0s - loss: 0.1206 - accuracy: 0.96 - ETA: 0s - loss: 0.1218 - accuracy: 0.96 - ETA: 0s - loss: 0.1222 - accuracy: 0.96 - 2s 1ms/step - loss: 0.1220 - accuracy: 0.9677 - val_loss: 0.1931 - val_accuracy: 0.9595\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 243474dcf7c0317d5ff8df094b1d5882</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.9596333305040995</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-learning_rate: 0.01</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units: 96</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 2.2394 - accuracy: 0.12 - ETA: 2s - loss: 2.1518 - accuracy: 0.25 - ETA: 2s - loss: 1.9763 - accuracy: 0.42 - ETA: 2s - loss: 1.7918 - accuracy: 0.53 - ETA: 2s - loss: 1.6382 - accuracy: 0.59 - ETA: 2s - loss: 1.5416 - accuracy: 0.62 - ETA: 2s - loss: 1.4250 - accuracy: 0.65 - ETA: 2s - loss: 1.3299 - accuracy: 0.68 - ETA: 2s - loss: 1.2390 - accuracy: 0.70 - ETA: 2s - loss: 1.1823 - accuracy: 0.72 - ETA: 2s - loss: 1.1246 - accuracy: 0.73 - ETA: 2s - loss: 1.0741 - accuracy: 0.74 - ETA: 2s - loss: 1.0315 - accuracy: 0.75 - ETA: 2s - loss: 0.9929 - accuracy: 0.76 - ETA: 2s - loss: 0.9481 - accuracy: 0.77 - ETA: 1s - loss: 0.9111 - accuracy: 0.78 - ETA: 1s - loss: 0.8779 - accuracy: 0.79 - ETA: 1s - loss: 0.8464 - accuracy: 0.79 - ETA: 1s - loss: 0.8232 - accuracy: 0.80 - ETA: 1s - loss: 0.7997 - accuracy: 0.80 - ETA: 1s - loss: 0.7770 - accuracy: 0.81 - ETA: 1s - loss: 0.7572 - accuracy: 0.81 - ETA: 1s - loss: 0.7380 - accuracy: 0.82 - ETA: 1s - loss: 0.7214 - accuracy: 0.82 - ETA: 1s - loss: 0.7043 - accuracy: 0.82 - ETA: 1s - loss: 0.6895 - accuracy: 0.83 - ETA: 1s - loss: 0.6747 - accuracy: 0.83 - ETA: 1s - loss: 0.6627 - accuracy: 0.83 - ETA: 1s - loss: 0.6497 - accuracy: 0.84 - ETA: 1s - loss: 0.6398 - accuracy: 0.84 - ETA: 1s - loss: 0.6298 - accuracy: 0.84 - ETA: 0s - loss: 0.6208 - accuracy: 0.84 - ETA: 0s - loss: 0.6131 - accuracy: 0.84 - ETA: 0s - loss: 0.6052 - accuracy: 0.84 - ETA: 0s - loss: 0.5956 - accuracy: 0.85 - ETA: 0s - loss: 0.5857 - accuracy: 0.85 - ETA: 0s - loss: 0.5785 - accuracy: 0.85 - ETA: 0s - loss: 0.5701 - accuracy: 0.85 - ETA: 0s - loss: 0.5631 - accuracy: 0.85 - ETA: 0s - loss: 0.5566 - accuracy: 0.86 - ETA: 0s - loss: 0.5499 - accuracy: 0.86 - ETA: 0s - loss: 0.5433 - accuracy: 0.86 - ETA: 0s - loss: 0.5368 - accuracy: 0.86 - ETA: 0s - loss: 0.5312 - accuracy: 0.86 - ETA: 0s - loss: 0.5245 - accuracy: 0.86 - ETA: 0s - loss: 0.5191 - accuracy: 0.86 - ETA: 0s - loss: 0.5144 - accuracy: 0.87 - ETA: 0s - loss: 0.5092 - accuracy: 0.87 - ETA: 0s - loss: 0.5046 - accuracy: 0.87 - 3s 2ms/step - loss: 0.5012 - accuracy: 0.8736 - val_loss: 0.2664 - val_accuracy: 0.9234\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.1358 - accuracy: 0.96 - ETA: 2s - loss: 0.3043 - accuracy: 0.91 - ETA: 2s - loss: 0.2667 - accuracy: 0.92 - ETA: 2s - loss: 0.2654 - accuracy: 0.92 - ETA: 2s - loss: 0.2724 - accuracy: 0.92 - ETA: 2s - loss: 0.2759 - accuracy: 0.92 - ETA: 2s - loss: 0.2739 - accuracy: 0.92 - ETA: 2s - loss: 0.2755 - accuracy: 0.92 - ETA: 2s - loss: 0.2717 - accuracy: 0.92 - ETA: 2s - loss: 0.2692 - accuracy: 0.92 - ETA: 2s - loss: 0.2696 - accuracy: 0.92 - ETA: 1s - loss: 0.2660 - accuracy: 0.92 - ETA: 1s - loss: 0.2635 - accuracy: 0.92 - ETA: 1s - loss: 0.2616 - accuracy: 0.92 - ETA: 1s - loss: 0.2626 - accuracy: 0.92 - ETA: 1s - loss: 0.2606 - accuracy: 0.92 - ETA: 1s - loss: 0.2595 - accuracy: 0.92 - ETA: 1s - loss: 0.2590 - accuracy: 0.92 - ETA: 1s - loss: 0.2584 - accuracy: 0.92 - ETA: 1s - loss: 0.2571 - accuracy: 0.92 - ETA: 1s - loss: 0.2559 - accuracy: 0.92 - ETA: 1s - loss: 0.2552 - accuracy: 0.93 - ETA: 1s - loss: 0.2548 - accuracy: 0.93 - ETA: 1s - loss: 0.2559 - accuracy: 0.93 - ETA: 1s - loss: 0.2559 - accuracy: 0.93 - ETA: 1s - loss: 0.2550 - accuracy: 0.93 - ETA: 1s - loss: 0.2544 - accuracy: 0.93 - ETA: 1s - loss: 0.2540 - accuracy: 0.93 - ETA: 1s - loss: 0.2520 - accuracy: 0.93 - ETA: 0s - loss: 0.2506 - accuracy: 0.93 - ETA: 0s - loss: 0.2507 - accuracy: 0.93 - ETA: 0s - loss: 0.2493 - accuracy: 0.93 - ETA: 0s - loss: 0.2488 - accuracy: 0.93 - ETA: 0s - loss: 0.2473 - accuracy: 0.93 - ETA: 0s - loss: 0.2462 - accuracy: 0.93 - ETA: 0s - loss: 0.2456 - accuracy: 0.93 - ETA: 0s - loss: 0.2455 - accuracy: 0.93 - ETA: 0s - loss: 0.2454 - accuracy: 0.93 - ETA: 0s - loss: 0.2439 - accuracy: 0.93 - ETA: 0s - loss: 0.2430 - accuracy: 0.93 - ETA: 0s - loss: 0.2423 - accuracy: 0.93 - ETA: 0s - loss: 0.2422 - accuracy: 0.93 - ETA: 0s - loss: 0.2416 - accuracy: 0.93 - ETA: 0s - loss: 0.2412 - accuracy: 0.93 - ETA: 0s - loss: 0.2408 - accuracy: 0.93 - ETA: 0s - loss: 0.2405 - accuracy: 0.93 - ETA: 0s - loss: 0.2397 - accuracy: 0.93 - ETA: 0s - loss: 0.2392 - accuracy: 0.93 - 3s 1ms/step - loss: 0.2392 - accuracy: 0.9345 - val_loss: 0.2052 - val_accuracy: 0.9426\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.1858 - accuracy: 0.96 - ETA: 2s - loss: 0.2106 - accuracy: 0.94 - ETA: 2s - loss: 0.2028 - accuracy: 0.94 - ETA: 2s - loss: 0.1976 - accuracy: 0.94 - ETA: 2s - loss: 0.1977 - accuracy: 0.94 - ETA: 2s - loss: 0.2050 - accuracy: 0.94 - ETA: 2s - loss: 0.2085 - accuracy: 0.94 - ETA: 2s - loss: 0.2104 - accuracy: 0.94 - ETA: 2s - loss: 0.2075 - accuracy: 0.94 - ETA: 2s - loss: 0.2080 - accuracy: 0.94 - ETA: 2s - loss: 0.2073 - accuracy: 0.94 - ETA: 2s - loss: 0.2047 - accuracy: 0.94 - ETA: 1s - loss: 0.2071 - accuracy: 0.94 - ETA: 1s - loss: 0.2062 - accuracy: 0.94 - ETA: 1s - loss: 0.2046 - accuracy: 0.94 - ETA: 1s - loss: 0.2037 - accuracy: 0.94 - ETA: 1s - loss: 0.2034 - accuracy: 0.94 - ETA: 1s - loss: 0.2006 - accuracy: 0.94 - ETA: 1s - loss: 0.2008 - accuracy: 0.94 - ETA: 1s - loss: 0.2023 - accuracy: 0.94 - ETA: 1s - loss: 0.2018 - accuracy: 0.94 - ETA: 1s - loss: 0.2009 - accuracy: 0.94 - ETA: 1s - loss: 0.2019 - accuracy: 0.94 - ETA: 1s - loss: 0.2017 - accuracy: 0.94 - ETA: 1s - loss: 0.2012 - accuracy: 0.94 - ETA: 1s - loss: 0.1992 - accuracy: 0.94 - ETA: 1s - loss: 0.1990 - accuracy: 0.94 - ETA: 1s - loss: 0.1982 - accuracy: 0.94 - ETA: 1s - loss: 0.1979 - accuracy: 0.94 - ETA: 1s - loss: 0.1983 - accuracy: 0.94 - ETA: 1s - loss: 0.1975 - accuracy: 0.94 - ETA: 1s - loss: 0.1963 - accuracy: 0.94 - ETA: 1s - loss: 0.1962 - accuracy: 0.94 - ETA: 0s - loss: 0.1962 - accuracy: 0.94 - ETA: 0s - loss: 0.1954 - accuracy: 0.94 - ETA: 0s - loss: 0.1937 - accuracy: 0.94 - ETA: 0s - loss: 0.1933 - accuracy: 0.94 - ETA: 0s - loss: 0.1920 - accuracy: 0.94 - ETA: 0s - loss: 0.1910 - accuracy: 0.94 - ETA: 0s - loss: 0.1906 - accuracy: 0.94 - ETA: 0s - loss: 0.1901 - accuracy: 0.94 - ETA: 0s - loss: 0.1904 - accuracy: 0.94 - ETA: 0s - loss: 0.1900 - accuracy: 0.94 - ETA: 0s - loss: 0.1901 - accuracy: 0.94 - ETA: 0s - loss: 0.1893 - accuracy: 0.94 - ETA: 0s - loss: 0.1891 - accuracy: 0.94 - ETA: 0s - loss: 0.1889 - accuracy: 0.94 - ETA: 0s - loss: 0.1883 - accuracy: 0.94 - ETA: 0s - loss: 0.1875 - accuracy: 0.94 - ETA: 0s - loss: 0.1870 - accuracy: 0.94 - ETA: 0s - loss: 0.1868 - accuracy: 0.94 - ETA: 0s - loss: 0.1866 - accuracy: 0.94 - ETA: 0s - loss: 0.1870 - accuracy: 0.94 - 3s 2ms/step - loss: 0.1867 - accuracy: 0.9485 - val_loss: 0.1688 - val_accuracy: 0.9511\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.1268 - accuracy: 0.96 - ETA: 2s - loss: 0.1653 - accuracy: 0.95 - ETA: 2s - loss: 0.1703 - accuracy: 0.95 - ETA: 2s - loss: 0.1721 - accuracy: 0.95 - ETA: 2s - loss: 0.1645 - accuracy: 0.95 - ETA: 2s - loss: 0.1622 - accuracy: 0.95 - ETA: 2s - loss: 0.1661 - accuracy: 0.95 - ETA: 2s - loss: 0.1699 - accuracy: 0.95 - ETA: 2s - loss: 0.1709 - accuracy: 0.95 - ETA: 1s - loss: 0.1707 - accuracy: 0.95 - ETA: 1s - loss: 0.1684 - accuracy: 0.95 - ETA: 1s - loss: 0.1695 - accuracy: 0.95 - ETA: 1s - loss: 0.1672 - accuracy: 0.95 - ETA: 1s - loss: 0.1651 - accuracy: 0.95 - ETA: 1s - loss: 0.1644 - accuracy: 0.95 - ETA: 1s - loss: 0.1628 - accuracy: 0.95 - ETA: 1s - loss: 0.1627 - accuracy: 0.95 - ETA: 1s - loss: 0.1640 - accuracy: 0.95 - ETA: 1s - loss: 0.1629 - accuracy: 0.95 - ETA: 1s - loss: 0.1619 - accuracy: 0.95 - ETA: 1s - loss: 0.1613 - accuracy: 0.95 - ETA: 1s - loss: 0.1615 - accuracy: 0.95 - ETA: 1s - loss: 0.1617 - accuracy: 0.95 - ETA: 1s - loss: 0.1605 - accuracy: 0.95 - ETA: 1s - loss: 0.1601 - accuracy: 0.95 - ETA: 1s - loss: 0.1600 - accuracy: 0.95 - ETA: 1s - loss: 0.1592 - accuracy: 0.95 - ETA: 1s - loss: 0.1598 - accuracy: 0.95 - ETA: 0s - loss: 0.1589 - accuracy: 0.95 - ETA: 0s - loss: 0.1589 - accuracy: 0.95 - ETA: 0s - loss: 0.1590 - accuracy: 0.95 - ETA: 0s - loss: 0.1590 - accuracy: 0.95 - ETA: 0s - loss: 0.1585 - accuracy: 0.95 - ETA: 0s - loss: 0.1582 - accuracy: 0.95 - ETA: 0s - loss: 0.1582 - accuracy: 0.95 - ETA: 0s - loss: 0.1583 - accuracy: 0.95 - ETA: 0s - loss: 0.1576 - accuracy: 0.95 - ETA: 0s - loss: 0.1569 - accuracy: 0.95 - ETA: 0s - loss: 0.1568 - accuracy: 0.95 - ETA: 0s - loss: 0.1571 - accuracy: 0.95 - ETA: 0s - loss: 0.1563 - accuracy: 0.95 - ETA: 0s - loss: 0.1560 - accuracy: 0.95 - ETA: 0s - loss: 0.1557 - accuracy: 0.95 - ETA: 0s - loss: 0.1554 - accuracy: 0.95 - ETA: 0s - loss: 0.1549 - accuracy: 0.95 - ETA: 0s - loss: 0.1543 - accuracy: 0.95 - ETA: 0s - loss: 0.1539 - accuracy: 0.95 - 3s 1ms/step - loss: 0.1535 - accuracy: 0.9569 - val_loss: 0.1446 - val_accuracy: 0.9580\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0728 - accuracy: 0.96 - ETA: 2s - loss: 0.1393 - accuracy: 0.96 - ETA: 2s - loss: 0.1346 - accuracy: 0.96 - ETA: 2s - loss: 0.1404 - accuracy: 0.96 - ETA: 2s - loss: 0.1332 - accuracy: 0.96 - ETA: 1s - loss: 0.1313 - accuracy: 0.96 - ETA: 1s - loss: 0.1317 - accuracy: 0.96 - ETA: 1s - loss: 0.1305 - accuracy: 0.96 - ETA: 1s - loss: 0.1306 - accuracy: 0.96 - ETA: 1s - loss: 0.1282 - accuracy: 0.96 - ETA: 1s - loss: 0.1299 - accuracy: 0.96 - ETA: 1s - loss: 0.1303 - accuracy: 0.96 - ETA: 1s - loss: 0.1303 - accuracy: 0.96 - ETA: 1s - loss: 0.1317 - accuracy: 0.96 - ETA: 1s - loss: 0.1313 - accuracy: 0.96 - ETA: 1s - loss: 0.1300 - accuracy: 0.96 - ETA: 1s - loss: 0.1308 - accuracy: 0.96 - ETA: 1s - loss: 0.1290 - accuracy: 0.96 - ETA: 1s - loss: 0.1301 - accuracy: 0.96 - ETA: 1s - loss: 0.1297 - accuracy: 0.96 - ETA: 1s - loss: 0.1302 - accuracy: 0.96 - ETA: 1s - loss: 0.1309 - accuracy: 0.96 - ETA: 1s - loss: 0.1303 - accuracy: 0.96 - ETA: 1s - loss: 0.1303 - accuracy: 0.96 - ETA: 0s - loss: 0.1302 - accuracy: 0.96 - ETA: 0s - loss: 0.1299 - accuracy: 0.96 - ETA: 0s - loss: 0.1310 - accuracy: 0.96 - ETA: 0s - loss: 0.1316 - accuracy: 0.96 - ETA: 0s - loss: 0.1323 - accuracy: 0.96 - ETA: 0s - loss: 0.1314 - accuracy: 0.96 - ETA: 0s - loss: 0.1316 - accuracy: 0.96 - ETA: 0s - loss: 0.1316 - accuracy: 0.96 - ETA: 0s - loss: 0.1314 - accuracy: 0.96 - ETA: 0s - loss: 0.1313 - accuracy: 0.96 - ETA: 0s - loss: 0.1310 - accuracy: 0.96 - ETA: 0s - loss: 0.1311 - accuracy: 0.96 - ETA: 0s - loss: 0.1309 - accuracy: 0.96 - ETA: 0s - loss: 0.1305 - accuracy: 0.96 - ETA: 0s - loss: 0.1304 - accuracy: 0.96 - ETA: 0s - loss: 0.1301 - accuracy: 0.96 - ETA: 0s - loss: 0.1299 - accuracy: 0.96 - ETA: 0s - loss: 0.1299 - accuracy: 0.96 - ETA: 0s - loss: 0.1297 - accuracy: 0.96 - ETA: 0s - loss: 0.1296 - accuracy: 0.96 - ETA: 0s - loss: 0.1298 - accuracy: 0.96 - 2s 1ms/step - loss: 0.1299 - accuracy: 0.9641 - val_loss: 0.1266 - val_accuracy: 0.9625\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 2.1953 - accuracy: 0.31 - ETA: 2s - loss: 2.0524 - accuracy: 0.31 - ETA: 2s - loss: 1.8206 - accuracy: 0.49 - ETA: 1s - loss: 1.6181 - accuracy: 0.59 - ETA: 1s - loss: 1.4450 - accuracy: 0.64 - ETA: 1s - loss: 1.3123 - accuracy: 0.68 - ETA: 1s - loss: 1.2119 - accuracy: 0.71 - ETA: 1s - loss: 1.1495 - accuracy: 0.72 - ETA: 1s - loss: 1.0830 - accuracy: 0.74 - ETA: 1s - loss: 1.0313 - accuracy: 0.75 - ETA: 1s - loss: 0.9828 - accuracy: 0.76 - ETA: 1s - loss: 0.9469 - accuracy: 0.77 - ETA: 1s - loss: 0.9104 - accuracy: 0.78 - ETA: 1s - loss: 0.8755 - accuracy: 0.79 - ETA: 1s - loss: 0.8427 - accuracy: 0.79 - ETA: 1s - loss: 0.8131 - accuracy: 0.80 - ETA: 1s - loss: 0.7872 - accuracy: 0.81 - ETA: 1s - loss: 0.7622 - accuracy: 0.81 - ETA: 1s - loss: 0.7428 - accuracy: 0.82 - ETA: 1s - loss: 0.7222 - accuracy: 0.82 - ETA: 1s - loss: 0.7043 - accuracy: 0.82 - ETA: 1s - loss: 0.6873 - accuracy: 0.83 - ETA: 1s - loss: 0.6728 - accuracy: 0.83 - ETA: 1s - loss: 0.6580 - accuracy: 0.83 - ETA: 0s - loss: 0.6431 - accuracy: 0.84 - ETA: 0s - loss: 0.6308 - accuracy: 0.84 - ETA: 0s - loss: 0.6193 - accuracy: 0.84 - ETA: 0s - loss: 0.6089 - accuracy: 0.84 - ETA: 0s - loss: 0.5985 - accuracy: 0.85 - ETA: 0s - loss: 0.5882 - accuracy: 0.85 - ETA: 0s - loss: 0.5780 - accuracy: 0.85 - ETA: 0s - loss: 0.5702 - accuracy: 0.85 - ETA: 0s - loss: 0.5615 - accuracy: 0.86 - ETA: 0s - loss: 0.5535 - accuracy: 0.86 - ETA: 0s - loss: 0.5464 - accuracy: 0.86 - ETA: 0s - loss: 0.5397 - accuracy: 0.86 - ETA: 0s - loss: 0.5327 - accuracy: 0.86 - ETA: 0s - loss: 0.5266 - accuracy: 0.86 - ETA: 0s - loss: 0.5210 - accuracy: 0.86 - ETA: 0s - loss: 0.5143 - accuracy: 0.87 - ETA: 0s - loss: 0.5080 - accuracy: 0.87 - ETA: 0s - loss: 0.5028 - accuracy: 0.87 - ETA: 0s - loss: 0.4992 - accuracy: 0.87 - 2s 1ms/step - loss: 0.4957 - accuracy: 0.8748 - val_loss: 0.2704 - val_accuracy: 0.9247\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.3890 - accuracy: 0.87 - ETA: 2s - loss: 0.3024 - accuracy: 0.91 - ETA: 2s - loss: 0.2792 - accuracy: 0.92 - ETA: 2s - loss: 0.2741 - accuracy: 0.92 - ETA: 2s - loss: 0.2649 - accuracy: 0.92 - ETA: 2s - loss: 0.2661 - accuracy: 0.92 - ETA: 1s - loss: 0.2682 - accuracy: 0.92 - ETA: 2s - loss: 0.2690 - accuracy: 0.92 - ETA: 1s - loss: 0.2626 - accuracy: 0.92 - ETA: 1s - loss: 0.2641 - accuracy: 0.92 - ETA: 1s - loss: 0.2617 - accuracy: 0.92 - ETA: 1s - loss: 0.2615 - accuracy: 0.92 - ETA: 1s - loss: 0.2614 - accuracy: 0.92 - ETA: 1s - loss: 0.2591 - accuracy: 0.92 - ETA: 1s - loss: 0.2580 - accuracy: 0.92 - ETA: 1s - loss: 0.2567 - accuracy: 0.92 - ETA: 1s - loss: 0.2584 - accuracy: 0.92 - ETA: 1s - loss: 0.2565 - accuracy: 0.93 - ETA: 1s - loss: 0.2552 - accuracy: 0.93 - ETA: 1s - loss: 0.2542 - accuracy: 0.93 - ETA: 1s - loss: 0.2550 - accuracy: 0.93 - ETA: 1s - loss: 0.2541 - accuracy: 0.93 - ETA: 1s - loss: 0.2537 - accuracy: 0.93 - ETA: 1s - loss: 0.2536 - accuracy: 0.93 - ETA: 1s - loss: 0.2535 - accuracy: 0.93 - ETA: 0s - loss: 0.2549 - accuracy: 0.92 - ETA: 0s - loss: 0.2548 - accuracy: 0.92 - ETA: 0s - loss: 0.2548 - accuracy: 0.92 - ETA: 0s - loss: 0.2530 - accuracy: 0.93 - ETA: 0s - loss: 0.2528 - accuracy: 0.93 - ETA: 0s - loss: 0.2527 - accuracy: 0.93 - ETA: 0s - loss: 0.2513 - accuracy: 0.93 - ETA: 0s - loss: 0.2507 - accuracy: 0.93 - ETA: 0s - loss: 0.2502 - accuracy: 0.93 - ETA: 0s - loss: 0.2485 - accuracy: 0.93 - ETA: 0s - loss: 0.2475 - accuracy: 0.93 - ETA: 0s - loss: 0.2464 - accuracy: 0.93 - ETA: 0s - loss: 0.2457 - accuracy: 0.93 - ETA: 0s - loss: 0.2452 - accuracy: 0.93 - ETA: 0s - loss: 0.2444 - accuracy: 0.93 - ETA: 0s - loss: 0.2437 - accuracy: 0.93 - ETA: 0s - loss: 0.2427 - accuracy: 0.93 - ETA: 0s - loss: 0.2428 - accuracy: 0.93 - ETA: 0s - loss: 0.2421 - accuracy: 0.93 - ETA: 0s - loss: 0.2417 - accuracy: 0.93 - 2s 1ms/step - loss: 0.2417 - accuracy: 0.9329 - val_loss: 0.2058 - val_accuracy: 0.9423\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.2621 - accuracy: 0.93 - ETA: 1s - loss: 0.1930 - accuracy: 0.94 - ETA: 2s - loss: 0.2013 - accuracy: 0.94 - ETA: 2s - loss: 0.2006 - accuracy: 0.94 - ETA: 2s - loss: 0.2077 - accuracy: 0.94 - ETA: 2s - loss: 0.2092 - accuracy: 0.94 - ETA: 2s - loss: 0.2105 - accuracy: 0.94 - ETA: 1s - loss: 0.2101 - accuracy: 0.94 - ETA: 1s - loss: 0.2071 - accuracy: 0.94 - ETA: 1s - loss: 0.2071 - accuracy: 0.94 - ETA: 1s - loss: 0.2088 - accuracy: 0.94 - ETA: 1s - loss: 0.2067 - accuracy: 0.94 - ETA: 1s - loss: 0.2067 - accuracy: 0.94 - ETA: 1s - loss: 0.2055 - accuracy: 0.94 - ETA: 1s - loss: 0.2044 - accuracy: 0.94 - ETA: 1s - loss: 0.2024 - accuracy: 0.94 - ETA: 1s - loss: 0.2031 - accuracy: 0.94 - ETA: 1s - loss: 0.2010 - accuracy: 0.94 - ETA: 1s - loss: 0.2003 - accuracy: 0.94 - ETA: 1s - loss: 0.2010 - accuracy: 0.94 - ETA: 1s - loss: 0.2007 - accuracy: 0.94 - ETA: 1s - loss: 0.1989 - accuracy: 0.94 - ETA: 1s - loss: 0.1984 - accuracy: 0.94 - ETA: 1s - loss: 0.1984 - accuracy: 0.94 - ETA: 1s - loss: 0.1973 - accuracy: 0.94 - ETA: 0s - loss: 0.1963 - accuracy: 0.94 - ETA: 0s - loss: 0.1965 - accuracy: 0.94 - ETA: 0s - loss: 0.1963 - accuracy: 0.94 - ETA: 0s - loss: 0.1950 - accuracy: 0.94 - ETA: 0s - loss: 0.1936 - accuracy: 0.94 - ETA: 0s - loss: 0.1933 - accuracy: 0.94 - ETA: 0s - loss: 0.1924 - accuracy: 0.94 - ETA: 0s - loss: 0.1923 - accuracy: 0.94 - ETA: 0s - loss: 0.1920 - accuracy: 0.94 - ETA: 0s - loss: 0.1922 - accuracy: 0.94 - ETA: 0s - loss: 0.1917 - accuracy: 0.94 - ETA: 0s - loss: 0.1915 - accuracy: 0.94 - ETA: 0s - loss: 0.1912 - accuracy: 0.94 - ETA: 0s - loss: 0.1909 - accuracy: 0.94 - ETA: 0s - loss: 0.1903 - accuracy: 0.94 - ETA: 0s - loss: 0.1906 - accuracy: 0.94 - ETA: 0s - loss: 0.1906 - accuracy: 0.94 - ETA: 0s - loss: 0.1897 - accuracy: 0.94 - ETA: 0s - loss: 0.1895 - accuracy: 0.94 - 2s 1ms/step - loss: 0.1895 - accuracy: 0.9473 - val_loss: 0.1694 - val_accuracy: 0.9533\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.1461 - accuracy: 0.96 - ETA: 2s - loss: 0.1764 - accuracy: 0.94 - ETA: 2s - loss: 0.1607 - accuracy: 0.95 - ETA: 2s - loss: 0.1698 - accuracy: 0.95 - ETA: 2s - loss: 0.1671 - accuracy: 0.95 - ETA: 2s - loss: 0.1666 - accuracy: 0.95 - ETA: 1s - loss: 0.1650 - accuracy: 0.95 - ETA: 1s - loss: 0.1634 - accuracy: 0.95 - ETA: 1s - loss: 0.1626 - accuracy: 0.95 - ETA: 1s - loss: 0.1609 - accuracy: 0.95 - ETA: 1s - loss: 0.1630 - accuracy: 0.95 - ETA: 1s - loss: 0.1629 - accuracy: 0.95 - ETA: 1s - loss: 0.1614 - accuracy: 0.95 - ETA: 1s - loss: 0.1624 - accuracy: 0.95 - ETA: 1s - loss: 0.1615 - accuracy: 0.95 - ETA: 1s - loss: 0.1608 - accuracy: 0.95 - ETA: 1s - loss: 0.1622 - accuracy: 0.95 - ETA: 1s - loss: 0.1625 - accuracy: 0.95 - ETA: 1s - loss: 0.1619 - accuracy: 0.95 - ETA: 1s - loss: 0.1617 - accuracy: 0.95 - ETA: 1s - loss: 0.1622 - accuracy: 0.95 - ETA: 1s - loss: 0.1618 - accuracy: 0.95 - ETA: 1s - loss: 0.1621 - accuracy: 0.95 - ETA: 1s - loss: 0.1613 - accuracy: 0.95 - ETA: 1s - loss: 0.1599 - accuracy: 0.95 - ETA: 1s - loss: 0.1601 - accuracy: 0.95 - ETA: 0s - loss: 0.1598 - accuracy: 0.95 - ETA: 0s - loss: 0.1598 - accuracy: 0.95 - ETA: 0s - loss: 0.1598 - accuracy: 0.95 - ETA: 0s - loss: 0.1595 - accuracy: 0.95 - ETA: 0s - loss: 0.1600 - accuracy: 0.95 - ETA: 0s - loss: 0.1596 - accuracy: 0.95 - ETA: 0s - loss: 0.1591 - accuracy: 0.95 - ETA: 0s - loss: 0.1586 - accuracy: 0.95 - ETA: 0s - loss: 0.1582 - accuracy: 0.95 - ETA: 0s - loss: 0.1579 - accuracy: 0.95 - ETA: 0s - loss: 0.1576 - accuracy: 0.95 - ETA: 0s - loss: 0.1572 - accuracy: 0.95 - ETA: 0s - loss: 0.1567 - accuracy: 0.95 - ETA: 0s - loss: 0.1564 - accuracy: 0.95 - ETA: 0s - loss: 0.1566 - accuracy: 0.95 - ETA: 0s - loss: 0.1561 - accuracy: 0.95 - ETA: 0s - loss: 0.1561 - accuracy: 0.95 - ETA: 0s - loss: 0.1561 - accuracy: 0.95 - ETA: 0s - loss: 0.1558 - accuracy: 0.95 - ETA: 0s - loss: 0.1560 - accuracy: 0.95 - ETA: 0s - loss: 0.1559 - accuracy: 0.95 - 3s 1ms/step - loss: 0.1560 - accuracy: 0.9566 - val_loss: 0.1478 - val_accuracy: 0.9594\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0321 - accuracy: 1.00 - ETA: 2s - loss: 0.1495 - accuracy: 0.95 - ETA: 2s - loss: 0.1459 - accuracy: 0.95 - ETA: 2s - loss: 0.1467 - accuracy: 0.95 - ETA: 2s - loss: 0.1424 - accuracy: 0.96 - ETA: 2s - loss: 0.1388 - accuracy: 0.96 - ETA: 2s - loss: 0.1357 - accuracy: 0.96 - ETA: 1s - loss: 0.1380 - accuracy: 0.96 - ETA: 1s - loss: 0.1385 - accuracy: 0.96 - ETA: 1s - loss: 0.1374 - accuracy: 0.96 - ETA: 1s - loss: 0.1368 - accuracy: 0.96 - ETA: 1s - loss: 0.1372 - accuracy: 0.96 - ETA: 1s - loss: 0.1369 - accuracy: 0.96 - ETA: 1s - loss: 0.1354 - accuracy: 0.96 - ETA: 1s - loss: 0.1350 - accuracy: 0.96 - ETA: 1s - loss: 0.1365 - accuracy: 0.96 - ETA: 1s - loss: 0.1376 - accuracy: 0.96 - ETA: 1s - loss: 0.1368 - accuracy: 0.96 - ETA: 1s - loss: 0.1359 - accuracy: 0.96 - ETA: 1s - loss: 0.1358 - accuracy: 0.96 - ETA: 1s - loss: 0.1354 - accuracy: 0.96 - ETA: 1s - loss: 0.1367 - accuracy: 0.96 - ETA: 1s - loss: 0.1360 - accuracy: 0.96 - ETA: 1s - loss: 0.1364 - accuracy: 0.96 - ETA: 1s - loss: 0.1361 - accuracy: 0.96 - ETA: 0s - loss: 0.1367 - accuracy: 0.96 - ETA: 0s - loss: 0.1360 - accuracy: 0.96 - ETA: 0s - loss: 0.1355 - accuracy: 0.96 - ETA: 0s - loss: 0.1353 - accuracy: 0.96 - ETA: 0s - loss: 0.1353 - accuracy: 0.96 - ETA: 0s - loss: 0.1348 - accuracy: 0.96 - ETA: 0s - loss: 0.1346 - accuracy: 0.96 - ETA: 0s - loss: 0.1341 - accuracy: 0.96 - ETA: 0s - loss: 0.1333 - accuracy: 0.96 - ETA: 0s - loss: 0.1331 - accuracy: 0.96 - ETA: 0s - loss: 0.1329 - accuracy: 0.96 - ETA: 0s - loss: 0.1332 - accuracy: 0.96 - ETA: 0s - loss: 0.1337 - accuracy: 0.96 - ETA: 0s - loss: 0.1330 - accuracy: 0.96 - ETA: 0s - loss: 0.1329 - accuracy: 0.96 - ETA: 0s - loss: 0.1323 - accuracy: 0.96 - ETA: 0s - loss: 0.1316 - accuracy: 0.96 - ETA: 0s - loss: 0.1315 - accuracy: 0.96 - ETA: 0s - loss: 0.1322 - accuracy: 0.96 - ETA: 0s - loss: 0.1316 - accuracy: 0.96 - ETA: 0s - loss: 0.1315 - accuracy: 0.96 - 3s 1ms/step - loss: 0.1318 - accuracy: 0.9638 - val_loss: 0.1285 - val_accuracy: 0.9654\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 2.4210 - accuracy: 0.09 - ETA: 2s - loss: 2.1278 - accuracy: 0.31 - ETA: 2s - loss: 1.9173 - accuracy: 0.46 - ETA: 2s - loss: 1.7303 - accuracy: 0.56 - ETA: 2s - loss: 1.5741 - accuracy: 0.61 - ETA: 2s - loss: 1.4423 - accuracy: 0.65 - ETA: 2s - loss: 1.3311 - accuracy: 0.68 - ETA: 2s - loss: 1.2439 - accuracy: 0.71 - ETA: 2s - loss: 1.1720 - accuracy: 0.72 - ETA: 1s - loss: 1.0970 - accuracy: 0.74 - ETA: 1s - loss: 1.0416 - accuracy: 0.75 - ETA: 1s - loss: 0.9933 - accuracy: 0.76 - ETA: 1s - loss: 0.9485 - accuracy: 0.78 - ETA: 1s - loss: 0.9090 - accuracy: 0.78 - ETA: 1s - loss: 0.8793 - accuracy: 0.79 - ETA: 1s - loss: 0.8480 - accuracy: 0.80 - ETA: 1s - loss: 0.8200 - accuracy: 0.80 - ETA: 1s - loss: 0.7998 - accuracy: 0.81 - ETA: 1s - loss: 0.7772 - accuracy: 0.81 - ETA: 1s - loss: 0.7575 - accuracy: 0.82 - ETA: 1s - loss: 0.7381 - accuracy: 0.82 - ETA: 1s - loss: 0.7200 - accuracy: 0.82 - ETA: 1s - loss: 0.7015 - accuracy: 0.83 - ETA: 1s - loss: 0.6862 - accuracy: 0.83 - ETA: 1s - loss: 0.6709 - accuracy: 0.83 - ETA: 1s - loss: 0.6587 - accuracy: 0.84 - ETA: 0s - loss: 0.6478 - accuracy: 0.84 - ETA: 0s - loss: 0.6383 - accuracy: 0.84 - ETA: 0s - loss: 0.6291 - accuracy: 0.84 - ETA: 0s - loss: 0.6189 - accuracy: 0.85 - ETA: 0s - loss: 0.6088 - accuracy: 0.85 - ETA: 0s - loss: 0.5990 - accuracy: 0.85 - ETA: 0s - loss: 0.5904 - accuracy: 0.85 - ETA: 0s - loss: 0.5833 - accuracy: 0.85 - ETA: 0s - loss: 0.5746 - accuracy: 0.85 - ETA: 0s - loss: 0.5669 - accuracy: 0.86 - ETA: 0s - loss: 0.5591 - accuracy: 0.86 - ETA: 0s - loss: 0.5525 - accuracy: 0.86 - ETA: 0s - loss: 0.5457 - accuracy: 0.86 - ETA: 0s - loss: 0.5395 - accuracy: 0.86 - ETA: 0s - loss: 0.5316 - accuracy: 0.86 - ETA: 0s - loss: 0.5257 - accuracy: 0.87 - ETA: 0s - loss: 0.5233 - accuracy: 0.87 - ETA: 0s - loss: 0.5207 - accuracy: 0.87 - ETA: 0s - loss: 0.5175 - accuracy: 0.87 - ETA: 0s - loss: 0.5147 - accuracy: 0.87 - ETA: 0s - loss: 0.5115 - accuracy: 0.87 - ETA: 0s - loss: 0.5069 - accuracy: 0.87 - ETA: 0s - loss: 0.5023 - accuracy: 0.87 - 3s 2ms/step - loss: 0.5010 - accuracy: 0.8752 - val_loss: 0.2645 - val_accuracy: 0.9277\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.2662 - accuracy: 0.96 - ETA: 2s - loss: 0.2852 - accuracy: 0.92 - ETA: 2s - loss: 0.2797 - accuracy: 0.92 - ETA: 2s - loss: 0.2755 - accuracy: 0.92 - ETA: 2s - loss: 0.2704 - accuracy: 0.93 - ETA: 2s - loss: 0.2705 - accuracy: 0.92 - ETA: 2s - loss: 0.2718 - accuracy: 0.92 - ETA: 2s - loss: 0.2688 - accuracy: 0.92 - ETA: 2s - loss: 0.2711 - accuracy: 0.92 - ETA: 2s - loss: 0.2715 - accuracy: 0.92 - ETA: 2s - loss: 0.2730 - accuracy: 0.92 - ETA: 2s - loss: 0.2675 - accuracy: 0.92 - ETA: 2s - loss: 0.2668 - accuracy: 0.92 - ETA: 1s - loss: 0.2642 - accuracy: 0.92 - ETA: 1s - loss: 0.2613 - accuracy: 0.92 - ETA: 1s - loss: 0.2594 - accuracy: 0.92 - ETA: 1s - loss: 0.2618 - accuracy: 0.92 - ETA: 1s - loss: 0.2611 - accuracy: 0.92 - ETA: 1s - loss: 0.2617 - accuracy: 0.92 - ETA: 1s - loss: 0.2615 - accuracy: 0.92 - ETA: 1s - loss: 0.2599 - accuracy: 0.92 - ETA: 1s - loss: 0.2589 - accuracy: 0.92 - ETA: 1s - loss: 0.2592 - accuracy: 0.92 - ETA: 1s - loss: 0.2580 - accuracy: 0.92 - ETA: 1s - loss: 0.2571 - accuracy: 0.92 - ETA: 1s - loss: 0.2559 - accuracy: 0.92 - ETA: 1s - loss: 0.2544 - accuracy: 0.92 - ETA: 1s - loss: 0.2544 - accuracy: 0.92 - ETA: 1s - loss: 0.2537 - accuracy: 0.93 - ETA: 1s - loss: 0.2533 - accuracy: 0.93 - ETA: 1s - loss: 0.2525 - accuracy: 0.93 - ETA: 1s - loss: 0.2514 - accuracy: 0.93 - ETA: 1s - loss: 0.2508 - accuracy: 0.93 - ETA: 0s - loss: 0.2501 - accuracy: 0.93 - ETA: 0s - loss: 0.2496 - accuracy: 0.93 - ETA: 0s - loss: 0.2484 - accuracy: 0.93 - ETA: 0s - loss: 0.2476 - accuracy: 0.93 - ETA: 0s - loss: 0.2465 - accuracy: 0.93 - ETA: 0s - loss: 0.2469 - accuracy: 0.93 - ETA: 0s - loss: 0.2465 - accuracy: 0.93 - ETA: 0s - loss: 0.2459 - accuracy: 0.93 - ETA: 0s - loss: 0.2456 - accuracy: 0.93 - ETA: 0s - loss: 0.2448 - accuracy: 0.93 - ETA: 0s - loss: 0.2443 - accuracy: 0.93 - ETA: 0s - loss: 0.2431 - accuracy: 0.93 - ETA: 0s - loss: 0.2421 - accuracy: 0.93 - ETA: 0s - loss: 0.2415 - accuracy: 0.93 - ETA: 0s - loss: 0.2412 - accuracy: 0.93 - ETA: 0s - loss: 0.2403 - accuracy: 0.93 - ETA: 0s - loss: 0.2393 - accuracy: 0.93 - ETA: 0s - loss: 0.2385 - accuracy: 0.93 - ETA: 0s - loss: 0.2377 - accuracy: 0.93 - ETA: 0s - loss: 0.2371 - accuracy: 0.93 - 3s 2ms/step - loss: 0.2371 - accuracy: 0.9351 - val_loss: 0.2035 - val_accuracy: 0.9437\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.2048 - accuracy: 0.93 - ETA: 2s - loss: 0.1999 - accuracy: 0.94 - ETA: 2s - loss: 0.2025 - accuracy: 0.94 - ETA: 2s - loss: 0.2092 - accuracy: 0.94 - ETA: 2s - loss: 0.2018 - accuracy: 0.94 - ETA: 2s - loss: 0.2002 - accuracy: 0.94 - ETA: 2s - loss: 0.2057 - accuracy: 0.94 - ETA: 2s - loss: 0.2003 - accuracy: 0.94 - ETA: 2s - loss: 0.2012 - accuracy: 0.94 - ETA: 1s - loss: 0.1995 - accuracy: 0.94 - ETA: 1s - loss: 0.1988 - accuracy: 0.94 - ETA: 1s - loss: 0.1982 - accuracy: 0.94 - ETA: 1s - loss: 0.1989 - accuracy: 0.94 - ETA: 1s - loss: 0.1977 - accuracy: 0.94 - ETA: 1s - loss: 0.1987 - accuracy: 0.94 - ETA: 1s - loss: 0.1993 - accuracy: 0.94 - ETA: 1s - loss: 0.1994 - accuracy: 0.94 - ETA: 1s - loss: 0.1982 - accuracy: 0.94 - ETA: 1s - loss: 0.1995 - accuracy: 0.94 - ETA: 1s - loss: 0.1990 - accuracy: 0.94 - ETA: 1s - loss: 0.1989 - accuracy: 0.94 - ETA: 1s - loss: 0.1974 - accuracy: 0.94 - ETA: 1s - loss: 0.1974 - accuracy: 0.94 - ETA: 1s - loss: 0.1960 - accuracy: 0.94 - ETA: 1s - loss: 0.1954 - accuracy: 0.94 - ETA: 1s - loss: 0.1942 - accuracy: 0.94 - ETA: 1s - loss: 0.1932 - accuracy: 0.94 - ETA: 1s - loss: 0.1917 - accuracy: 0.94 - ETA: 1s - loss: 0.1924 - accuracy: 0.94 - ETA: 1s - loss: 0.1916 - accuracy: 0.94 - ETA: 0s - loss: 0.1908 - accuracy: 0.94 - ETA: 0s - loss: 0.1907 - accuracy: 0.94 - ETA: 0s - loss: 0.1903 - accuracy: 0.94 - ETA: 0s - loss: 0.1903 - accuracy: 0.94 - ETA: 0s - loss: 0.1899 - accuracy: 0.94 - ETA: 0s - loss: 0.1895 - accuracy: 0.94 - ETA: 0s - loss: 0.1886 - accuracy: 0.94 - ETA: 0s - loss: 0.1885 - accuracy: 0.94 - ETA: 0s - loss: 0.1884 - accuracy: 0.94 - ETA: 0s - loss: 0.1876 - accuracy: 0.94 - ETA: 0s - loss: 0.1875 - accuracy: 0.94 - ETA: 0s - loss: 0.1875 - accuracy: 0.94 - ETA: 0s - loss: 0.1863 - accuracy: 0.94 - ETA: 0s - loss: 0.1863 - accuracy: 0.94 - ETA: 0s - loss: 0.1854 - accuracy: 0.94 - ETA: 0s - loss: 0.1849 - accuracy: 0.94 - ETA: 0s - loss: 0.1846 - accuracy: 0.94 - ETA: 0s - loss: 0.1849 - accuracy: 0.94 - ETA: 0s - loss: 0.1849 - accuracy: 0.94 - ETA: 0s - loss: 0.1848 - accuracy: 0.94 - 3s 1ms/step - loss: 0.1848 - accuracy: 0.9484 - val_loss: 0.1656 - val_accuracy: 0.9530\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - ETA: 1s - loss: 0.1355 - accuracy: 0.93 - ETA: 2s - loss: 0.1651 - accuracy: 0.95 - ETA: 2s - loss: 0.1650 - accuracy: 0.95 - ETA: 2s - loss: 0.1633 - accuracy: 0.95 - ETA: 2s - loss: 0.1575 - accuracy: 0.95 - ETA: 2s - loss: 0.1575 - accuracy: 0.95 - ETA: 2s - loss: 0.1576 - accuracy: 0.95 - ETA: 2s - loss: 0.1547 - accuracy: 0.95 - ETA: 2s - loss: 0.1519 - accuracy: 0.95 - ETA: 2s - loss: 0.1534 - accuracy: 0.95 - ETA: 2s - loss: 0.1537 - accuracy: 0.95 - ETA: 2s - loss: 0.1529 - accuracy: 0.95 - ETA: 1s - loss: 0.1545 - accuracy: 0.95 - ETA: 1s - loss: 0.1540 - accuracy: 0.95 - ETA: 1s - loss: 0.1551 - accuracy: 0.95 - ETA: 1s - loss: 0.1545 - accuracy: 0.95 - ETA: 1s - loss: 0.1545 - accuracy: 0.95 - ETA: 1s - loss: 0.1552 - accuracy: 0.95 - ETA: 1s - loss: 0.1533 - accuracy: 0.95 - ETA: 1s - loss: 0.1539 - accuracy: 0.95 - ETA: 1s - loss: 0.1538 - accuracy: 0.95 - ETA: 1s - loss: 0.1535 - accuracy: 0.95 - ETA: 1s - loss: 0.1536 - accuracy: 0.95 - ETA: 1s - loss: 0.1543 - accuracy: 0.95 - ETA: 1s - loss: 0.1537 - accuracy: 0.95 - ETA: 1s - loss: 0.1546 - accuracy: 0.95 - ETA: 1s - loss: 0.1541 - accuracy: 0.95 - ETA: 1s - loss: 0.1534 - accuracy: 0.95 - ETA: 1s - loss: 0.1542 - accuracy: 0.95 - ETA: 1s - loss: 0.1529 - accuracy: 0.95 - ETA: 0s - loss: 0.1530 - accuracy: 0.95 - ETA: 0s - loss: 0.1537 - accuracy: 0.95 - ETA: 0s - loss: 0.1545 - accuracy: 0.95 - ETA: 0s - loss: 0.1539 - accuracy: 0.95 - ETA: 0s - loss: 0.1544 - accuracy: 0.95 - ETA: 0s - loss: 0.1537 - accuracy: 0.95 - ETA: 0s - loss: 0.1536 - accuracy: 0.95 - ETA: 0s - loss: 0.1528 - accuracy: 0.95 - ETA: 0s - loss: 0.1526 - accuracy: 0.95 - ETA: 0s - loss: 0.1520 - accuracy: 0.95 - ETA: 0s - loss: 0.1522 - accuracy: 0.95 - ETA: 0s - loss: 0.1519 - accuracy: 0.95 - ETA: 0s - loss: 0.1525 - accuracy: 0.95 - ETA: 0s - loss: 0.1526 - accuracy: 0.95 - ETA: 0s - loss: 0.1528 - accuracy: 0.95 - ETA: 0s - loss: 0.1524 - accuracy: 0.95 - ETA: 0s - loss: 0.1520 - accuracy: 0.95 - ETA: 0s - loss: 0.1524 - accuracy: 0.95 - ETA: 0s - loss: 0.1519 - accuracy: 0.95 - 3s 1ms/step - loss: 0.1519 - accuracy: 0.9573 - val_loss: 0.1402 - val_accuracy: 0.9604\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.1193 - accuracy: 0.96 - ETA: 2s - loss: 0.1507 - accuracy: 0.95 - ETA: 2s - loss: 0.1327 - accuracy: 0.96 - ETA: 2s - loss: 0.1287 - accuracy: 0.96 - ETA: 2s - loss: 0.1231 - accuracy: 0.96 - ETA: 2s - loss: 0.1221 - accuracy: 0.96 - ETA: 2s - loss: 0.1262 - accuracy: 0.96 - ETA: 1s - loss: 0.1291 - accuracy: 0.96 - ETA: 1s - loss: 0.1314 - accuracy: 0.96 - ETA: 1s - loss: 0.1306 - accuracy: 0.96 - ETA: 1s - loss: 0.1286 - accuracy: 0.96 - ETA: 1s - loss: 0.1299 - accuracy: 0.96 - ETA: 1s - loss: 0.1305 - accuracy: 0.96 - ETA: 1s - loss: 0.1307 - accuracy: 0.96 - ETA: 1s - loss: 0.1318 - accuracy: 0.96 - ETA: 1s - loss: 0.1319 - accuracy: 0.96 - ETA: 1s - loss: 0.1325 - accuracy: 0.96 - ETA: 1s - loss: 0.1322 - accuracy: 0.96 - ETA: 1s - loss: 0.1308 - accuracy: 0.96 - ETA: 1s - loss: 0.1308 - accuracy: 0.96 - ETA: 1s - loss: 0.1311 - accuracy: 0.96 - ETA: 1s - loss: 0.1310 - accuracy: 0.96 - ETA: 1s - loss: 0.1307 - accuracy: 0.96 - ETA: 1s - loss: 0.1300 - accuracy: 0.96 - ETA: 1s - loss: 0.1305 - accuracy: 0.96 - ETA: 1s - loss: 0.1306 - accuracy: 0.96 - ETA: 1s - loss: 0.1310 - accuracy: 0.96 - ETA: 0s - loss: 0.1311 - accuracy: 0.96 - ETA: 0s - loss: 0.1316 - accuracy: 0.96 - ETA: 0s - loss: 0.1315 - accuracy: 0.96 - ETA: 0s - loss: 0.1320 - accuracy: 0.96 - ETA: 0s - loss: 0.1313 - accuracy: 0.96 - ETA: 0s - loss: 0.1317 - accuracy: 0.96 - ETA: 0s - loss: 0.1322 - accuracy: 0.96 - ETA: 0s - loss: 0.1324 - accuracy: 0.96 - ETA: 0s - loss: 0.1318 - accuracy: 0.96 - ETA: 0s - loss: 0.1314 - accuracy: 0.96 - ETA: 0s - loss: 0.1311 - accuracy: 0.96 - ETA: 0s - loss: 0.1300 - accuracy: 0.96 - ETA: 0s - loss: 0.1297 - accuracy: 0.96 - ETA: 0s - loss: 0.1296 - accuracy: 0.96 - ETA: 0s - loss: 0.1293 - accuracy: 0.96 - ETA: 0s - loss: 0.1289 - accuracy: 0.96 - ETA: 0s - loss: 0.1285 - accuracy: 0.96 - ETA: 0s - loss: 0.1286 - accuracy: 0.96 - ETA: 0s - loss: 0.1282 - accuracy: 0.96 - ETA: 0s - loss: 0.1281 - accuracy: 0.96 - 3s 1ms/step - loss: 0.1284 - accuracy: 0.9644 - val_loss: 0.1253 - val_accuracy: 0.9646\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 1e8e04b3313084ed605bb898c10f92e5</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.9641666611035665</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-learning_rate: 0.0001</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units: 320</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1858/1875 [============================>.] - ETA: 0s - loss: 2.4554 - accuracy: 0.06 - ETA: 1s - loss: 0.7512 - accuracy: 0.75 - ETA: 1s - loss: 0.5681 - accuracy: 0.81 - ETA: 1s - loss: 0.4861 - accuracy: 0.84 - ETA: 1s - loss: 0.4387 - accuracy: 0.85 - ETA: 1s - loss: 0.4139 - accuracy: 0.86 - ETA: 1s - loss: 0.3889 - accuracy: 0.87 - ETA: 1s - loss: 0.3732 - accuracy: 0.88 - ETA: 1s - loss: 0.3592 - accuracy: 0.88 - ETA: 1s - loss: 0.3434 - accuracy: 0.89 - ETA: 1s - loss: 0.3364 - accuracy: 0.89 - ETA: 1s - loss: 0.3245 - accuracy: 0.89 - ETA: 1s - loss: 0.3142 - accuracy: 0.90 - ETA: 1s - loss: 0.3081 - accuracy: 0.90 - ETA: 1s - loss: 0.3017 - accuracy: 0.90 - ETA: 1s - loss: 0.2949 - accuracy: 0.90 - ETA: 1s - loss: 0.2929 - accuracy: 0.91 - ETA: 1s - loss: 0.2894 - accuracy: 0.91 - ETA: 0s - loss: 0.2844 - accuracy: 0.91 - ETA: 0s - loss: 0.2819 - accuracy: 0.91 - ETA: 0s - loss: 0.2804 - accuracy: 0.91 - ETA: 0s - loss: 0.2771 - accuracy: 0.91 - ETA: 0s - loss: 0.2736 - accuracy: 0.91 - ETA: 0s - loss: 0.2705 - accuracy: 0.91 - ETA: 0s - loss: 0.2683 - accuracy: 0.91 - ETA: 0s - loss: 0.2641 - accuracy: 0.92 - ETA: 0s - loss: 0.2612 - accuracy: 0.92 - ETA: 0s - loss: 0.2594 - accuracy: 0.92 - ETA: 0s - loss: 0.2580 - accuracy: 0.92 - ETA: 0s - loss: 0.2562 - accuracy: 0.92 - ETA: 0s - loss: 0.2545 - accuracy: 0.92 - ETA: 0s - loss: 0.2522 - accuracy: 0.92 - ETA: 0s - loss: 0.2503 - accuracy: 0.92 - ETA: 0s - loss: 0.2481 - accuracy: 0.92 - ETA: 0s - loss: 0.2469 - accuracy: 0.92 - ETA: 0s - loss: 0.2457 - accuracy: 0.92 - ETA: 0s - loss: 0.2431 - accuracy: 0.92 - ETA: 0s - loss: 0.2414 - accuracy: 0.9287WARNING:tensorflow:Callbacks method `on_test_batch_begin` is slow compared to the batch time (batch time: 0.0005s vs `on_test_batch_begin` time: 0.0010s). Check your callbacks.\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.2409 - accuracy: 0.9289 - val_loss: 0.1584 - val_accuracy: 0.9542\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.1143 - accuracy: 0.96 - ETA: 1s - loss: 0.1407 - accuracy: 0.95 - ETA: 1s - loss: 0.1384 - accuracy: 0.95 - ETA: 1s - loss: 0.1329 - accuracy: 0.95 - ETA: 1s - loss: 0.1290 - accuracy: 0.95 - ETA: 1s - loss: 0.1349 - accuracy: 0.95 - ETA: 1s - loss: 0.1435 - accuracy: 0.95 - ETA: 1s - loss: 0.1388 - accuracy: 0.95 - ETA: 1s - loss: 0.1384 - accuracy: 0.95 - ETA: 1s - loss: 0.1412 - accuracy: 0.95 - ETA: 1s - loss: 0.1435 - accuracy: 0.95 - ETA: 1s - loss: 0.1417 - accuracy: 0.95 - ETA: 1s - loss: 0.1427 - accuracy: 0.95 - ETA: 1s - loss: 0.1453 - accuracy: 0.95 - ETA: 1s - loss: 0.1461 - accuracy: 0.95 - ETA: 1s - loss: 0.1490 - accuracy: 0.95 - ETA: 1s - loss: 0.1544 - accuracy: 0.95 - ETA: 1s - loss: 0.1578 - accuracy: 0.95 - ETA: 1s - loss: 0.1591 - accuracy: 0.95 - ETA: 0s - loss: 0.1597 - accuracy: 0.95 - ETA: 0s - loss: 0.1593 - accuracy: 0.95 - ETA: 0s - loss: 0.1598 - accuracy: 0.95 - ETA: 0s - loss: 0.1602 - accuracy: 0.95 - ETA: 0s - loss: 0.1605 - accuracy: 0.95 - ETA: 0s - loss: 0.1592 - accuracy: 0.95 - ETA: 0s - loss: 0.1593 - accuracy: 0.95 - ETA: 0s - loss: 0.1582 - accuracy: 0.95 - ETA: 0s - loss: 0.1585 - accuracy: 0.95 - ETA: 0s - loss: 0.1608 - accuracy: 0.95 - ETA: 0s - loss: 0.1614 - accuracy: 0.95 - ETA: 0s - loss: 0.1613 - accuracy: 0.95 - ETA: 0s - loss: 0.1605 - accuracy: 0.95 - ETA: 0s - loss: 0.1600 - accuracy: 0.95 - ETA: 0s - loss: 0.1592 - accuracy: 0.95 - ETA: 0s - loss: 0.1611 - accuracy: 0.95 - ETA: 0s - loss: 0.1619 - accuracy: 0.95 - ETA: 0s - loss: 0.1617 - accuracy: 0.95 - ETA: 0s - loss: 0.1622 - accuracy: 0.95 - 2s 1ms/step - loss: 0.1632 - accuracy: 0.9546 - val_loss: 0.1694 - val_accuracy: 0.9515\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0383 - accuracy: 1.00 - ETA: 1s - loss: 0.1303 - accuracy: 0.96 - ETA: 1s - loss: 0.1248 - accuracy: 0.96 - ETA: 1s - loss: 0.1264 - accuracy: 0.96 - ETA: 1s - loss: 0.1378 - accuracy: 0.96 - ETA: 1s - loss: 0.1475 - accuracy: 0.95 - ETA: 1s - loss: 0.1443 - accuracy: 0.95 - ETA: 1s - loss: 0.1453 - accuracy: 0.95 - ETA: 1s - loss: 0.1454 - accuracy: 0.95 - ETA: 1s - loss: 0.1425 - accuracy: 0.95 - ETA: 1s - loss: 0.1396 - accuracy: 0.95 - ETA: 1s - loss: 0.1421 - accuracy: 0.95 - ETA: 1s - loss: 0.1426 - accuracy: 0.95 - ETA: 1s - loss: 0.1405 - accuracy: 0.95 - ETA: 1s - loss: 0.1403 - accuracy: 0.95 - ETA: 1s - loss: 0.1407 - accuracy: 0.95 - ETA: 1s - loss: 0.1401 - accuracy: 0.95 - ETA: 1s - loss: 0.1402 - accuracy: 0.95 - ETA: 1s - loss: 0.1401 - accuracy: 0.95 - ETA: 0s - loss: 0.1389 - accuracy: 0.95 - ETA: 0s - loss: 0.1388 - accuracy: 0.95 - ETA: 0s - loss: 0.1392 - accuracy: 0.95 - ETA: 0s - loss: 0.1400 - accuracy: 0.95 - ETA: 0s - loss: 0.1390 - accuracy: 0.95 - ETA: 0s - loss: 0.1399 - accuracy: 0.95 - ETA: 0s - loss: 0.1400 - accuracy: 0.95 - ETA: 0s - loss: 0.1411 - accuracy: 0.95 - ETA: 0s - loss: 0.1408 - accuracy: 0.95 - ETA: 0s - loss: 0.1401 - accuracy: 0.95 - ETA: 0s - loss: 0.1411 - accuracy: 0.95 - ETA: 0s - loss: 0.1416 - accuracy: 0.95 - ETA: 0s - loss: 0.1426 - accuracy: 0.95 - ETA: 0s - loss: 0.1424 - accuracy: 0.95 - ETA: 0s - loss: 0.1435 - accuracy: 0.95 - ETA: 0s - loss: 0.1440 - accuracy: 0.95 - ETA: 0s - loss: 0.1447 - accuracy: 0.95 - ETA: 0s - loss: 0.1444 - accuracy: 0.95 - ETA: 0s - loss: 0.1445 - accuracy: 0.95 - ETA: 0s - loss: 0.1441 - accuracy: 0.95 - 2s 1ms/step - loss: 0.1440 - accuracy: 0.9595 - val_loss: 0.1587 - val_accuracy: 0.9599\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0608 - accuracy: 0.96 - ETA: 1s - loss: 0.1166 - accuracy: 0.96 - ETA: 1s - loss: 0.1198 - accuracy: 0.96 - ETA: 1s - loss: 0.1138 - accuracy: 0.96 - ETA: 1s - loss: 0.1128 - accuracy: 0.96 - ETA: 1s - loss: 0.1115 - accuracy: 0.96 - ETA: 1s - loss: 0.1105 - accuracy: 0.96 - ETA: 1s - loss: 0.1133 - accuracy: 0.96 - ETA: 1s - loss: 0.1145 - accuracy: 0.96 - ETA: 1s - loss: 0.1137 - accuracy: 0.96 - ETA: 1s - loss: 0.1168 - accuracy: 0.96 - ETA: 1s - loss: 0.1164 - accuracy: 0.96 - ETA: 1s - loss: 0.1163 - accuracy: 0.96 - ETA: 1s - loss: 0.1145 - accuracy: 0.96 - ETA: 1s - loss: 0.1128 - accuracy: 0.96 - ETA: 1s - loss: 0.1132 - accuracy: 0.96 - ETA: 1s - loss: 0.1147 - accuracy: 0.96 - ETA: 1s - loss: 0.1173 - accuracy: 0.96 - ETA: 1s - loss: 0.1200 - accuracy: 0.96 - ETA: 0s - loss: 0.1220 - accuracy: 0.96 - ETA: 0s - loss: 0.1251 - accuracy: 0.96 - ETA: 0s - loss: 0.1248 - accuracy: 0.96 - ETA: 0s - loss: 0.1256 - accuracy: 0.96 - ETA: 0s - loss: 0.1255 - accuracy: 0.96 - ETA: 0s - loss: 0.1248 - accuracy: 0.96 - ETA: 0s - loss: 0.1257 - accuracy: 0.96 - ETA: 0s - loss: 0.1251 - accuracy: 0.96 - ETA: 0s - loss: 0.1267 - accuracy: 0.96 - ETA: 0s - loss: 0.1291 - accuracy: 0.96 - ETA: 0s - loss: 0.1306 - accuracy: 0.96 - ETA: 0s - loss: 0.1308 - accuracy: 0.96 - ETA: 0s - loss: 0.1322 - accuracy: 0.96 - ETA: 0s - loss: 0.1309 - accuracy: 0.96 - ETA: 0s - loss: 0.1321 - accuracy: 0.96 - ETA: 0s - loss: 0.1323 - accuracy: 0.96 - ETA: 0s - loss: 0.1321 - accuracy: 0.96 - ETA: 0s - loss: 0.1315 - accuracy: 0.96 - ETA: 0s - loss: 0.1318 - accuracy: 0.96 - ETA: 0s - loss: 0.1318 - accuracy: 0.96 - 2s 1ms/step - loss: 0.1318 - accuracy: 0.9647 - val_loss: 0.1668 - val_accuracy: 0.9602\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0042 - accuracy: 1.00 - ETA: 1s - loss: 0.0773 - accuracy: 0.97 - ETA: 1s - loss: 0.0856 - accuracy: 0.97 - ETA: 1s - loss: 0.0846 - accuracy: 0.97 - ETA: 1s - loss: 0.0879 - accuracy: 0.97 - ETA: 1s - loss: 0.0904 - accuracy: 0.97 - ETA: 1s - loss: 0.0928 - accuracy: 0.97 - ETA: 1s - loss: 0.0963 - accuracy: 0.97 - ETA: 1s - loss: 0.0984 - accuracy: 0.97 - ETA: 1s - loss: 0.0996 - accuracy: 0.97 - ETA: 1s - loss: 0.1023 - accuracy: 0.97 - ETA: 1s - loss: 0.1037 - accuracy: 0.97 - ETA: 1s - loss: 0.1048 - accuracy: 0.97 - ETA: 1s - loss: 0.1096 - accuracy: 0.97 - ETA: 1s - loss: 0.1105 - accuracy: 0.97 - ETA: 1s - loss: 0.1116 - accuracy: 0.97 - ETA: 1s - loss: 0.1124 - accuracy: 0.96 - ETA: 1s - loss: 0.1127 - accuracy: 0.96 - ETA: 1s - loss: 0.1151 - accuracy: 0.96 - ETA: 0s - loss: 0.1141 - accuracy: 0.96 - ETA: 0s - loss: 0.1165 - accuracy: 0.96 - ETA: 0s - loss: 0.1173 - accuracy: 0.96 - ETA: 0s - loss: 0.1180 - accuracy: 0.96 - ETA: 0s - loss: 0.1184 - accuracy: 0.96 - ETA: 0s - loss: 0.1186 - accuracy: 0.96 - ETA: 0s - loss: 0.1198 - accuracy: 0.96 - ETA: 0s - loss: 0.1202 - accuracy: 0.96 - ETA: 0s - loss: 0.1198 - accuracy: 0.96 - ETA: 0s - loss: 0.1203 - accuracy: 0.96 - ETA: 0s - loss: 0.1202 - accuracy: 0.96 - ETA: 0s - loss: 0.1208 - accuracy: 0.96 - ETA: 0s - loss: 0.1226 - accuracy: 0.96 - ETA: 0s - loss: 0.1233 - accuracy: 0.96 - ETA: 0s - loss: 0.1241 - accuracy: 0.96 - ETA: 0s - loss: 0.1249 - accuracy: 0.96 - ETA: 0s - loss: 0.1247 - accuracy: 0.96 - ETA: 0s - loss: 0.1256 - accuracy: 0.96 - ETA: 0s - loss: 0.1245 - accuracy: 0.96 - ETA: 0s - loss: 0.1249 - accuracy: 0.96 - 2s 1ms/step - loss: 0.1258 - accuracy: 0.9679 - val_loss: 0.2046 - val_accuracy: 0.9570\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 2.2481 - accuracy: 0.21 - ETA: 1s - loss: 0.7260 - accuracy: 0.77 - ETA: 1s - loss: 0.5719 - accuracy: 0.82 - ETA: 1s - loss: 0.4914 - accuracy: 0.85 - ETA: 1s - loss: 0.4453 - accuracy: 0.86 - ETA: 1s - loss: 0.4090 - accuracy: 0.87 - ETA: 1s - loss: 0.3856 - accuracy: 0.88 - ETA: 1s - loss: 0.3687 - accuracy: 0.88 - ETA: 1s - loss: 0.3580 - accuracy: 0.88 - ETA: 1s - loss: 0.3523 - accuracy: 0.89 - ETA: 1s - loss: 0.3415 - accuracy: 0.89 - ETA: 1s - loss: 0.3367 - accuracy: 0.89 - ETA: 1s - loss: 0.3295 - accuracy: 0.90 - ETA: 1s - loss: 0.3219 - accuracy: 0.90 - ETA: 1s - loss: 0.3147 - accuracy: 0.90 - ETA: 1s - loss: 0.3074 - accuracy: 0.90 - ETA: 1s - loss: 0.3013 - accuracy: 0.91 - ETA: 1s - loss: 0.2956 - accuracy: 0.91 - ETA: 1s - loss: 0.2910 - accuracy: 0.91 - ETA: 0s - loss: 0.2889 - accuracy: 0.91 - ETA: 0s - loss: 0.2845 - accuracy: 0.91 - ETA: 0s - loss: 0.2809 - accuracy: 0.91 - ETA: 0s - loss: 0.2768 - accuracy: 0.91 - ETA: 0s - loss: 0.2737 - accuracy: 0.91 - ETA: 0s - loss: 0.2704 - accuracy: 0.91 - ETA: 0s - loss: 0.2670 - accuracy: 0.92 - ETA: 0s - loss: 0.2642 - accuracy: 0.92 - ETA: 0s - loss: 0.2613 - accuracy: 0.92 - ETA: 0s - loss: 0.2592 - accuracy: 0.92 - ETA: 0s - loss: 0.2571 - accuracy: 0.92 - ETA: 0s - loss: 0.2538 - accuracy: 0.92 - ETA: 0s - loss: 0.2521 - accuracy: 0.92 - ETA: 0s - loss: 0.2505 - accuracy: 0.92 - ETA: 0s - loss: 0.2488 - accuracy: 0.92 - ETA: 0s - loss: 0.2474 - accuracy: 0.92 - ETA: 0s - loss: 0.2472 - accuracy: 0.92 - ETA: 0s - loss: 0.2451 - accuracy: 0.92 - ETA: 0s - loss: 0.2426 - accuracy: 0.92 - 2s 1ms/step - loss: 0.2415 - accuracy: 0.9289 - val_loss: 0.1729 - val_accuracy: 0.9531\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.1236 - accuracy: 0.93 - ETA: 1s - loss: 0.1838 - accuracy: 0.94 - ETA: 1s - loss: 0.1857 - accuracy: 0.94 - ETA: 1s - loss: 0.1789 - accuracy: 0.94 - ETA: 1s - loss: 0.1643 - accuracy: 0.95 - ETA: 1s - loss: 0.1584 - accuracy: 0.95 - ETA: 1s - loss: 0.1566 - accuracy: 0.95 - ETA: 1s - loss: 0.1597 - accuracy: 0.95 - ETA: 1s - loss: 0.1582 - accuracy: 0.95 - ETA: 1s - loss: 0.1551 - accuracy: 0.95 - ETA: 1s - loss: 0.1585 - accuracy: 0.95 - ETA: 1s - loss: 0.1602 - accuracy: 0.95 - ETA: 1s - loss: 0.1630 - accuracy: 0.95 - ETA: 1s - loss: 0.1641 - accuracy: 0.95 - ETA: 1s - loss: 0.1661 - accuracy: 0.95 - ETA: 1s - loss: 0.1657 - accuracy: 0.95 - ETA: 1s - loss: 0.1653 - accuracy: 0.95 - ETA: 1s - loss: 0.1642 - accuracy: 0.95 - ETA: 1s - loss: 0.1635 - accuracy: 0.95 - ETA: 0s - loss: 0.1631 - accuracy: 0.95 - ETA: 0s - loss: 0.1635 - accuracy: 0.95 - ETA: 0s - loss: 0.1644 - accuracy: 0.95 - ETA: 0s - loss: 0.1640 - accuracy: 0.95 - ETA: 0s - loss: 0.1667 - accuracy: 0.95 - ETA: 0s - loss: 0.1670 - accuracy: 0.95 - ETA: 0s - loss: 0.1668 - accuracy: 0.95 - ETA: 0s - loss: 0.1669 - accuracy: 0.95 - ETA: 0s - loss: 0.1668 - accuracy: 0.95 - ETA: 0s - loss: 0.1660 - accuracy: 0.95 - ETA: 0s - loss: 0.1663 - accuracy: 0.95 - ETA: 0s - loss: 0.1659 - accuracy: 0.95 - ETA: 0s - loss: 0.1676 - accuracy: 0.95 - ETA: 0s - loss: 0.1669 - accuracy: 0.95 - ETA: 0s - loss: 0.1669 - accuracy: 0.95 - ETA: 0s - loss: 0.1674 - accuracy: 0.95 - ETA: 0s - loss: 0.1679 - accuracy: 0.95 - ETA: 0s - loss: 0.1678 - accuracy: 0.95 - ETA: 0s - loss: 0.1673 - accuracy: 0.95 - ETA: 0s - loss: 0.1678 - accuracy: 0.95 - 2s 1ms/step - loss: 0.1676 - accuracy: 0.9545 - val_loss: 0.1526 - val_accuracy: 0.9594\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0339 - accuracy: 1.00 - ETA: 1s - loss: 0.0928 - accuracy: 0.96 - ETA: 1s - loss: 0.1109 - accuracy: 0.96 - ETA: 1s - loss: 0.1121 - accuracy: 0.97 - ETA: 1s - loss: 0.1110 - accuracy: 0.96 - ETA: 1s - loss: 0.1123 - accuracy: 0.96 - ETA: 1s - loss: 0.1074 - accuracy: 0.96 - ETA: 1s - loss: 0.1045 - accuracy: 0.96 - ETA: 1s - loss: 0.1081 - accuracy: 0.96 - ETA: 1s - loss: 0.1126 - accuracy: 0.96 - ETA: 1s - loss: 0.1140 - accuracy: 0.96 - ETA: 1s - loss: 0.1142 - accuracy: 0.96 - ETA: 1s - loss: 0.1175 - accuracy: 0.96 - ETA: 1s - loss: 0.1189 - accuracy: 0.96 - ETA: 1s - loss: 0.1191 - accuracy: 0.96 - ETA: 1s - loss: 0.1208 - accuracy: 0.96 - ETA: 1s - loss: 0.1247 - accuracy: 0.96 - ETA: 1s - loss: 0.1245 - accuracy: 0.96 - ETA: 1s - loss: 0.1294 - accuracy: 0.96 - ETA: 0s - loss: 0.1290 - accuracy: 0.96 - ETA: 0s - loss: 0.1300 - accuracy: 0.96 - ETA: 0s - loss: 0.1294 - accuracy: 0.96 - ETA: 0s - loss: 0.1303 - accuracy: 0.96 - ETA: 0s - loss: 0.1323 - accuracy: 0.96 - ETA: 0s - loss: 0.1326 - accuracy: 0.96 - ETA: 0s - loss: 0.1342 - accuracy: 0.96 - ETA: 0s - loss: 0.1349 - accuracy: 0.96 - ETA: 0s - loss: 0.1354 - accuracy: 0.96 - ETA: 0s - loss: 0.1359 - accuracy: 0.96 - ETA: 0s - loss: 0.1369 - accuracy: 0.96 - ETA: 0s - loss: 0.1379 - accuracy: 0.96 - ETA: 0s - loss: 0.1379 - accuracy: 0.96 - ETA: 0s - loss: 0.1382 - accuracy: 0.96 - ETA: 0s - loss: 0.1390 - accuracy: 0.96 - ETA: 0s - loss: 0.1394 - accuracy: 0.96 - ETA: 0s - loss: 0.1410 - accuracy: 0.96 - ETA: 0s - loss: 0.1425 - accuracy: 0.96 - ETA: 0s - loss: 0.1434 - accuracy: 0.96 - ETA: 0s - loss: 0.1445 - accuracy: 0.96 - 2s 1ms/step - loss: 0.1441 - accuracy: 0.9616 - val_loss: 0.1875 - val_accuracy: 0.9535\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.2119 - accuracy: 0.93 - ETA: 1s - loss: 0.0927 - accuracy: 0.97 - ETA: 1s - loss: 0.0884 - accuracy: 0.97 - ETA: 1s - loss: 0.1091 - accuracy: 0.97 - ETA: 1s - loss: 0.1091 - accuracy: 0.97 - ETA: 1s - loss: 0.1109 - accuracy: 0.97 - ETA: 1s - loss: 0.1099 - accuracy: 0.97 - ETA: 1s - loss: 0.1125 - accuracy: 0.97 - ETA: 1s - loss: 0.1131 - accuracy: 0.97 - ETA: 1s - loss: 0.1166 - accuracy: 0.96 - ETA: 1s - loss: 0.1152 - accuracy: 0.97 - ETA: 1s - loss: 0.1171 - accuracy: 0.96 - ETA: 1s - loss: 0.1242 - accuracy: 0.96 - ETA: 1s - loss: 0.1232 - accuracy: 0.96 - ETA: 1s - loss: 0.1222 - accuracy: 0.96 - ETA: 1s - loss: 0.1245 - accuracy: 0.96 - ETA: 1s - loss: 0.1224 - accuracy: 0.96 - ETA: 1s - loss: 0.1211 - accuracy: 0.96 - ETA: 1s - loss: 0.1207 - accuracy: 0.96 - ETA: 0s - loss: 0.1238 - accuracy: 0.96 - ETA: 0s - loss: 0.1238 - accuracy: 0.96 - ETA: 0s - loss: 0.1264 - accuracy: 0.96 - ETA: 0s - loss: 0.1264 - accuracy: 0.96 - ETA: 0s - loss: 0.1270 - accuracy: 0.96 - ETA: 0s - loss: 0.1256 - accuracy: 0.96 - ETA: 0s - loss: 0.1250 - accuracy: 0.96 - ETA: 0s - loss: 0.1250 - accuracy: 0.96 - ETA: 0s - loss: 0.1258 - accuracy: 0.96 - ETA: 0s - loss: 0.1258 - accuracy: 0.96 - ETA: 0s - loss: 0.1268 - accuracy: 0.96 - ETA: 0s - loss: 0.1265 - accuracy: 0.96 - ETA: 0s - loss: 0.1275 - accuracy: 0.96 - ETA: 0s - loss: 0.1284 - accuracy: 0.96 - ETA: 0s - loss: 0.1291 - accuracy: 0.96 - ETA: 0s - loss: 0.1304 - accuracy: 0.96 - ETA: 0s - loss: 0.1308 - accuracy: 0.96 - ETA: 0s - loss: 0.1329 - accuracy: 0.96 - ETA: 0s - loss: 0.1331 - accuracy: 0.96 - ETA: 0s - loss: 0.1324 - accuracy: 0.96 - 2s 1ms/step - loss: 0.1323 - accuracy: 0.9659 - val_loss: 0.1694 - val_accuracy: 0.9638\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.2035 - accuracy: 0.96 - ETA: 1s - loss: 0.1155 - accuracy: 0.96 - ETA: 1s - loss: 0.1027 - accuracy: 0.96 - ETA: 1s - loss: 0.1161 - accuracy: 0.96 - ETA: 1s - loss: 0.1099 - accuracy: 0.96 - ETA: 1s - loss: 0.1065 - accuracy: 0.97 - ETA: 1s - loss: 0.1071 - accuracy: 0.97 - ETA: 1s - loss: 0.1101 - accuracy: 0.96 - ETA: 1s - loss: 0.1114 - accuracy: 0.96 - ETA: 1s - loss: 0.1164 - accuracy: 0.96 - ETA: 1s - loss: 0.1128 - accuracy: 0.97 - ETA: 1s - loss: 0.1092 - accuracy: 0.97 - ETA: 1s - loss: 0.1105 - accuracy: 0.97 - ETA: 1s - loss: 0.1089 - accuracy: 0.97 - ETA: 1s - loss: 0.1106 - accuracy: 0.97 - ETA: 1s - loss: 0.1112 - accuracy: 0.97 - ETA: 1s - loss: 0.1109 - accuracy: 0.97 - ETA: 1s - loss: 0.1106 - accuracy: 0.97 - ETA: 1s - loss: 0.1135 - accuracy: 0.97 - ETA: 0s - loss: 0.1155 - accuracy: 0.97 - ETA: 0s - loss: 0.1164 - accuracy: 0.97 - ETA: 0s - loss: 0.1170 - accuracy: 0.97 - ETA: 0s - loss: 0.1175 - accuracy: 0.97 - ETA: 0s - loss: 0.1175 - accuracy: 0.97 - ETA: 0s - loss: 0.1183 - accuracy: 0.96 - ETA: 0s - loss: 0.1185 - accuracy: 0.96 - ETA: 0s - loss: 0.1187 - accuracy: 0.96 - ETA: 0s - loss: 0.1194 - accuracy: 0.96 - ETA: 0s - loss: 0.1185 - accuracy: 0.96 - ETA: 0s - loss: 0.1198 - accuracy: 0.96 - ETA: 0s - loss: 0.1201 - accuracy: 0.96 - ETA: 0s - loss: 0.1210 - accuracy: 0.96 - ETA: 0s - loss: 0.1206 - accuracy: 0.96 - ETA: 0s - loss: 0.1225 - accuracy: 0.96 - ETA: 0s - loss: 0.1241 - accuracy: 0.96 - ETA: 0s - loss: 0.1246 - accuracy: 0.96 - ETA: 0s - loss: 0.1252 - accuracy: 0.96 - ETA: 0s - loss: 0.1259 - accuracy: 0.96 - 2s 1ms/step - loss: 0.1257 - accuracy: 0.9681 - val_loss: 0.2123 - val_accuracy: 0.9576\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 2.2319 - accuracy: 0.15 - ETA: 1s - loss: 0.7172 - accuracy: 0.76 - ETA: 1s - loss: 0.5476 - accuracy: 0.82 - ETA: 2s - loss: 0.4980 - accuracy: 0.84 - ETA: 1s - loss: 0.4526 - accuracy: 0.85 - ETA: 1s - loss: 0.4161 - accuracy: 0.87 - ETA: 1s - loss: 0.3958 - accuracy: 0.87 - ETA: 1s - loss: 0.3772 - accuracy: 0.88 - ETA: 1s - loss: 0.3590 - accuracy: 0.88 - ETA: 1s - loss: 0.3455 - accuracy: 0.89 - ETA: 1s - loss: 0.3329 - accuracy: 0.89 - ETA: 1s - loss: 0.3238 - accuracy: 0.90 - ETA: 1s - loss: 0.3120 - accuracy: 0.90 - ETA: 1s - loss: 0.3107 - accuracy: 0.90 - ETA: 1s - loss: 0.3030 - accuracy: 0.90 - ETA: 1s - loss: 0.2981 - accuracy: 0.91 - ETA: 1s - loss: 0.2920 - accuracy: 0.91 - ETA: 1s - loss: 0.2896 - accuracy: 0.91 - ETA: 1s - loss: 0.2853 - accuracy: 0.91 - ETA: 0s - loss: 0.2812 - accuracy: 0.91 - ETA: 0s - loss: 0.2794 - accuracy: 0.91 - ETA: 0s - loss: 0.2763 - accuracy: 0.91 - ETA: 0s - loss: 0.2718 - accuracy: 0.91 - ETA: 0s - loss: 0.2679 - accuracy: 0.92 - ETA: 0s - loss: 0.2660 - accuracy: 0.92 - ETA: 0s - loss: 0.2631 - accuracy: 0.92 - ETA: 0s - loss: 0.2600 - accuracy: 0.92 - ETA: 0s - loss: 0.2573 - accuracy: 0.92 - ETA: 0s - loss: 0.2537 - accuracy: 0.92 - ETA: 0s - loss: 0.2519 - accuracy: 0.92 - ETA: 0s - loss: 0.2498 - accuracy: 0.92 - ETA: 0s - loss: 0.2478 - accuracy: 0.92 - ETA: 0s - loss: 0.2465 - accuracy: 0.92 - ETA: 0s - loss: 0.2476 - accuracy: 0.92 - ETA: 0s - loss: 0.2466 - accuracy: 0.92 - ETA: 0s - loss: 0.2461 - accuracy: 0.92 - ETA: 0s - loss: 0.2444 - accuracy: 0.92 - ETA: 0s - loss: 0.2422 - accuracy: 0.92 - 2s 1ms/step - loss: 0.2402 - accuracy: 0.9300 - val_loss: 0.1556 - val_accuracy: 0.9568\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0040 - accuracy: 1.00 - ETA: 1s - loss: 0.1276 - accuracy: 0.96 - ETA: 1s - loss: 0.1416 - accuracy: 0.95 - ETA: 1s - loss: 0.1426 - accuracy: 0.95 - ETA: 1s - loss: 0.1506 - accuracy: 0.95 - ETA: 1s - loss: 0.1470 - accuracy: 0.95 - ETA: 1s - loss: 0.1510 - accuracy: 0.95 - ETA: 1s - loss: 0.1518 - accuracy: 0.95 - ETA: 1s - loss: 0.1561 - accuracy: 0.95 - ETA: 1s - loss: 0.1604 - accuracy: 0.95 - ETA: 1s - loss: 0.1610 - accuracy: 0.95 - ETA: 1s - loss: 0.1637 - accuracy: 0.95 - ETA: 1s - loss: 0.1602 - accuracy: 0.95 - ETA: 1s - loss: 0.1601 - accuracy: 0.95 - ETA: 1s - loss: 0.1618 - accuracy: 0.95 - ETA: 1s - loss: 0.1668 - accuracy: 0.95 - ETA: 1s - loss: 0.1696 - accuracy: 0.95 - ETA: 1s - loss: 0.1691 - accuracy: 0.95 - ETA: 0s - loss: 0.1704 - accuracy: 0.95 - ETA: 0s - loss: 0.1708 - accuracy: 0.95 - ETA: 0s - loss: 0.1697 - accuracy: 0.95 - ETA: 0s - loss: 0.1691 - accuracy: 0.95 - ETA: 0s - loss: 0.1688 - accuracy: 0.95 - ETA: 0s - loss: 0.1678 - accuracy: 0.95 - ETA: 0s - loss: 0.1673 - accuracy: 0.95 - ETA: 0s - loss: 0.1669 - accuracy: 0.95 - ETA: 0s - loss: 0.1674 - accuracy: 0.95 - ETA: 0s - loss: 0.1677 - accuracy: 0.95 - ETA: 0s - loss: 0.1678 - accuracy: 0.95 - ETA: 0s - loss: 0.1674 - accuracy: 0.95 - ETA: 0s - loss: 0.1664 - accuracy: 0.95 - ETA: 0s - loss: 0.1681 - accuracy: 0.95 - ETA: 0s - loss: 0.1679 - accuracy: 0.95 - ETA: 0s - loss: 0.1675 - accuracy: 0.95 - ETA: 0s - loss: 0.1676 - accuracy: 0.95 - ETA: 0s - loss: 0.1680 - accuracy: 0.95 - ETA: 0s - loss: 0.1681 - accuracy: 0.95 - ETA: 0s - loss: 0.1682 - accuracy: 0.95 - 2s 1ms/step - loss: 0.1670 - accuracy: 0.9541 - val_loss: 0.1572 - val_accuracy: 0.9598\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0489 - accuracy: 0.96 - ETA: 1s - loss: 0.1357 - accuracy: 0.96 - ETA: 1s - loss: 0.1339 - accuracy: 0.96 - ETA: 1s - loss: 0.1292 - accuracy: 0.96 - ETA: 1s - loss: 0.1284 - accuracy: 0.96 - ETA: 1s - loss: 0.1280 - accuracy: 0.96 - ETA: 1s - loss: 0.1279 - accuracy: 0.96 - ETA: 1s - loss: 0.1258 - accuracy: 0.96 - ETA: 1s - loss: 0.1234 - accuracy: 0.96 - ETA: 1s - loss: 0.1271 - accuracy: 0.96 - ETA: 1s - loss: 0.1307 - accuracy: 0.96 - ETA: 1s - loss: 0.1303 - accuracy: 0.96 - ETA: 1s - loss: 0.1341 - accuracy: 0.96 - ETA: 1s - loss: 0.1337 - accuracy: 0.96 - ETA: 1s - loss: 0.1298 - accuracy: 0.96 - ETA: 1s - loss: 0.1297 - accuracy: 0.96 - ETA: 1s - loss: 0.1313 - accuracy: 0.96 - ETA: 1s - loss: 0.1329 - accuracy: 0.96 - ETA: 0s - loss: 0.1349 - accuracy: 0.96 - ETA: 0s - loss: 0.1379 - accuracy: 0.96 - ETA: 0s - loss: 0.1390 - accuracy: 0.96 - ETA: 0s - loss: 0.1391 - accuracy: 0.96 - ETA: 0s - loss: 0.1395 - accuracy: 0.96 - ETA: 0s - loss: 0.1392 - accuracy: 0.96 - ETA: 0s - loss: 0.1391 - accuracy: 0.96 - ETA: 0s - loss: 0.1390 - accuracy: 0.96 - ETA: 0s - loss: 0.1406 - accuracy: 0.96 - ETA: 0s - loss: 0.1412 - accuracy: 0.96 - ETA: 0s - loss: 0.1402 - accuracy: 0.96 - ETA: 0s - loss: 0.1405 - accuracy: 0.96 - ETA: 0s - loss: 0.1419 - accuracy: 0.96 - ETA: 0s - loss: 0.1439 - accuracy: 0.96 - ETA: 0s - loss: 0.1431 - accuracy: 0.96 - ETA: 0s - loss: 0.1434 - accuracy: 0.96 - ETA: 0s - loss: 0.1430 - accuracy: 0.96 - ETA: 0s - loss: 0.1424 - accuracy: 0.96 - ETA: 0s - loss: 0.1439 - accuracy: 0.96 - ETA: 0s - loss: 0.1446 - accuracy: 0.96 - 2s 1ms/step - loss: 0.1454 - accuracy: 0.9612 - val_loss: 0.2040 - val_accuracy: 0.9533\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.1497 - accuracy: 0.93 - ETA: 1s - loss: 0.1248 - accuracy: 0.96 - ETA: 1s - loss: 0.1111 - accuracy: 0.97 - ETA: 1s - loss: 0.1073 - accuracy: 0.97 - ETA: 1s - loss: 0.1069 - accuracy: 0.97 - ETA: 1s - loss: 0.1122 - accuracy: 0.97 - ETA: 1s - loss: 0.1182 - accuracy: 0.96 - ETA: 1s - loss: 0.1198 - accuracy: 0.96 - ETA: 1s - loss: 0.1217 - accuracy: 0.96 - ETA: 1s - loss: 0.1201 - accuracy: 0.96 - ETA: 1s - loss: 0.1182 - accuracy: 0.96 - ETA: 1s - loss: 0.1193 - accuracy: 0.96 - ETA: 1s - loss: 0.1196 - accuracy: 0.96 - ETA: 1s - loss: 0.1224 - accuracy: 0.96 - ETA: 1s - loss: 0.1222 - accuracy: 0.96 - ETA: 1s - loss: 0.1250 - accuracy: 0.96 - ETA: 1s - loss: 0.1257 - accuracy: 0.96 - ETA: 1s - loss: 0.1268 - accuracy: 0.96 - ETA: 1s - loss: 0.1263 - accuracy: 0.96 - ETA: 0s - loss: 0.1267 - accuracy: 0.96 - ETA: 0s - loss: 0.1289 - accuracy: 0.96 - ETA: 0s - loss: 0.1304 - accuracy: 0.96 - ETA: 0s - loss: 0.1308 - accuracy: 0.96 - ETA: 0s - loss: 0.1325 - accuracy: 0.96 - ETA: 0s - loss: 0.1329 - accuracy: 0.96 - ETA: 0s - loss: 0.1327 - accuracy: 0.96 - ETA: 0s - loss: 0.1346 - accuracy: 0.96 - ETA: 0s - loss: 0.1333 - accuracy: 0.96 - ETA: 0s - loss: 0.1331 - accuracy: 0.96 - ETA: 0s - loss: 0.1338 - accuracy: 0.96 - ETA: 0s - loss: 0.1338 - accuracy: 0.96 - ETA: 0s - loss: 0.1344 - accuracy: 0.96 - ETA: 0s - loss: 0.1349 - accuracy: 0.96 - ETA: 0s - loss: 0.1355 - accuracy: 0.96 - ETA: 0s - loss: 0.1366 - accuracy: 0.96 - ETA: 0s - loss: 0.1363 - accuracy: 0.96 - ETA: 0s - loss: 0.1361 - accuracy: 0.96 - ETA: 0s - loss: 0.1367 - accuracy: 0.96 - 2s 1ms/step - loss: 0.1369 - accuracy: 0.9647 - val_loss: 0.1823 - val_accuracy: 0.9590\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0919 - accuracy: 0.96 - ETA: 1s - loss: 0.1112 - accuracy: 0.96 - ETA: 1s - loss: 0.1050 - accuracy: 0.96 - ETA: 1s - loss: 0.1085 - accuracy: 0.96 - ETA: 1s - loss: 0.1092 - accuracy: 0.96 - ETA: 1s - loss: 0.1132 - accuracy: 0.96 - ETA: 1s - loss: 0.1081 - accuracy: 0.96 - ETA: 1s - loss: 0.1071 - accuracy: 0.97 - ETA: 1s - loss: 0.1085 - accuracy: 0.96 - ETA: 1s - loss: 0.1098 - accuracy: 0.96 - ETA: 1s - loss: 0.1120 - accuracy: 0.96 - ETA: 1s - loss: 0.1141 - accuracy: 0.96 - ETA: 1s - loss: 0.1130 - accuracy: 0.96 - ETA: 1s - loss: 0.1122 - accuracy: 0.96 - ETA: 1s - loss: 0.1142 - accuracy: 0.96 - ETA: 1s - loss: 0.1124 - accuracy: 0.97 - ETA: 1s - loss: 0.1129 - accuracy: 0.96 - ETA: 1s - loss: 0.1156 - accuracy: 0.96 - ETA: 0s - loss: 0.1167 - accuracy: 0.96 - ETA: 0s - loss: 0.1170 - accuracy: 0.96 - ETA: 0s - loss: 0.1198 - accuracy: 0.96 - ETA: 0s - loss: 0.1196 - accuracy: 0.96 - ETA: 0s - loss: 0.1182 - accuracy: 0.96 - ETA: 0s - loss: 0.1177 - accuracy: 0.96 - ETA: 0s - loss: 0.1181 - accuracy: 0.96 - ETA: 0s - loss: 0.1182 - accuracy: 0.96 - ETA: 0s - loss: 0.1189 - accuracy: 0.96 - ETA: 0s - loss: 0.1192 - accuracy: 0.96 - ETA: 0s - loss: 0.1193 - accuracy: 0.96 - ETA: 0s - loss: 0.1203 - accuracy: 0.96 - ETA: 0s - loss: 0.1200 - accuracy: 0.96 - ETA: 0s - loss: 0.1210 - accuracy: 0.96 - ETA: 0s - loss: 0.1201 - accuracy: 0.96 - ETA: 0s - loss: 0.1205 - accuracy: 0.96 - ETA: 0s - loss: 0.1209 - accuracy: 0.96 - ETA: 0s - loss: 0.1216 - accuracy: 0.96 - ETA: 0s - loss: 0.1217 - accuracy: 0.96 - ETA: 0s - loss: 0.1232 - accuracy: 0.96 - 2s 1ms/step - loss: 0.1223 - accuracy: 0.9692 - val_loss: 0.1871 - val_accuracy: 0.9578\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 06f02a198a90afd6da6c4c8acbe3c22d</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.9612666765848795</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-learning_rate: 0.01</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units: 160</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 2.4724 - accuracy: 0.03 - ETA: 3s - loss: 2.2182 - accuracy: 0.19 - ETA: 3s - loss: 2.0285 - accuracy: 0.36 - ETA: 3s - loss: 1.8702 - accuracy: 0.47 - ETA: 3s - loss: 1.7244 - accuracy: 0.54 - ETA: 2s - loss: 1.5939 - accuracy: 0.60 - ETA: 2s - loss: 1.4880 - accuracy: 0.63 - ETA: 2s - loss: 1.3881 - accuracy: 0.66 - ETA: 2s - loss: 1.3124 - accuracy: 0.68 - ETA: 2s - loss: 1.2519 - accuracy: 0.70 - ETA: 2s - loss: 1.1951 - accuracy: 0.71 - ETA: 2s - loss: 1.1413 - accuracy: 0.73 - ETA: 2s - loss: 1.0908 - accuracy: 0.74 - ETA: 2s - loss: 1.0472 - accuracy: 0.75 - ETA: 2s - loss: 1.0110 - accuracy: 0.76 - ETA: 2s - loss: 0.9746 - accuracy: 0.77 - ETA: 2s - loss: 0.9438 - accuracy: 0.77 - ETA: 2s - loss: 0.9184 - accuracy: 0.78 - ETA: 2s - loss: 0.8979 - accuracy: 0.78 - ETA: 2s - loss: 0.8755 - accuracy: 0.79 - ETA: 2s - loss: 0.8542 - accuracy: 0.79 - ETA: 2s - loss: 0.8369 - accuracy: 0.80 - ETA: 2s - loss: 0.8192 - accuracy: 0.80 - ETA: 2s - loss: 0.8041 - accuracy: 0.80 - ETA: 2s - loss: 0.7864 - accuracy: 0.81 - ETA: 2s - loss: 0.7725 - accuracy: 0.81 - ETA: 2s - loss: 0.7577 - accuracy: 0.81 - ETA: 1s - loss: 0.7442 - accuracy: 0.82 - ETA: 1s - loss: 0.7311 - accuracy: 0.82 - ETA: 1s - loss: 0.7195 - accuracy: 0.82 - ETA: 1s - loss: 0.7068 - accuracy: 0.82 - ETA: 1s - loss: 0.6949 - accuracy: 0.83 - ETA: 1s - loss: 0.6836 - accuracy: 0.83 - ETA: 1s - loss: 0.6740 - accuracy: 0.83 - ETA: 1s - loss: 0.6652 - accuracy: 0.83 - ETA: 1s - loss: 0.6561 - accuracy: 0.83 - ETA: 1s - loss: 0.6463 - accuracy: 0.84 - ETA: 1s - loss: 0.6384 - accuracy: 0.84 - ETA: 1s - loss: 0.6312 - accuracy: 0.84 - ETA: 1s - loss: 0.6242 - accuracy: 0.84 - ETA: 1s - loss: 0.6172 - accuracy: 0.84 - ETA: 1s - loss: 0.6108 - accuracy: 0.84 - ETA: 1s - loss: 0.6056 - accuracy: 0.85 - ETA: 1s - loss: 0.5993 - accuracy: 0.85 - ETA: 1s - loss: 0.5930 - accuracy: 0.85 - ETA: 1s - loss: 0.5868 - accuracy: 0.85 - ETA: 1s - loss: 0.5812 - accuracy: 0.85 - ETA: 0s - loss: 0.5756 - accuracy: 0.85 - ETA: 0s - loss: 0.5710 - accuracy: 0.85 - ETA: 0s - loss: 0.5657 - accuracy: 0.86 - ETA: 0s - loss: 0.5602 - accuracy: 0.86 - ETA: 0s - loss: 0.5550 - accuracy: 0.86 - ETA: 0s - loss: 0.5505 - accuracy: 0.86 - ETA: 0s - loss: 0.5451 - accuracy: 0.86 - ETA: 0s - loss: 0.5414 - accuracy: 0.86 - ETA: 0s - loss: 0.5369 - accuracy: 0.86 - ETA: 0s - loss: 0.5326 - accuracy: 0.86 - ETA: 0s - loss: 0.5284 - accuracy: 0.86 - ETA: 0s - loss: 0.5242 - accuracy: 0.86 - ETA: 0s - loss: 0.5201 - accuracy: 0.86 - ETA: 0s - loss: 0.5156 - accuracy: 0.87 - ETA: 0s - loss: 0.5119 - accuracy: 0.87 - ETA: 0s - loss: 0.5088 - accuracy: 0.87 - ETA: 0s - loss: 0.5053 - accuracy: 0.87 - ETA: 0s - loss: 0.5022 - accuracy: 0.87 - ETA: 0s - loss: 0.4991 - accuracy: 0.87 - ETA: 0s - loss: 0.4959 - accuracy: 0.8753WARNING:tensorflow:Callbacks method `on_test_batch_begin` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_begin` time: 0.0010s). Check your callbacks.\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.4954 - accuracy: 0.8754 - val_loss: 0.2658 - val_accuracy: 0.9269\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.4266 - accuracy: 0.87 - ETA: 3s - loss: 0.2952 - accuracy: 0.91 - ETA: 3s - loss: 0.2659 - accuracy: 0.92 - ETA: 3s - loss: 0.2464 - accuracy: 0.93 - ETA: 3s - loss: 0.2570 - accuracy: 0.93 - ETA: 3s - loss: 0.2654 - accuracy: 0.92 - ETA: 3s - loss: 0.2693 - accuracy: 0.92 - ETA: 3s - loss: 0.2646 - accuracy: 0.92 - ETA: 3s - loss: 0.2673 - accuracy: 0.92 - ETA: 2s - loss: 0.2679 - accuracy: 0.92 - ETA: 2s - loss: 0.2672 - accuracy: 0.92 - ETA: 2s - loss: 0.2673 - accuracy: 0.92 - ETA: 2s - loss: 0.2654 - accuracy: 0.92 - ETA: 2s - loss: 0.2665 - accuracy: 0.92 - ETA: 2s - loss: 0.2646 - accuracy: 0.92 - ETA: 2s - loss: 0.2640 - accuracy: 0.92 - ETA: 2s - loss: 0.2635 - accuracy: 0.92 - ETA: 2s - loss: 0.2623 - accuracy: 0.92 - ETA: 2s - loss: 0.2622 - accuracy: 0.92 - ETA: 2s - loss: 0.2614 - accuracy: 0.92 - ETA: 2s - loss: 0.2607 - accuracy: 0.92 - ETA: 2s - loss: 0.2612 - accuracy: 0.92 - ETA: 2s - loss: 0.2602 - accuracy: 0.92 - ETA: 2s - loss: 0.2581 - accuracy: 0.92 - ETA: 2s - loss: 0.2572 - accuracy: 0.92 - ETA: 2s - loss: 0.2565 - accuracy: 0.92 - ETA: 2s - loss: 0.2547 - accuracy: 0.92 - ETA: 2s - loss: 0.2555 - accuracy: 0.92 - ETA: 1s - loss: 0.2541 - accuracy: 0.92 - ETA: 1s - loss: 0.2536 - accuracy: 0.92 - ETA: 1s - loss: 0.2533 - accuracy: 0.92 - ETA: 1s - loss: 0.2538 - accuracy: 0.92 - ETA: 1s - loss: 0.2542 - accuracy: 0.92 - ETA: 1s - loss: 0.2532 - accuracy: 0.92 - ETA: 1s - loss: 0.2543 - accuracy: 0.92 - ETA: 1s - loss: 0.2529 - accuracy: 0.92 - ETA: 1s - loss: 0.2523 - accuracy: 0.92 - ETA: 1s - loss: 0.2523 - accuracy: 0.93 - ETA: 1s - loss: 0.2518 - accuracy: 0.93 - ETA: 1s - loss: 0.2513 - accuracy: 0.93 - ETA: 1s - loss: 0.2510 - accuracy: 0.93 - ETA: 1s - loss: 0.2505 - accuracy: 0.93 - ETA: 1s - loss: 0.2503 - accuracy: 0.93 - ETA: 1s - loss: 0.2494 - accuracy: 0.93 - ETA: 1s - loss: 0.2493 - accuracy: 0.93 - ETA: 1s - loss: 0.2489 - accuracy: 0.93 - ETA: 1s - loss: 0.2489 - accuracy: 0.93 - ETA: 1s - loss: 0.2488 - accuracy: 0.93 - ETA: 0s - loss: 0.2484 - accuracy: 0.93 - ETA: 0s - loss: 0.2471 - accuracy: 0.93 - ETA: 0s - loss: 0.2471 - accuracy: 0.93 - ETA: 0s - loss: 0.2464 - accuracy: 0.93 - ETA: 0s - loss: 0.2454 - accuracy: 0.93 - ETA: 0s - loss: 0.2451 - accuracy: 0.93 - ETA: 0s - loss: 0.2450 - accuracy: 0.93 - ETA: 0s - loss: 0.2452 - accuracy: 0.93 - ETA: 0s - loss: 0.2446 - accuracy: 0.93 - ETA: 0s - loss: 0.2451 - accuracy: 0.93 - ETA: 0s - loss: 0.2443 - accuracy: 0.93 - ETA: 0s - loss: 0.2439 - accuracy: 0.93 - ETA: 0s - loss: 0.2440 - accuracy: 0.93 - ETA: 0s - loss: 0.2429 - accuracy: 0.93 - ETA: 0s - loss: 0.2420 - accuracy: 0.93 - ETA: 0s - loss: 0.2410 - accuracy: 0.93 - ETA: 0s - loss: 0.2409 - accuracy: 0.93 - ETA: 0s - loss: 0.2404 - accuracy: 0.93 - ETA: 0s - loss: 0.2400 - accuracy: 0.93 - ETA: 0s - loss: 0.2396 - accuracy: 0.93 - 4s 2ms/step - loss: 0.2387 - accuracy: 0.9340 - val_loss: 0.2032 - val_accuracy: 0.9431\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.3309 - accuracy: 0.90 - ETA: 3s - loss: 0.1679 - accuracy: 0.95 - ETA: 3s - loss: 0.1930 - accuracy: 0.94 - ETA: 3s - loss: 0.1996 - accuracy: 0.94 - ETA: 3s - loss: 0.2046 - accuracy: 0.94 - ETA: 3s - loss: 0.2063 - accuracy: 0.94 - ETA: 3s - loss: 0.2057 - accuracy: 0.94 - ETA: 3s - loss: 0.2041 - accuracy: 0.94 - ETA: 2s - loss: 0.1991 - accuracy: 0.94 - ETA: 2s - loss: 0.1978 - accuracy: 0.94 - ETA: 2s - loss: 0.1991 - accuracy: 0.94 - ETA: 2s - loss: 0.1970 - accuracy: 0.94 - ETA: 2s - loss: 0.1976 - accuracy: 0.94 - ETA: 2s - loss: 0.1972 - accuracy: 0.94 - ETA: 2s - loss: 0.1983 - accuracy: 0.94 - ETA: 2s - loss: 0.1973 - accuracy: 0.94 - ETA: 2s - loss: 0.1957 - accuracy: 0.94 - ETA: 2s - loss: 0.1950 - accuracy: 0.94 - ETA: 2s - loss: 0.1963 - accuracy: 0.94 - ETA: 2s - loss: 0.1951 - accuracy: 0.94 - ETA: 2s - loss: 0.1939 - accuracy: 0.94 - ETA: 2s - loss: 0.1936 - accuracy: 0.94 - ETA: 2s - loss: 0.1921 - accuracy: 0.94 - ETA: 2s - loss: 0.1929 - accuracy: 0.94 - ETA: 2s - loss: 0.1930 - accuracy: 0.94 - ETA: 2s - loss: 0.1915 - accuracy: 0.94 - ETA: 2s - loss: 0.1924 - accuracy: 0.94 - ETA: 2s - loss: 0.1936 - accuracy: 0.94 - ETA: 1s - loss: 0.1943 - accuracy: 0.94 - ETA: 1s - loss: 0.1937 - accuracy: 0.94 - ETA: 1s - loss: 0.1950 - accuracy: 0.94 - ETA: 1s - loss: 0.1949 - accuracy: 0.94 - ETA: 1s - loss: 0.1938 - accuracy: 0.94 - ETA: 1s - loss: 0.1930 - accuracy: 0.94 - ETA: 1s - loss: 0.1929 - accuracy: 0.94 - ETA: 1s - loss: 0.1917 - accuracy: 0.94 - ETA: 1s - loss: 0.1919 - accuracy: 0.94 - ETA: 1s - loss: 0.1918 - accuracy: 0.94 - ETA: 1s - loss: 0.1915 - accuracy: 0.94 - ETA: 1s - loss: 0.1920 - accuracy: 0.94 - ETA: 1s - loss: 0.1920 - accuracy: 0.94 - ETA: 1s - loss: 0.1919 - accuracy: 0.94 - ETA: 1s - loss: 0.1915 - accuracy: 0.94 - ETA: 1s - loss: 0.1910 - accuracy: 0.94 - ETA: 1s - loss: 0.1909 - accuracy: 0.94 - ETA: 1s - loss: 0.1903 - accuracy: 0.94 - ETA: 1s - loss: 0.1903 - accuracy: 0.94 - ETA: 1s - loss: 0.1903 - accuracy: 0.94 - ETA: 0s - loss: 0.1897 - accuracy: 0.94 - ETA: 0s - loss: 0.1890 - accuracy: 0.94 - ETA: 0s - loss: 0.1889 - accuracy: 0.94 - ETA: 0s - loss: 0.1885 - accuracy: 0.94 - ETA: 0s - loss: 0.1883 - accuracy: 0.94 - ETA: 0s - loss: 0.1883 - accuracy: 0.94 - ETA: 0s - loss: 0.1885 - accuracy: 0.94 - ETA: 0s - loss: 0.1886 - accuracy: 0.94 - ETA: 0s - loss: 0.1885 - accuracy: 0.94 - ETA: 0s - loss: 0.1881 - accuracy: 0.94 - ETA: 0s - loss: 0.1877 - accuracy: 0.94 - ETA: 0s - loss: 0.1877 - accuracy: 0.94 - ETA: 0s - loss: 0.1872 - accuracy: 0.94 - ETA: 0s - loss: 0.1877 - accuracy: 0.94 - ETA: 0s - loss: 0.1874 - accuracy: 0.94 - ETA: 0s - loss: 0.1872 - accuracy: 0.94 - ETA: 0s - loss: 0.1868 - accuracy: 0.94 - ETA: 0s - loss: 0.1870 - accuracy: 0.94 - ETA: 0s - loss: 0.1869 - accuracy: 0.94 - 4s 2ms/step - loss: 0.1863 - accuracy: 0.9485 - val_loss: 0.1666 - val_accuracy: 0.9518\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.2821 - accuracy: 0.93 - ETA: 3s - loss: 0.1557 - accuracy: 0.95 - ETA: 3s - loss: 0.1551 - accuracy: 0.95 - ETA: 3s - loss: 0.1570 - accuracy: 0.95 - ETA: 3s - loss: 0.1579 - accuracy: 0.95 - ETA: 3s - loss: 0.1622 - accuracy: 0.95 - ETA: 3s - loss: 0.1632 - accuracy: 0.95 - ETA: 3s - loss: 0.1643 - accuracy: 0.95 - ETA: 3s - loss: 0.1637 - accuracy: 0.95 - ETA: 3s - loss: 0.1662 - accuracy: 0.95 - ETA: 2s - loss: 0.1703 - accuracy: 0.95 - ETA: 2s - loss: 0.1686 - accuracy: 0.95 - ETA: 2s - loss: 0.1658 - accuracy: 0.95 - ETA: 2s - loss: 0.1666 - accuracy: 0.95 - ETA: 2s - loss: 0.1681 - accuracy: 0.95 - ETA: 2s - loss: 0.1663 - accuracy: 0.95 - ETA: 2s - loss: 0.1687 - accuracy: 0.95 - ETA: 2s - loss: 0.1666 - accuracy: 0.95 - ETA: 2s - loss: 0.1669 - accuracy: 0.95 - ETA: 2s - loss: 0.1672 - accuracy: 0.95 - ETA: 2s - loss: 0.1680 - accuracy: 0.95 - ETA: 2s - loss: 0.1670 - accuracy: 0.95 - ETA: 2s - loss: 0.1672 - accuracy: 0.95 - ETA: 2s - loss: 0.1678 - accuracy: 0.95 - ETA: 2s - loss: 0.1661 - accuracy: 0.95 - ETA: 2s - loss: 0.1657 - accuracy: 0.95 - ETA: 2s - loss: 0.1643 - accuracy: 0.95 - ETA: 2s - loss: 0.1640 - accuracy: 0.95 - ETA: 2s - loss: 0.1628 - accuracy: 0.95 - ETA: 2s - loss: 0.1632 - accuracy: 0.95 - ETA: 2s - loss: 0.1613 - accuracy: 0.95 - ETA: 2s - loss: 0.1611 - accuracy: 0.95 - ETA: 2s - loss: 0.1607 - accuracy: 0.95 - ETA: 2s - loss: 0.1605 - accuracy: 0.95 - ETA: 2s - loss: 0.1603 - accuracy: 0.95 - ETA: 2s - loss: 0.1601 - accuracy: 0.95 - ETA: 2s - loss: 0.1594 - accuracy: 0.95 - ETA: 1s - loss: 0.1585 - accuracy: 0.95 - ETA: 1s - loss: 0.1584 - accuracy: 0.95 - ETA: 1s - loss: 0.1583 - accuracy: 0.95 - ETA: 1s - loss: 0.1587 - accuracy: 0.95 - ETA: 1s - loss: 0.1585 - accuracy: 0.95 - ETA: 1s - loss: 0.1580 - accuracy: 0.95 - ETA: 1s - loss: 0.1584 - accuracy: 0.95 - ETA: 1s - loss: 0.1587 - accuracy: 0.95 - ETA: 1s - loss: 0.1580 - accuracy: 0.95 - ETA: 1s - loss: 0.1579 - accuracy: 0.95 - ETA: 1s - loss: 0.1579 - accuracy: 0.95 - ETA: 1s - loss: 0.1571 - accuracy: 0.95 - ETA: 1s - loss: 0.1564 - accuracy: 0.95 - ETA: 1s - loss: 0.1560 - accuracy: 0.95 - ETA: 1s - loss: 0.1554 - accuracy: 0.95 - ETA: 1s - loss: 0.1553 - accuracy: 0.95 - ETA: 1s - loss: 0.1552 - accuracy: 0.95 - ETA: 1s - loss: 0.1550 - accuracy: 0.95 - ETA: 1s - loss: 0.1551 - accuracy: 0.95 - ETA: 0s - loss: 0.1548 - accuracy: 0.95 - ETA: 0s - loss: 0.1552 - accuracy: 0.95 - ETA: 0s - loss: 0.1547 - accuracy: 0.95 - ETA: 0s - loss: 0.1545 - accuracy: 0.95 - ETA: 0s - loss: 0.1541 - accuracy: 0.95 - ETA: 0s - loss: 0.1539 - accuracy: 0.95 - ETA: 0s - loss: 0.1540 - accuracy: 0.95 - ETA: 0s - loss: 0.1535 - accuracy: 0.95 - ETA: 0s - loss: 0.1537 - accuracy: 0.95 - ETA: 0s - loss: 0.1537 - accuracy: 0.95 - ETA: 0s - loss: 0.1536 - accuracy: 0.95 - ETA: 0s - loss: 0.1536 - accuracy: 0.95 - ETA: 0s - loss: 0.1534 - accuracy: 0.95 - ETA: 0s - loss: 0.1542 - accuracy: 0.95 - ETA: 0s - loss: 0.1538 - accuracy: 0.95 - ETA: 0s - loss: 0.1535 - accuracy: 0.95 - ETA: 0s - loss: 0.1533 - accuracy: 0.95 - ETA: 0s - loss: 0.1531 - accuracy: 0.95 - 4s 2ms/step - loss: 0.1529 - accuracy: 0.9578 - val_loss: 0.1418 - val_accuracy: 0.9589\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0919 - accuracy: 1.00 - ETA: 3s - loss: 0.1166 - accuracy: 0.97 - ETA: 3s - loss: 0.1215 - accuracy: 0.96 - ETA: 3s - loss: 0.1236 - accuracy: 0.96 - ETA: 3s - loss: 0.1204 - accuracy: 0.96 - ETA: 3s - loss: 0.1262 - accuracy: 0.96 - ETA: 3s - loss: 0.1246 - accuracy: 0.96 - ETA: 3s - loss: 0.1273 - accuracy: 0.96 - ETA: 3s - loss: 0.1276 - accuracy: 0.96 - ETA: 3s - loss: 0.1270 - accuracy: 0.96 - ETA: 3s - loss: 0.1289 - accuracy: 0.96 - ETA: 3s - loss: 0.1287 - accuracy: 0.96 - ETA: 3s - loss: 0.1292 - accuracy: 0.96 - ETA: 3s - loss: 0.1312 - accuracy: 0.96 - ETA: 3s - loss: 0.1306 - accuracy: 0.96 - ETA: 2s - loss: 0.1297 - accuracy: 0.96 - ETA: 2s - loss: 0.1296 - accuracy: 0.96 - ETA: 2s - loss: 0.1316 - accuracy: 0.96 - ETA: 2s - loss: 0.1326 - accuracy: 0.96 - ETA: 2s - loss: 0.1338 - accuracy: 0.96 - ETA: 2s - loss: 0.1335 - accuracy: 0.96 - ETA: 2s - loss: 0.1322 - accuracy: 0.96 - ETA: 2s - loss: 0.1327 - accuracy: 0.96 - ETA: 2s - loss: 0.1323 - accuracy: 0.96 - ETA: 2s - loss: 0.1335 - accuracy: 0.96 - ETA: 2s - loss: 0.1330 - accuracy: 0.96 - ETA: 2s - loss: 0.1320 - accuracy: 0.96 - ETA: 2s - loss: 0.1325 - accuracy: 0.96 - ETA: 2s - loss: 0.1345 - accuracy: 0.96 - ETA: 2s - loss: 0.1343 - accuracy: 0.96 - ETA: 2s - loss: 0.1347 - accuracy: 0.96 - ETA: 2s - loss: 0.1345 - accuracy: 0.96 - ETA: 2s - loss: 0.1351 - accuracy: 0.96 - ETA: 1s - loss: 0.1343 - accuracy: 0.96 - ETA: 1s - loss: 0.1337 - accuracy: 0.96 - ETA: 1s - loss: 0.1330 - accuracy: 0.96 - ETA: 1s - loss: 0.1325 - accuracy: 0.96 - ETA: 1s - loss: 0.1330 - accuracy: 0.96 - ETA: 1s - loss: 0.1319 - accuracy: 0.96 - ETA: 1s - loss: 0.1324 - accuracy: 0.96 - ETA: 1s - loss: 0.1325 - accuracy: 0.96 - ETA: 1s - loss: 0.1329 - accuracy: 0.96 - ETA: 1s - loss: 0.1331 - accuracy: 0.96 - ETA: 1s - loss: 0.1331 - accuracy: 0.96 - ETA: 1s - loss: 0.1325 - accuracy: 0.96 - ETA: 1s - loss: 0.1323 - accuracy: 0.96 - ETA: 1s - loss: 0.1324 - accuracy: 0.96 - ETA: 1s - loss: 0.1324 - accuracy: 0.96 - ETA: 1s - loss: 0.1317 - accuracy: 0.96 - ETA: 1s - loss: 0.1315 - accuracy: 0.96 - ETA: 1s - loss: 0.1315 - accuracy: 0.96 - ETA: 1s - loss: 0.1321 - accuracy: 0.96 - ETA: 0s - loss: 0.1316 - accuracy: 0.96 - ETA: 0s - loss: 0.1313 - accuracy: 0.96 - ETA: 0s - loss: 0.1315 - accuracy: 0.96 - ETA: 0s - loss: 0.1310 - accuracy: 0.96 - ETA: 0s - loss: 0.1304 - accuracy: 0.96 - ETA: 0s - loss: 0.1300 - accuracy: 0.96 - ETA: 0s - loss: 0.1296 - accuracy: 0.96 - ETA: 0s - loss: 0.1297 - accuracy: 0.96 - ETA: 0s - loss: 0.1301 - accuracy: 0.96 - ETA: 0s - loss: 0.1305 - accuracy: 0.96 - ETA: 0s - loss: 0.1301 - accuracy: 0.96 - ETA: 0s - loss: 0.1305 - accuracy: 0.96 - ETA: 0s - loss: 0.1309 - accuracy: 0.96 - ETA: 0s - loss: 0.1309 - accuracy: 0.96 - ETA: 0s - loss: 0.1305 - accuracy: 0.96 - ETA: 0s - loss: 0.1300 - accuracy: 0.96 - ETA: 0s - loss: 0.1300 - accuracy: 0.96 - ETA: 0s - loss: 0.1295 - accuracy: 0.96 - ETA: 0s - loss: 0.1289 - accuracy: 0.96 - 4s 2ms/step - loss: 0.1288 - accuracy: 0.9645 - val_loss: 0.1242 - val_accuracy: 0.9640\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 2.3393 - accuracy: 0.18 - ETA: 3s - loss: 2.1190 - accuracy: 0.30 - ETA: 3s - loss: 1.9615 - accuracy: 0.44 - ETA: 3s - loss: 1.8101 - accuracy: 0.53 - ETA: 3s - loss: 1.6724 - accuracy: 0.60 - ETA: 3s - loss: 1.5521 - accuracy: 0.64 - ETA: 3s - loss: 1.4489 - accuracy: 0.66 - ETA: 3s - loss: 1.3659 - accuracy: 0.68 - ETA: 2s - loss: 1.2877 - accuracy: 0.70 - ETA: 2s - loss: 1.2225 - accuracy: 0.72 - ETA: 2s - loss: 1.1707 - accuracy: 0.73 - ETA: 2s - loss: 1.1217 - accuracy: 0.74 - ETA: 2s - loss: 1.0749 - accuracy: 0.75 - ETA: 2s - loss: 1.0292 - accuracy: 0.77 - ETA: 2s - loss: 0.9957 - accuracy: 0.77 - ETA: 2s - loss: 0.9620 - accuracy: 0.78 - ETA: 2s - loss: 0.9318 - accuracy: 0.78 - ETA: 2s - loss: 0.9070 - accuracy: 0.79 - ETA: 2s - loss: 0.8837 - accuracy: 0.79 - ETA: 2s - loss: 0.8594 - accuracy: 0.80 - ETA: 2s - loss: 0.8405 - accuracy: 0.80 - ETA: 2s - loss: 0.8201 - accuracy: 0.81 - ETA: 2s - loss: 0.8018 - accuracy: 0.81 - ETA: 2s - loss: 0.7846 - accuracy: 0.81 - ETA: 2s - loss: 0.7688 - accuracy: 0.82 - ETA: 2s - loss: 0.7544 - accuracy: 0.82 - ETA: 2s - loss: 0.7386 - accuracy: 0.82 - ETA: 1s - loss: 0.7246 - accuracy: 0.82 - ETA: 1s - loss: 0.7113 - accuracy: 0.83 - ETA: 1s - loss: 0.7006 - accuracy: 0.83 - ETA: 1s - loss: 0.6893 - accuracy: 0.83 - ETA: 1s - loss: 0.6788 - accuracy: 0.84 - ETA: 1s - loss: 0.6683 - accuracy: 0.84 - ETA: 1s - loss: 0.6593 - accuracy: 0.84 - ETA: 1s - loss: 0.6502 - accuracy: 0.84 - ETA: 1s - loss: 0.6428 - accuracy: 0.84 - ETA: 1s - loss: 0.6344 - accuracy: 0.84 - ETA: 1s - loss: 0.6277 - accuracy: 0.85 - ETA: 1s - loss: 0.6216 - accuracy: 0.85 - ETA: 1s - loss: 0.6151 - accuracy: 0.85 - ETA: 1s - loss: 0.6078 - accuracy: 0.85 - ETA: 1s - loss: 0.6001 - accuracy: 0.85 - ETA: 1s - loss: 0.5929 - accuracy: 0.85 - ETA: 1s - loss: 0.5871 - accuracy: 0.85 - ETA: 1s - loss: 0.5810 - accuracy: 0.85 - ETA: 1s - loss: 0.5753 - accuracy: 0.86 - ETA: 1s - loss: 0.5691 - accuracy: 0.86 - ETA: 1s - loss: 0.5632 - accuracy: 0.86 - ETA: 0s - loss: 0.5582 - accuracy: 0.86 - ETA: 0s - loss: 0.5532 - accuracy: 0.86 - ETA: 0s - loss: 0.5488 - accuracy: 0.86 - ETA: 0s - loss: 0.5441 - accuracy: 0.86 - ETA: 0s - loss: 0.5396 - accuracy: 0.86 - ETA: 0s - loss: 0.5353 - accuracy: 0.86 - ETA: 0s - loss: 0.5308 - accuracy: 0.86 - ETA: 0s - loss: 0.5267 - accuracy: 0.87 - ETA: 0s - loss: 0.5234 - accuracy: 0.87 - ETA: 0s - loss: 0.5192 - accuracy: 0.87 - ETA: 0s - loss: 0.5150 - accuracy: 0.87 - ETA: 0s - loss: 0.5118 - accuracy: 0.87 - ETA: 0s - loss: 0.5081 - accuracy: 0.87 - ETA: 0s - loss: 0.5041 - accuracy: 0.87 - ETA: 0s - loss: 0.5004 - accuracy: 0.87 - ETA: 0s - loss: 0.4960 - accuracy: 0.87 - ETA: 0s - loss: 0.4914 - accuracy: 0.87 - ETA: 0s - loss: 0.4877 - accuracy: 0.87 - ETA: 0s - loss: 0.4834 - accuracy: 0.88 - 4s 2ms/step - loss: 0.4808 - accuracy: 0.8806 - val_loss: 0.2597 - val_accuracy: 0.9292\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0832 - accuracy: 1.00 - ETA: 2s - loss: 0.2689 - accuracy: 0.92 - ETA: 2s - loss: 0.2611 - accuracy: 0.92 - ETA: 2s - loss: 0.2675 - accuracy: 0.92 - ETA: 2s - loss: 0.2709 - accuracy: 0.92 - ETA: 2s - loss: 0.2681 - accuracy: 0.92 - ETA: 2s - loss: 0.2638 - accuracy: 0.92 - ETA: 2s - loss: 0.2578 - accuracy: 0.92 - ETA: 2s - loss: 0.2542 - accuracy: 0.93 - ETA: 2s - loss: 0.2517 - accuracy: 0.93 - ETA: 2s - loss: 0.2488 - accuracy: 0.93 - ETA: 2s - loss: 0.2499 - accuracy: 0.93 - ETA: 2s - loss: 0.2481 - accuracy: 0.93 - ETA: 2s - loss: 0.2476 - accuracy: 0.93 - ETA: 2s - loss: 0.2458 - accuracy: 0.93 - ETA: 2s - loss: 0.2444 - accuracy: 0.93 - ETA: 2s - loss: 0.2452 - accuracy: 0.93 - ETA: 2s - loss: 0.2448 - accuracy: 0.93 - ETA: 2s - loss: 0.2416 - accuracy: 0.93 - ETA: 2s - loss: 0.2420 - accuracy: 0.93 - ETA: 2s - loss: 0.2426 - accuracy: 0.93 - ETA: 2s - loss: 0.2431 - accuracy: 0.93 - ETA: 2s - loss: 0.2425 - accuracy: 0.93 - ETA: 2s - loss: 0.2421 - accuracy: 0.93 - ETA: 2s - loss: 0.2434 - accuracy: 0.93 - ETA: 1s - loss: 0.2413 - accuracy: 0.93 - ETA: 1s - loss: 0.2415 - accuracy: 0.93 - ETA: 1s - loss: 0.2423 - accuracy: 0.93 - ETA: 1s - loss: 0.2415 - accuracy: 0.93 - ETA: 1s - loss: 0.2407 - accuracy: 0.93 - ETA: 1s - loss: 0.2403 - accuracy: 0.93 - ETA: 1s - loss: 0.2403 - accuracy: 0.93 - ETA: 1s - loss: 0.2399 - accuracy: 0.93 - ETA: 1s - loss: 0.2398 - accuracy: 0.93 - ETA: 1s - loss: 0.2413 - accuracy: 0.93 - ETA: 1s - loss: 0.2406 - accuracy: 0.93 - ETA: 1s - loss: 0.2398 - accuracy: 0.93 - ETA: 1s - loss: 0.2389 - accuracy: 0.93 - ETA: 1s - loss: 0.2384 - accuracy: 0.93 - ETA: 1s - loss: 0.2379 - accuracy: 0.93 - ETA: 1s - loss: 0.2381 - accuracy: 0.93 - ETA: 1s - loss: 0.2381 - accuracy: 0.93 - ETA: 1s - loss: 0.2382 - accuracy: 0.93 - ETA: 1s - loss: 0.2379 - accuracy: 0.93 - ETA: 1s - loss: 0.2372 - accuracy: 0.93 - ETA: 1s - loss: 0.2364 - accuracy: 0.93 - ETA: 0s - loss: 0.2362 - accuracy: 0.93 - ETA: 0s - loss: 0.2365 - accuracy: 0.93 - ETA: 0s - loss: 0.2371 - accuracy: 0.93 - ETA: 0s - loss: 0.2368 - accuracy: 0.93 - ETA: 0s - loss: 0.2363 - accuracy: 0.93 - ETA: 0s - loss: 0.2356 - accuracy: 0.93 - ETA: 0s - loss: 0.2350 - accuracy: 0.93 - ETA: 0s - loss: 0.2339 - accuracy: 0.93 - ETA: 0s - loss: 0.2331 - accuracy: 0.93 - ETA: 0s - loss: 0.2326 - accuracy: 0.93 - ETA: 0s - loss: 0.2321 - accuracy: 0.93 - ETA: 0s - loss: 0.2315 - accuracy: 0.93 - ETA: 0s - loss: 0.2307 - accuracy: 0.93 - ETA: 0s - loss: 0.2306 - accuracy: 0.93 - ETA: 0s - loss: 0.2310 - accuracy: 0.93 - ETA: 0s - loss: 0.2310 - accuracy: 0.93 - ETA: 0s - loss: 0.2305 - accuracy: 0.93 - ETA: 0s - loss: 0.2302 - accuracy: 0.93 - ETA: 0s - loss: 0.2297 - accuracy: 0.93 - ETA: 0s - loss: 0.2295 - accuracy: 0.93 - 4s 2ms/step - loss: 0.2296 - accuracy: 0.9369 - val_loss: 0.1976 - val_accuracy: 0.9430\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.2830 - accuracy: 0.90 - ETA: 3s - loss: 0.1860 - accuracy: 0.95 - ETA: 3s - loss: 0.1920 - accuracy: 0.94 - ETA: 3s - loss: 0.1912 - accuracy: 0.94 - ETA: 3s - loss: 0.1882 - accuracy: 0.94 - ETA: 3s - loss: 0.1867 - accuracy: 0.94 - ETA: 2s - loss: 0.1898 - accuracy: 0.94 - ETA: 2s - loss: 0.1921 - accuracy: 0.94 - ETA: 2s - loss: 0.1939 - accuracy: 0.94 - ETA: 2s - loss: 0.1933 - accuracy: 0.94 - ETA: 2s - loss: 0.1969 - accuracy: 0.94 - ETA: 2s - loss: 0.1965 - accuracy: 0.94 - ETA: 2s - loss: 0.1961 - accuracy: 0.94 - ETA: 2s - loss: 0.1938 - accuracy: 0.94 - ETA: 2s - loss: 0.1945 - accuracy: 0.94 - ETA: 2s - loss: 0.1929 - accuracy: 0.94 - ETA: 2s - loss: 0.1908 - accuracy: 0.94 - ETA: 2s - loss: 0.1910 - accuracy: 0.94 - ETA: 2s - loss: 0.1898 - accuracy: 0.94 - ETA: 2s - loss: 0.1894 - accuracy: 0.94 - ETA: 2s - loss: 0.1895 - accuracy: 0.94 - ETA: 2s - loss: 0.1891 - accuracy: 0.94 - ETA: 2s - loss: 0.1881 - accuracy: 0.94 - ETA: 2s - loss: 0.1883 - accuracy: 0.94 - ETA: 2s - loss: 0.1878 - accuracy: 0.94 - ETA: 1s - loss: 0.1874 - accuracy: 0.94 - ETA: 1s - loss: 0.1868 - accuracy: 0.94 - ETA: 1s - loss: 0.1855 - accuracy: 0.94 - ETA: 1s - loss: 0.1851 - accuracy: 0.94 - ETA: 1s - loss: 0.1846 - accuracy: 0.94 - ETA: 1s - loss: 0.1868 - accuracy: 0.94 - ETA: 1s - loss: 0.1874 - accuracy: 0.94 - ETA: 1s - loss: 0.1869 - accuracy: 0.94 - ETA: 1s - loss: 0.1853 - accuracy: 0.94 - ETA: 1s - loss: 0.1848 - accuracy: 0.94 - ETA: 1s - loss: 0.1862 - accuracy: 0.94 - ETA: 1s - loss: 0.1865 - accuracy: 0.94 - ETA: 1s - loss: 0.1865 - accuracy: 0.94 - ETA: 1s - loss: 0.1854 - accuracy: 0.94 - ETA: 1s - loss: 0.1850 - accuracy: 0.94 - ETA: 1s - loss: 0.1842 - accuracy: 0.94 - ETA: 1s - loss: 0.1849 - accuracy: 0.94 - ETA: 1s - loss: 0.1847 - accuracy: 0.94 - ETA: 1s - loss: 0.1837 - accuracy: 0.94 - ETA: 1s - loss: 0.1831 - accuracy: 0.94 - ETA: 0s - loss: 0.1826 - accuracy: 0.94 - ETA: 0s - loss: 0.1832 - accuracy: 0.94 - ETA: 0s - loss: 0.1827 - accuracy: 0.94 - ETA: 0s - loss: 0.1821 - accuracy: 0.94 - ETA: 0s - loss: 0.1813 - accuracy: 0.94 - ETA: 0s - loss: 0.1813 - accuracy: 0.94 - ETA: 0s - loss: 0.1804 - accuracy: 0.94 - ETA: 0s - loss: 0.1795 - accuracy: 0.95 - ETA: 0s - loss: 0.1795 - accuracy: 0.95 - ETA: 0s - loss: 0.1791 - accuracy: 0.95 - ETA: 0s - loss: 0.1790 - accuracy: 0.95 - ETA: 0s - loss: 0.1789 - accuracy: 0.95 - ETA: 0s - loss: 0.1786 - accuracy: 0.95 - ETA: 0s - loss: 0.1785 - accuracy: 0.95 - ETA: 0s - loss: 0.1783 - accuracy: 0.95 - ETA: 0s - loss: 0.1781 - accuracy: 0.95 - ETA: 0s - loss: 0.1786 - accuracy: 0.95 - ETA: 0s - loss: 0.1787 - accuracy: 0.95 - ETA: 0s - loss: 0.1785 - accuracy: 0.95 - 3s 2ms/step - loss: 0.1782 - accuracy: 0.9508 - val_loss: 0.1589 - val_accuracy: 0.9542\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.4725 - accuracy: 0.84 - ETA: 3s - loss: 0.1583 - accuracy: 0.95 - ETA: 3s - loss: 0.1623 - accuracy: 0.95 - ETA: 3s - loss: 0.1610 - accuracy: 0.95 - ETA: 3s - loss: 0.1568 - accuracy: 0.95 - ETA: 2s - loss: 0.1586 - accuracy: 0.95 - ETA: 2s - loss: 0.1584 - accuracy: 0.95 - ETA: 2s - loss: 0.1602 - accuracy: 0.95 - ETA: 2s - loss: 0.1582 - accuracy: 0.95 - ETA: 2s - loss: 0.1554 - accuracy: 0.95 - ETA: 2s - loss: 0.1535 - accuracy: 0.95 - ETA: 2s - loss: 0.1545 - accuracy: 0.95 - ETA: 2s - loss: 0.1525 - accuracy: 0.95 - ETA: 2s - loss: 0.1519 - accuracy: 0.95 - ETA: 2s - loss: 0.1515 - accuracy: 0.95 - ETA: 2s - loss: 0.1540 - accuracy: 0.95 - ETA: 2s - loss: 0.1538 - accuracy: 0.95 - ETA: 2s - loss: 0.1524 - accuracy: 0.95 - ETA: 2s - loss: 0.1511 - accuracy: 0.95 - ETA: 2s - loss: 0.1503 - accuracy: 0.95 - ETA: 2s - loss: 0.1500 - accuracy: 0.95 - ETA: 2s - loss: 0.1515 - accuracy: 0.95 - ETA: 2s - loss: 0.1506 - accuracy: 0.95 - ETA: 2s - loss: 0.1515 - accuracy: 0.95 - ETA: 2s - loss: 0.1535 - accuracy: 0.95 - ETA: 1s - loss: 0.1533 - accuracy: 0.95 - ETA: 1s - loss: 0.1533 - accuracy: 0.95 - ETA: 1s - loss: 0.1526 - accuracy: 0.95 - ETA: 1s - loss: 0.1532 - accuracy: 0.95 - ETA: 1s - loss: 0.1530 - accuracy: 0.95 - ETA: 1s - loss: 0.1530 - accuracy: 0.95 - ETA: 1s - loss: 0.1528 - accuracy: 0.95 - ETA: 1s - loss: 0.1528 - accuracy: 0.95 - ETA: 1s - loss: 0.1520 - accuracy: 0.95 - ETA: 1s - loss: 0.1513 - accuracy: 0.95 - ETA: 1s - loss: 0.1520 - accuracy: 0.95 - ETA: 1s - loss: 0.1517 - accuracy: 0.95 - ETA: 1s - loss: 0.1509 - accuracy: 0.95 - ETA: 1s - loss: 0.1513 - accuracy: 0.95 - ETA: 1s - loss: 0.1512 - accuracy: 0.95 - ETA: 1s - loss: 0.1508 - accuracy: 0.95 - ETA: 1s - loss: 0.1505 - accuracy: 0.95 - ETA: 1s - loss: 0.1499 - accuracy: 0.95 - ETA: 1s - loss: 0.1492 - accuracy: 0.95 - ETA: 1s - loss: 0.1495 - accuracy: 0.95 - ETA: 0s - loss: 0.1492 - accuracy: 0.95 - ETA: 0s - loss: 0.1486 - accuracy: 0.95 - ETA: 0s - loss: 0.1482 - accuracy: 0.95 - ETA: 0s - loss: 0.1476 - accuracy: 0.95 - ETA: 0s - loss: 0.1474 - accuracy: 0.95 - ETA: 0s - loss: 0.1473 - accuracy: 0.95 - ETA: 0s - loss: 0.1471 - accuracy: 0.95 - ETA: 0s - loss: 0.1474 - accuracy: 0.95 - ETA: 0s - loss: 0.1477 - accuracy: 0.95 - ETA: 0s - loss: 0.1472 - accuracy: 0.95 - ETA: 0s - loss: 0.1471 - accuracy: 0.95 - ETA: 0s - loss: 0.1468 - accuracy: 0.95 - ETA: 0s - loss: 0.1471 - accuracy: 0.95 - ETA: 0s - loss: 0.1468 - accuracy: 0.95 - ETA: 0s - loss: 0.1464 - accuracy: 0.95 - ETA: 0s - loss: 0.1468 - accuracy: 0.95 - ETA: 0s - loss: 0.1464 - accuracy: 0.95 - ETA: 0s - loss: 0.1464 - accuracy: 0.95 - ETA: 0s - loss: 0.1463 - accuracy: 0.95 - 3s 2ms/step - loss: 0.1463 - accuracy: 0.9596 - val_loss: 0.1367 - val_accuracy: 0.9589\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.2056 - accuracy: 0.90 - ETA: 3s - loss: 0.0916 - accuracy: 0.97 - ETA: 3s - loss: 0.1181 - accuracy: 0.97 - ETA: 3s - loss: 0.1298 - accuracy: 0.96 - ETA: 3s - loss: 0.1234 - accuracy: 0.96 - ETA: 3s - loss: 0.1204 - accuracy: 0.96 - ETA: 2s - loss: 0.1254 - accuracy: 0.96 - ETA: 2s - loss: 0.1274 - accuracy: 0.96 - ETA: 2s - loss: 0.1268 - accuracy: 0.96 - ETA: 2s - loss: 0.1279 - accuracy: 0.96 - ETA: 2s - loss: 0.1283 - accuracy: 0.96 - ETA: 2s - loss: 0.1275 - accuracy: 0.96 - ETA: 2s - loss: 0.1252 - accuracy: 0.96 - ETA: 2s - loss: 0.1240 - accuracy: 0.96 - ETA: 2s - loss: 0.1233 - accuracy: 0.96 - ETA: 2s - loss: 0.1241 - accuracy: 0.96 - ETA: 2s - loss: 0.1248 - accuracy: 0.96 - ETA: 2s - loss: 0.1242 - accuracy: 0.96 - ETA: 2s - loss: 0.1261 - accuracy: 0.96 - ETA: 2s - loss: 0.1257 - accuracy: 0.96 - ETA: 2s - loss: 0.1266 - accuracy: 0.96 - ETA: 2s - loss: 0.1275 - accuracy: 0.96 - ETA: 2s - loss: 0.1271 - accuracy: 0.96 - ETA: 2s - loss: 0.1272 - accuracy: 0.96 - ETA: 2s - loss: 0.1266 - accuracy: 0.96 - ETA: 1s - loss: 0.1262 - accuracy: 0.96 - ETA: 1s - loss: 0.1271 - accuracy: 0.96 - ETA: 1s - loss: 0.1276 - accuracy: 0.96 - ETA: 1s - loss: 0.1278 - accuracy: 0.96 - ETA: 1s - loss: 0.1276 - accuracy: 0.96 - ETA: 1s - loss: 0.1270 - accuracy: 0.96 - ETA: 1s - loss: 0.1277 - accuracy: 0.96 - ETA: 1s - loss: 0.1278 - accuracy: 0.96 - ETA: 1s - loss: 0.1284 - accuracy: 0.96 - ETA: 1s - loss: 0.1278 - accuracy: 0.96 - ETA: 1s - loss: 0.1278 - accuracy: 0.96 - ETA: 1s - loss: 0.1276 - accuracy: 0.96 - ETA: 1s - loss: 0.1269 - accuracy: 0.96 - ETA: 1s - loss: 0.1268 - accuracy: 0.96 - ETA: 1s - loss: 0.1266 - accuracy: 0.96 - ETA: 1s - loss: 0.1260 - accuracy: 0.96 - ETA: 1s - loss: 0.1257 - accuracy: 0.96 - ETA: 1s - loss: 0.1256 - accuracy: 0.96 - ETA: 1s - loss: 0.1256 - accuracy: 0.96 - ETA: 1s - loss: 0.1267 - accuracy: 0.96 - ETA: 0s - loss: 0.1268 - accuracy: 0.96 - ETA: 0s - loss: 0.1270 - accuracy: 0.96 - ETA: 0s - loss: 0.1268 - accuracy: 0.96 - ETA: 0s - loss: 0.1271 - accuracy: 0.96 - ETA: 0s - loss: 0.1268 - accuracy: 0.96 - ETA: 0s - loss: 0.1265 - accuracy: 0.96 - ETA: 0s - loss: 0.1263 - accuracy: 0.96 - ETA: 0s - loss: 0.1259 - accuracy: 0.96 - ETA: 0s - loss: 0.1258 - accuracy: 0.96 - ETA: 0s - loss: 0.1252 - accuracy: 0.96 - ETA: 0s - loss: 0.1248 - accuracy: 0.96 - ETA: 0s - loss: 0.1248 - accuracy: 0.96 - ETA: 0s - loss: 0.1253 - accuracy: 0.96 - ETA: 0s - loss: 0.1250 - accuracy: 0.96 - ETA: 0s - loss: 0.1245 - accuracy: 0.96 - ETA: 0s - loss: 0.1239 - accuracy: 0.96 - ETA: 0s - loss: 0.1238 - accuracy: 0.96 - ETA: 0s - loss: 0.1238 - accuracy: 0.96 - ETA: 0s - loss: 0.1236 - accuracy: 0.96 - ETA: 0s - loss: 0.1232 - accuracy: 0.96 - 3s 2ms/step - loss: 0.1232 - accuracy: 0.9663 - val_loss: 0.1191 - val_accuracy: 0.9650\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 2.3936 - accuracy: 0.09 - ETA: 3s - loss: 2.0932 - accuracy: 0.33 - ETA: 3s - loss: 1.9128 - accuracy: 0.48 - ETA: 3s - loss: 1.7661 - accuracy: 0.55 - ETA: 3s - loss: 1.6357 - accuracy: 0.60 - ETA: 3s - loss: 1.5258 - accuracy: 0.64 - ETA: 2s - loss: 1.4148 - accuracy: 0.68 - ETA: 2s - loss: 1.3296 - accuracy: 0.70 - ETA: 2s - loss: 1.2560 - accuracy: 0.72 - ETA: 2s - loss: 1.2040 - accuracy: 0.73 - ETA: 2s - loss: 1.1448 - accuracy: 0.74 - ETA: 2s - loss: 1.0989 - accuracy: 0.75 - ETA: 2s - loss: 1.0528 - accuracy: 0.76 - ETA: 2s - loss: 1.0152 - accuracy: 0.77 - ETA: 2s - loss: 0.9818 - accuracy: 0.77 - ETA: 2s - loss: 0.9520 - accuracy: 0.78 - ETA: 2s - loss: 0.9217 - accuracy: 0.79 - ETA: 2s - loss: 0.8956 - accuracy: 0.79 - ETA: 2s - loss: 0.8756 - accuracy: 0.80 - ETA: 2s - loss: 0.8552 - accuracy: 0.80 - ETA: 2s - loss: 0.8330 - accuracy: 0.80 - ETA: 2s - loss: 0.8134 - accuracy: 0.81 - ETA: 2s - loss: 0.7947 - accuracy: 0.81 - ETA: 2s - loss: 0.7795 - accuracy: 0.81 - ETA: 2s - loss: 0.7632 - accuracy: 0.82 - ETA: 2s - loss: 0.7474 - accuracy: 0.82 - ETA: 2s - loss: 0.7350 - accuracy: 0.82 - ETA: 2s - loss: 0.7238 - accuracy: 0.83 - ETA: 1s - loss: 0.7121 - accuracy: 0.83 - ETA: 1s - loss: 0.7008 - accuracy: 0.83 - ETA: 1s - loss: 0.6893 - accuracy: 0.83 - ETA: 1s - loss: 0.6785 - accuracy: 0.84 - ETA: 1s - loss: 0.6683 - accuracy: 0.84 - ETA: 1s - loss: 0.6575 - accuracy: 0.84 - ETA: 1s - loss: 0.6484 - accuracy: 0.84 - ETA: 1s - loss: 0.6390 - accuracy: 0.84 - ETA: 1s - loss: 0.6299 - accuracy: 0.85 - ETA: 1s - loss: 0.6211 - accuracy: 0.85 - ETA: 1s - loss: 0.6140 - accuracy: 0.85 - ETA: 1s - loss: 0.6072 - accuracy: 0.85 - ETA: 1s - loss: 0.5998 - accuracy: 0.85 - ETA: 1s - loss: 0.5935 - accuracy: 0.85 - ETA: 1s - loss: 0.5859 - accuracy: 0.85 - ETA: 1s - loss: 0.5800 - accuracy: 0.86 - ETA: 1s - loss: 0.5739 - accuracy: 0.86 - ETA: 1s - loss: 0.5684 - accuracy: 0.86 - ETA: 0s - loss: 0.5623 - accuracy: 0.86 - ETA: 0s - loss: 0.5569 - accuracy: 0.86 - ETA: 0s - loss: 0.5512 - accuracy: 0.86 - ETA: 0s - loss: 0.5456 - accuracy: 0.86 - ETA: 0s - loss: 0.5410 - accuracy: 0.86 - ETA: 0s - loss: 0.5359 - accuracy: 0.86 - ETA: 0s - loss: 0.5312 - accuracy: 0.87 - ETA: 0s - loss: 0.5272 - accuracy: 0.87 - ETA: 0s - loss: 0.5228 - accuracy: 0.87 - ETA: 0s - loss: 0.5184 - accuracy: 0.87 - ETA: 0s - loss: 0.5145 - accuracy: 0.87 - ETA: 0s - loss: 0.5102 - accuracy: 0.87 - ETA: 0s - loss: 0.5062 - accuracy: 0.87 - ETA: 0s - loss: 0.5029 - accuracy: 0.87 - ETA: 0s - loss: 0.4983 - accuracy: 0.87 - ETA: 0s - loss: 0.4939 - accuracy: 0.87 - ETA: 0s - loss: 0.4898 - accuracy: 0.87 - ETA: 0s - loss: 0.4860 - accuracy: 0.87 - ETA: 0s - loss: 0.4823 - accuracy: 0.88 - ETA: 0s - loss: 0.4789 - accuracy: 0.88 - 4s 2ms/step - loss: 0.4781 - accuracy: 0.8818 - val_loss: 0.2601 - val_accuracy: 0.9260\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.2209 - accuracy: 0.93 - ETA: 3s - loss: 0.2346 - accuracy: 0.92 - ETA: 3s - loss: 0.2492 - accuracy: 0.92 - ETA: 2s - loss: 0.2531 - accuracy: 0.92 - ETA: 2s - loss: 0.2482 - accuracy: 0.93 - ETA: 2s - loss: 0.2470 - accuracy: 0.93 - ETA: 2s - loss: 0.2509 - accuracy: 0.93 - ETA: 2s - loss: 0.2535 - accuracy: 0.93 - ETA: 2s - loss: 0.2556 - accuracy: 0.93 - ETA: 2s - loss: 0.2597 - accuracy: 0.93 - ETA: 2s - loss: 0.2597 - accuracy: 0.93 - ETA: 2s - loss: 0.2598 - accuracy: 0.93 - ETA: 2s - loss: 0.2560 - accuracy: 0.93 - ETA: 2s - loss: 0.2577 - accuracy: 0.93 - ETA: 2s - loss: 0.2546 - accuracy: 0.93 - ETA: 2s - loss: 0.2573 - accuracy: 0.93 - ETA: 2s - loss: 0.2561 - accuracy: 0.93 - ETA: 2s - loss: 0.2566 - accuracy: 0.93 - ETA: 2s - loss: 0.2579 - accuracy: 0.92 - ETA: 2s - loss: 0.2572 - accuracy: 0.93 - ETA: 2s - loss: 0.2564 - accuracy: 0.93 - ETA: 2s - loss: 0.2540 - accuracy: 0.93 - ETA: 2s - loss: 0.2538 - accuracy: 0.93 - ETA: 2s - loss: 0.2515 - accuracy: 0.93 - ETA: 2s - loss: 0.2514 - accuracy: 0.93 - ETA: 1s - loss: 0.2487 - accuracy: 0.93 - ETA: 1s - loss: 0.2492 - accuracy: 0.93 - ETA: 1s - loss: 0.2486 - accuracy: 0.93 - ETA: 1s - loss: 0.2489 - accuracy: 0.93 - ETA: 1s - loss: 0.2487 - accuracy: 0.93 - ETA: 1s - loss: 0.2487 - accuracy: 0.93 - ETA: 1s - loss: 0.2473 - accuracy: 0.93 - ETA: 1s - loss: 0.2479 - accuracy: 0.93 - ETA: 1s - loss: 0.2479 - accuracy: 0.93 - ETA: 1s - loss: 0.2468 - accuracy: 0.93 - ETA: 1s - loss: 0.2466 - accuracy: 0.93 - ETA: 1s - loss: 0.2455 - accuracy: 0.93 - ETA: 1s - loss: 0.2454 - accuracy: 0.93 - ETA: 1s - loss: 0.2452 - accuracy: 0.93 - ETA: 1s - loss: 0.2439 - accuracy: 0.93 - ETA: 1s - loss: 0.2437 - accuracy: 0.93 - ETA: 1s - loss: 0.2428 - accuracy: 0.93 - ETA: 1s - loss: 0.2423 - accuracy: 0.93 - ETA: 1s - loss: 0.2412 - accuracy: 0.93 - ETA: 0s - loss: 0.2411 - accuracy: 0.93 - ETA: 0s - loss: 0.2417 - accuracy: 0.93 - ETA: 0s - loss: 0.2407 - accuracy: 0.93 - ETA: 0s - loss: 0.2403 - accuracy: 0.93 - ETA: 0s - loss: 0.2401 - accuracy: 0.93 - ETA: 0s - loss: 0.2397 - accuracy: 0.93 - ETA: 0s - loss: 0.2391 - accuracy: 0.93 - ETA: 0s - loss: 0.2395 - accuracy: 0.93 - ETA: 0s - loss: 0.2391 - accuracy: 0.93 - ETA: 0s - loss: 0.2385 - accuracy: 0.93 - ETA: 0s - loss: 0.2383 - accuracy: 0.93 - ETA: 0s - loss: 0.2381 - accuracy: 0.93 - ETA: 0s - loss: 0.2369 - accuracy: 0.93 - ETA: 0s - loss: 0.2360 - accuracy: 0.93 - ETA: 0s - loss: 0.2352 - accuracy: 0.93 - ETA: 0s - loss: 0.2350 - accuracy: 0.93 - ETA: 0s - loss: 0.2348 - accuracy: 0.93 - ETA: 0s - loss: 0.2352 - accuracy: 0.93 - ETA: 0s - loss: 0.2342 - accuracy: 0.93 - ETA: 0s - loss: 0.2336 - accuracy: 0.93 - 3s 2ms/step - loss: 0.2329 - accuracy: 0.9359 - val_loss: 0.2007 - val_accuracy: 0.9428\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.1624 - accuracy: 0.93 - ETA: 3s - loss: 0.1981 - accuracy: 0.94 - ETA: 3s - loss: 0.2155 - accuracy: 0.94 - ETA: 2s - loss: 0.2184 - accuracy: 0.94 - ETA: 2s - loss: 0.2102 - accuracy: 0.94 - ETA: 2s - loss: 0.2063 - accuracy: 0.94 - ETA: 2s - loss: 0.2055 - accuracy: 0.94 - ETA: 2s - loss: 0.1993 - accuracy: 0.94 - ETA: 2s - loss: 0.1966 - accuracy: 0.94 - ETA: 2s - loss: 0.1972 - accuracy: 0.94 - ETA: 2s - loss: 0.2007 - accuracy: 0.94 - ETA: 2s - loss: 0.1996 - accuracy: 0.94 - ETA: 2s - loss: 0.2002 - accuracy: 0.94 - ETA: 2s - loss: 0.2001 - accuracy: 0.94 - ETA: 2s - loss: 0.1985 - accuracy: 0.94 - ETA: 2s - loss: 0.1956 - accuracy: 0.94 - ETA: 2s - loss: 0.1945 - accuracy: 0.94 - ETA: 2s - loss: 0.1937 - accuracy: 0.94 - ETA: 2s - loss: 0.1922 - accuracy: 0.94 - ETA: 2s - loss: 0.1941 - accuracy: 0.94 - ETA: 2s - loss: 0.1938 - accuracy: 0.94 - ETA: 2s - loss: 0.1943 - accuracy: 0.94 - ETA: 2s - loss: 0.1934 - accuracy: 0.94 - ETA: 1s - loss: 0.1913 - accuracy: 0.94 - ETA: 1s - loss: 0.1905 - accuracy: 0.94 - ETA: 1s - loss: 0.1901 - accuracy: 0.94 - ETA: 1s - loss: 0.1897 - accuracy: 0.94 - ETA: 1s - loss: 0.1896 - accuracy: 0.94 - ETA: 1s - loss: 0.1890 - accuracy: 0.94 - ETA: 1s - loss: 0.1894 - accuracy: 0.94 - ETA: 1s - loss: 0.1894 - accuracy: 0.94 - ETA: 1s - loss: 0.1884 - accuracy: 0.94 - ETA: 1s - loss: 0.1887 - accuracy: 0.94 - ETA: 1s - loss: 0.1880 - accuracy: 0.94 - ETA: 1s - loss: 0.1884 - accuracy: 0.94 - ETA: 1s - loss: 0.1882 - accuracy: 0.94 - ETA: 1s - loss: 0.1880 - accuracy: 0.94 - ETA: 1s - loss: 0.1874 - accuracy: 0.94 - ETA: 1s - loss: 0.1871 - accuracy: 0.94 - ETA: 1s - loss: 0.1866 - accuracy: 0.94 - ETA: 1s - loss: 0.1870 - accuracy: 0.94 - ETA: 1s - loss: 0.1866 - accuracy: 0.94 - ETA: 1s - loss: 0.1868 - accuracy: 0.94 - ETA: 1s - loss: 0.1864 - accuracy: 0.94 - ETA: 0s - loss: 0.1862 - accuracy: 0.94 - ETA: 0s - loss: 0.1858 - accuracy: 0.94 - ETA: 0s - loss: 0.1865 - accuracy: 0.94 - ETA: 0s - loss: 0.1861 - accuracy: 0.94 - ETA: 0s - loss: 0.1857 - accuracy: 0.94 - ETA: 0s - loss: 0.1848 - accuracy: 0.94 - ETA: 0s - loss: 0.1846 - accuracy: 0.94 - ETA: 0s - loss: 0.1845 - accuracy: 0.94 - ETA: 0s - loss: 0.1841 - accuracy: 0.94 - ETA: 0s - loss: 0.1837 - accuracy: 0.95 - ETA: 0s - loss: 0.1834 - accuracy: 0.95 - ETA: 0s - loss: 0.1831 - accuracy: 0.94 - ETA: 0s - loss: 0.1831 - accuracy: 0.95 - ETA: 0s - loss: 0.1828 - accuracy: 0.95 - ETA: 0s - loss: 0.1829 - accuracy: 0.94 - ETA: 0s - loss: 0.1826 - accuracy: 0.94 - ETA: 0s - loss: 0.1817 - accuracy: 0.95 - ETA: 0s - loss: 0.1816 - accuracy: 0.95 - ETA: 0s - loss: 0.1817 - accuracy: 0.95 - ETA: 0s - loss: 0.1815 - accuracy: 0.95 - 3s 2ms/step - loss: 0.1814 - accuracy: 0.9503 - val_loss: 0.1641 - val_accuracy: 0.9543\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.1191 - accuracy: 0.96 - ETA: 2s - loss: 0.1360 - accuracy: 0.96 - ETA: 3s - loss: 0.1646 - accuracy: 0.95 - ETA: 3s - loss: 0.1643 - accuracy: 0.95 - ETA: 2s - loss: 0.1528 - accuracy: 0.95 - ETA: 2s - loss: 0.1476 - accuracy: 0.95 - ETA: 2s - loss: 0.1506 - accuracy: 0.95 - ETA: 2s - loss: 0.1492 - accuracy: 0.95 - ETA: 2s - loss: 0.1486 - accuracy: 0.95 - ETA: 2s - loss: 0.1527 - accuracy: 0.95 - ETA: 2s - loss: 0.1544 - accuracy: 0.95 - ETA: 2s - loss: 0.1543 - accuracy: 0.95 - ETA: 2s - loss: 0.1537 - accuracy: 0.95 - ETA: 2s - loss: 0.1559 - accuracy: 0.95 - ETA: 2s - loss: 0.1571 - accuracy: 0.95 - ETA: 2s - loss: 0.1575 - accuracy: 0.95 - ETA: 2s - loss: 0.1557 - accuracy: 0.95 - ETA: 2s - loss: 0.1548 - accuracy: 0.95 - ETA: 2s - loss: 0.1549 - accuracy: 0.95 - ETA: 2s - loss: 0.1548 - accuracy: 0.95 - ETA: 2s - loss: 0.1547 - accuracy: 0.95 - ETA: 2s - loss: 0.1543 - accuracy: 0.95 - ETA: 2s - loss: 0.1538 - accuracy: 0.95 - ETA: 2s - loss: 0.1522 - accuracy: 0.95 - ETA: 2s - loss: 0.1521 - accuracy: 0.95 - ETA: 1s - loss: 0.1522 - accuracy: 0.95 - ETA: 1s - loss: 0.1518 - accuracy: 0.95 - ETA: 1s - loss: 0.1507 - accuracy: 0.95 - ETA: 1s - loss: 0.1516 - accuracy: 0.95 - ETA: 1s - loss: 0.1517 - accuracy: 0.95 - ETA: 1s - loss: 0.1516 - accuracy: 0.95 - ETA: 1s - loss: 0.1511 - accuracy: 0.95 - ETA: 1s - loss: 0.1510 - accuracy: 0.95 - ETA: 1s - loss: 0.1508 - accuracy: 0.95 - ETA: 1s - loss: 0.1504 - accuracy: 0.95 - ETA: 1s - loss: 0.1510 - accuracy: 0.95 - ETA: 1s - loss: 0.1505 - accuracy: 0.95 - ETA: 1s - loss: 0.1512 - accuracy: 0.95 - ETA: 1s - loss: 0.1517 - accuracy: 0.95 - ETA: 1s - loss: 0.1517 - accuracy: 0.95 - ETA: 1s - loss: 0.1520 - accuracy: 0.95 - ETA: 1s - loss: 0.1521 - accuracy: 0.95 - ETA: 1s - loss: 0.1518 - accuracy: 0.95 - ETA: 1s - loss: 0.1520 - accuracy: 0.95 - ETA: 1s - loss: 0.1522 - accuracy: 0.95 - ETA: 0s - loss: 0.1521 - accuracy: 0.95 - ETA: 0s - loss: 0.1519 - accuracy: 0.95 - ETA: 0s - loss: 0.1513 - accuracy: 0.95 - ETA: 0s - loss: 0.1510 - accuracy: 0.95 - ETA: 0s - loss: 0.1505 - accuracy: 0.95 - ETA: 0s - loss: 0.1501 - accuracy: 0.95 - ETA: 0s - loss: 0.1505 - accuracy: 0.95 - ETA: 0s - loss: 0.1503 - accuracy: 0.95 - ETA: 0s - loss: 0.1501 - accuracy: 0.95 - ETA: 0s - loss: 0.1506 - accuracy: 0.95 - ETA: 0s - loss: 0.1499 - accuracy: 0.95 - ETA: 0s - loss: 0.1497 - accuracy: 0.95 - ETA: 0s - loss: 0.1496 - accuracy: 0.95 - ETA: 0s - loss: 0.1493 - accuracy: 0.95 - ETA: 0s - loss: 0.1494 - accuracy: 0.95 - ETA: 0s - loss: 0.1492 - accuracy: 0.95 - ETA: 0s - loss: 0.1489 - accuracy: 0.95 - ETA: 0s - loss: 0.1489 - accuracy: 0.95 - ETA: 0s - loss: 0.1493 - accuracy: 0.95 - ETA: 0s - loss: 0.1492 - accuracy: 0.95 - 3s 2ms/step - loss: 0.1492 - accuracy: 0.9586 - val_loss: 0.1394 - val_accuracy: 0.9589\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.1645 - accuracy: 0.93 - ETA: 3s - loss: 0.1212 - accuracy: 0.96 - ETA: 3s - loss: 0.1196 - accuracy: 0.96 - ETA: 3s - loss: 0.1397 - accuracy: 0.96 - ETA: 3s - loss: 0.1307 - accuracy: 0.96 - ETA: 3s - loss: 0.1281 - accuracy: 0.96 - ETA: 2s - loss: 0.1319 - accuracy: 0.96 - ETA: 2s - loss: 0.1314 - accuracy: 0.96 - ETA: 2s - loss: 0.1301 - accuracy: 0.96 - ETA: 2s - loss: 0.1296 - accuracy: 0.96 - ETA: 2s - loss: 0.1278 - accuracy: 0.96 - ETA: 2s - loss: 0.1304 - accuracy: 0.96 - ETA: 2s - loss: 0.1321 - accuracy: 0.96 - ETA: 2s - loss: 0.1321 - accuracy: 0.96 - ETA: 2s - loss: 0.1312 - accuracy: 0.96 - ETA: 2s - loss: 0.1296 - accuracy: 0.96 - ETA: 2s - loss: 0.1307 - accuracy: 0.96 - ETA: 2s - loss: 0.1298 - accuracy: 0.96 - ETA: 2s - loss: 0.1301 - accuracy: 0.96 - ETA: 2s - loss: 0.1312 - accuracy: 0.96 - ETA: 2s - loss: 0.1313 - accuracy: 0.96 - ETA: 2s - loss: 0.1315 - accuracy: 0.96 - ETA: 2s - loss: 0.1314 - accuracy: 0.96 - ETA: 2s - loss: 0.1321 - accuracy: 0.96 - ETA: 2s - loss: 0.1323 - accuracy: 0.96 - ETA: 1s - loss: 0.1316 - accuracy: 0.96 - ETA: 1s - loss: 0.1311 - accuracy: 0.96 - ETA: 1s - loss: 0.1305 - accuracy: 0.96 - ETA: 1s - loss: 0.1306 - accuracy: 0.96 - ETA: 1s - loss: 0.1307 - accuracy: 0.96 - ETA: 1s - loss: 0.1314 - accuracy: 0.96 - ETA: 1s - loss: 0.1301 - accuracy: 0.96 - ETA: 1s - loss: 0.1299 - accuracy: 0.96 - ETA: 1s - loss: 0.1299 - accuracy: 0.96 - ETA: 1s - loss: 0.1293 - accuracy: 0.96 - ETA: 1s - loss: 0.1284 - accuracy: 0.96 - ETA: 1s - loss: 0.1287 - accuracy: 0.96 - ETA: 1s - loss: 0.1284 - accuracy: 0.96 - ETA: 1s - loss: 0.1276 - accuracy: 0.96 - ETA: 1s - loss: 0.1272 - accuracy: 0.96 - ETA: 1s - loss: 0.1276 - accuracy: 0.96 - ETA: 1s - loss: 0.1272 - accuracy: 0.96 - ETA: 1s - loss: 0.1273 - accuracy: 0.96 - ETA: 1s - loss: 0.1272 - accuracy: 0.96 - ETA: 1s - loss: 0.1276 - accuracy: 0.96 - ETA: 0s - loss: 0.1272 - accuracy: 0.96 - ETA: 0s - loss: 0.1272 - accuracy: 0.96 - ETA: 0s - loss: 0.1275 - accuracy: 0.96 - ETA: 0s - loss: 0.1272 - accuracy: 0.96 - ETA: 0s - loss: 0.1273 - accuracy: 0.96 - ETA: 0s - loss: 0.1267 - accuracy: 0.96 - ETA: 0s - loss: 0.1271 - accuracy: 0.96 - ETA: 0s - loss: 0.1272 - accuracy: 0.96 - ETA: 0s - loss: 0.1272 - accuracy: 0.96 - ETA: 0s - loss: 0.1276 - accuracy: 0.96 - ETA: 0s - loss: 0.1272 - accuracy: 0.96 - ETA: 0s - loss: 0.1274 - accuracy: 0.96 - ETA: 0s - loss: 0.1276 - accuracy: 0.96 - ETA: 0s - loss: 0.1274 - accuracy: 0.96 - ETA: 0s - loss: 0.1269 - accuracy: 0.96 - ETA: 0s - loss: 0.1268 - accuracy: 0.96 - ETA: 0s - loss: 0.1265 - accuracy: 0.96 - ETA: 0s - loss: 0.1261 - accuracy: 0.96 - ETA: 0s - loss: 0.1259 - accuracy: 0.96 - 3s 2ms/step - loss: 0.1260 - accuracy: 0.9648 - val_loss: 0.1246 - val_accuracy: 0.9644\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 1f3d9949c03772980815879ca0362bb5</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.9644666512807211</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-learning_rate: 0.0001</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units: 352</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner.search(X_train, y_train,\n",
    "             epochs=5,\n",
    "             validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VCmme1xjIIMG"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tuner' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ae7185f22d37>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtuner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults_summary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'tuner' is not defined"
     ]
    }
   ],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hgVwg4MtIIMI"
   },
   "source": [
    "## Challenge\n",
    "\n",
    "Try to apply RandomSearchCV to your module project today. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h-e_aIT8IIMI"
   },
   "source": [
    "# Review\n",
    "* <a href=\"#p1\">Part 1</a>: Describe the major hyperparemeters to tune\n",
    "    - Activation Functions\n",
    "    - Optimizer\n",
    "    - Number of Layers\n",
    "    - Number of Neurons\n",
    "    - Batch Size\n",
    "    - Dropout Regulaization\n",
    "    - Learning Rate\n",
    "    - Number of Epochs\n",
    "    - and many more\n",
    "* <a href=\"#p2\">Part 2</a>: Implement an experiment tracking framework\n",
    "    - Weights & Biases\n",
    "    - Comet.ml\n",
    "    - By Hand / GridSearch\n",
    "    - TensorBoard\n",
    "* <a href=\"#p3\">Part 3</a>: Search the hyperparameter space using RandomSearch\n",
    "    - Keras-Tuner\n",
    "    - Advanced Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J7it81sfIIMJ"
   },
   "source": [
    "# Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xPkG9C2hIIMJ"
   },
   "source": [
    "## Additional Reading\n",
    "- https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/\n",
    "- https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/\n",
    "- https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n",
    "- https://machinelearningmastery.com/introduction-to-weight-constraints-to-reduce-generalization-error-in-deep-learning/\n",
    "- https://machinelearningmastery.com/how-to-configure-the-number-of-layers-and-nodes-in-a-neural-network/"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_424_Tune_Lecture.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "U4-S1-NLP (Python3)",
   "language": "python",
   "name": "u4-s1-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
