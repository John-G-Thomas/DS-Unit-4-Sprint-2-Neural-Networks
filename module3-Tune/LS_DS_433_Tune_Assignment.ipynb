{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NGGrt9EYlCqY"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Train Practice\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2 Assignment 3*\n",
    "\n",
    "Continue to use TensorFlow Keras & a sample of the [Quickdraw dataset](https://github.com/googlecreativelab/quickdraw-dataset) to build a sketch classification model. The dataset has been sampled to only 10 classes and 10000 observations per class. Using your baseline model from yesterday, hyperparameter tune it and report on your highest validation accuracy. Your singular goal today is to achieve the highest accuracy possible.\n",
    "\n",
    "*Don't forgot to switch to GPU on Colab!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ptJ2b3wk62Ud"
   },
   "source": [
    "### Hyperparameters to Tune\n",
    "\n",
    "At a minimum, tune each of these hyperparameters using any strategy we discussed during lecture today: \n",
    "- Optimizer\n",
    "- Learning Rate\n",
    "- Activiation Function\n",
    "  - At least 1 subparameter within the Relu activation function\n",
    "- Number of Neurons in Hidden Layers\n",
    "- Number of Hidden Layers\n",
    "- Weight Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cell\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import sklearn.model_selection as model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "import datetime\n",
    "# Our Model\n",
    "from tensorflow.keras.optimizers import Adamax\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorboard.plugins.hparams import api as hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "USXjs7Hk71Hy"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90000, 10000, 90000, 10000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your Code Starts Here - load in data and spilt\n",
    "data = np.load('quickdraw10.npz')\n",
    "def load_quickdraw10npz(path):\n",
    "    X = path['arr_0']\n",
    "    y = path['arr_1']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, train_size = 0.9, test_size = 0.1)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "X_train, X_test, y_train, y_test = load_quickdraw10npz(path=data)\n",
    "len(X_train), len(X_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes((90000, 784), (90000,), (10000, 784), (10000,))\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "y_train = y_train / 255.0\n",
    "y_test = y_test / 255.0\n",
    "print(f'shapes{X_train.shape,y_train.shape,X_test.shape,y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(opt, drop, activation, input_shape, output_size, n_layers):\n",
    "    opt = opt(lr=.001)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(50, activation=activation, input_shape=input_shape))\n",
    "    model.add(Dropout(drop))\n",
    "    for _ in range(n_layers):\n",
    "        model.add(Dense(50, activation=activation))\n",
    "        model.add(Dropout(drop))\n",
    "    if output_size == 1:\n",
    "        model.add(Dense(output_size, activation='sigmoid'))\n",
    "        model.compile(loss='binary_crossentropy', metrics=['mae', 'mse'], optimizer='adam')\n",
    "    else:\n",
    "        model.add(Dense(output_size, activation='softmax'))\n",
    "        model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=opt)\n",
    "    return model\"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dropout' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-873c3e54f87f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# create model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKerasClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuild_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m.02\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'softmax'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m526\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# define the grid search parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# batch_size = [10, 20, 40, 60, 80, 100]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-35-20fe34386901>\u001b[0m in \u001b[0;36mmodel\u001b[1;34m(opt, drop, activation, input_shape, output_size, n_layers)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Dropout' is not defined"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "model = KerasClassifier(build_fn=model(SGD,drop=.02,activation='softmax',input_shape=(32,0),output_size=526,n_layers=2), verbose=1)\n",
    "\n",
    "# define the grid search parameters\n",
    "# batch_size = [10, 20, 40, 60, 80, 100]\n",
    "# param_grid = dict(batch_size=batch_sizeiii, epochs=epochs)\n",
    "\n",
    "# define the grid search parameters\n",
    "param_grid = {'batch_size': [32,100,512],\n",
    "              'epochs': [25],\n",
    "              'units': [32],\n",
    "              # TODO add more params\n",
    "              }\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(X_train, y_train, validation_data=(X_test, y_test))\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown activation function: iisoftmax",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-c28651aed3a9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;31m# Create Grid Search\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[0mgrid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[0mgrid_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;31m# Report Results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    737\u001b[0m             \u001b[0mrefit_start_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 739\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    740\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    741\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\tensorflow\\python\\keras\\wrappers\\scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[0;32m    221\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Invalid shape for y: '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_classes_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mKerasClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\tensorflow\\python\\keras\\wrappers\\scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m           **self.filter_sk_params(self.build_fn.__call__))\n\u001b[0;32m    156\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter_sk_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m     if (losses.is_categorical_crossentropy(self.model.loss) and\n",
      "\u001b[1;32m<ipython-input-4-c28651aed3a9>\u001b[0m in \u001b[0;36mcreate_model\u001b[1;34m(units)\u001b[0m\n\u001b[0;32m     10\u001b[0m       ])\n\u001b[0;32m     11\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m784\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'iisoftmax'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[1;31m# Compile model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     model.compile(optimizer=opt, loss='sparse_categorical_crossentropy',\n",
      "\u001b[1;32m~\\anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, units, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1146\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munits\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0munits\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1147\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1148\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_bias\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muse_bias\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1149\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel_initializer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minitializers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkernel_initializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\tensorflow\\python\\keras\\activations.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(identifier)\u001b[0m\n\u001b[0;32m    527\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m     \u001b[0midentifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 529\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    530\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\tensorflow\\python\\keras\\activations.py\u001b[0m in \u001b[0;36mdeserialize\u001b[1;34m(name, custom_objects)\u001b[0m\n\u001b[0;32m    490\u001b[0m       \u001b[0mmodule_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mglobals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m       \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 492\u001b[1;33m       printable_module_name='activation function')\n\u001b[0m\u001b[0;32m    493\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[0;32m    376\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m         raise ValueError(\n\u001b[1;32m--> 378\u001b[1;33m             'Unknown ' + printable_module_name + ': ' + object_name)\n\u001b[0m\u001b[0;32m    379\u001b[0m     \u001b[1;31m# Classes passed by name are instantiated with no args, functions are\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m     \u001b[1;31m# returned as-is.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown activation function: iisoftmax"
     ]
    }
   ],
   "source": [
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(units=32):\n",
    "    opt = tf.keras.optimizers.Adagrad(\n",
    "    learning_rate=0.001, initial_accumulator_value=0.1, epsilon=1e-07,\n",
    "    name='Adagrad')\n",
    "    model = Sequential(\n",
    "      [\n",
    "       Dense(50, activation='relu', input_dim=784),\n",
    "       Dense(50, activation='relu')\n",
    "      ])\n",
    "    model.add(Dense(units, input_dim=784, activation='relu'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(optimizer=opt, loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=1)\n",
    "\n",
    "# define the grid search parameters\n",
    "# batch_size = [10, 20, 40, 60, 80, 100]\n",
    "# param_grid = dict(batch_size=batch_sizeiii, epochs=epochs)\n",
    "\n",
    "# define the grid search parameters\n",
    "param_grid = {'batch_size': [32,64,512],\n",
    "              'epochs': [20],\n",
    "              'units': [32],\n",
    "              # TODO add more params\n",
    "              }\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(X_train, y_train, validation_data=(X_test, y_test))\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1094/1094 - 1s - loss: 2.4421 - accuracy: 0.1056 - val_loss: 2.1326 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/50\n",
      "1094/1094 - 1s - loss: 1.8255 - accuracy: 0.3033 - val_loss: 4.6850 - val_accuracy: 0.0891\n",
      "Epoch 3/50\n",
      "1094/1094 - 1s - loss: 1.5536 - accuracy: 0.4417 - val_loss: 5.5766 - val_accuracy: 0.0909\n",
      "Epoch 4/50\n",
      "1094/1094 - 1s - loss: 1.4474 - accuracy: 0.4951 - val_loss: 5.6107 - val_accuracy: 0.0925\n",
      "Epoch 5/50\n",
      "1094/1094 - 1s - loss: 1.3870 - accuracy: 0.5235 - val_loss: 5.8763 - val_accuracy: 0.0917\n",
      "Epoch 6/50\n",
      "1094/1094 - 1s - loss: 1.3479 - accuracy: 0.5435 - val_loss: 6.0069 - val_accuracy: 0.0915\n",
      "Epoch 7/50\n",
      "1094/1094 - 1s - loss: 1.3146 - accuracy: 0.5554 - val_loss: 6.2407 - val_accuracy: 0.0913\n",
      "Epoch 8/50\n",
      "1094/1094 - 1s - loss: 1.2943 - accuracy: 0.5645 - val_loss: 6.2886 - val_accuracy: 0.0909\n",
      "Epoch 9/50\n",
      "1094/1094 - 1s - loss: 1.2785 - accuracy: 0.5694 - val_loss: 6.1062 - val_accuracy: 0.0924\n",
      "Epoch 10/50\n",
      "1094/1094 - 1s - loss: 1.2666 - accuracy: 0.5746 - val_loss: 6.1713 - val_accuracy: 0.0918\n",
      "Epoch 11/50\n",
      "1094/1094 - 1s - loss: 1.2551 - accuracy: 0.5795 - val_loss: 6.2789 - val_accuracy: 0.0916\n",
      "Epoch 12/50\n",
      "1094/1094 - 1s - loss: 1.2472 - accuracy: 0.5784 - val_loss: 6.3400 - val_accuracy: 0.0911\n",
      "Epoch 13/50\n",
      "1094/1094 - 1s - loss: 1.2394 - accuracy: 0.5812 - val_loss: 6.2585 - val_accuracy: 0.0918\n",
      "Epoch 14/50\n",
      "1094/1094 - 1s - loss: 1.2351 - accuracy: 0.5853 - val_loss: 6.4385 - val_accuracy: 0.0916\n",
      "Epoch 15/50\n",
      "1094/1094 - 1s - loss: 1.2293 - accuracy: 0.5842 - val_loss: 6.3477 - val_accuracy: 0.0913\n",
      "Epoch 16/50\n",
      "1094/1094 - 1s - loss: 1.2249 - accuracy: 0.5897 - val_loss: 6.3712 - val_accuracy: 0.0916\n",
      "Epoch 17/50\n",
      "1094/1094 - 1s - loss: 1.2200 - accuracy: 0.5896 - val_loss: 6.3003 - val_accuracy: 0.0917\n",
      "Epoch 18/50\n",
      "1094/1094 - 1s - loss: 1.2138 - accuracy: 0.5924 - val_loss: 6.3740 - val_accuracy: 0.0912\n",
      "Epoch 19/50\n",
      "1094/1094 - 1s - loss: 1.2092 - accuracy: 0.5921 - val_loss: 6.4245 - val_accuracy: 0.0913\n",
      "Epoch 20/50\n",
      "1094/1094 - 1s - loss: 1.2065 - accuracy: 0.5934 - val_loss: 6.2492 - val_accuracy: 0.0915\n",
      "Epoch 21/50\n",
      "1094/1094 - 1s - loss: 1.2007 - accuracy: 0.5972 - val_loss: 6.2079 - val_accuracy: 0.0921\n",
      "Epoch 22/50\n",
      "1094/1094 - 1s - loss: 1.2011 - accuracy: 0.5952 - val_loss: 6.3632 - val_accuracy: 0.0914\n",
      "Epoch 23/50\n",
      "1094/1094 - 1s - loss: 1.1941 - accuracy: 0.5980 - val_loss: 6.2110 - val_accuracy: 0.0918\n",
      "Epoch 24/50\n",
      "1094/1094 - 1s - loss: 1.1898 - accuracy: 0.5990 - val_loss: 6.3459 - val_accuracy: 0.0910\n",
      "Epoch 25/50\n",
      "1094/1094 - 1s - loss: 1.1890 - accuracy: 0.6004 - val_loss: 6.4059 - val_accuracy: 0.0909\n",
      "Epoch 26/50\n",
      "1094/1094 - 1s - loss: 1.1861 - accuracy: 0.6020 - val_loss: 6.4708 - val_accuracy: 0.0906\n",
      "Epoch 27/50\n",
      "1094/1094 - 1s - loss: 1.1854 - accuracy: 0.6006 - val_loss: 6.3040 - val_accuracy: 0.0910\n",
      "Epoch 28/50\n",
      "1094/1094 - 1s - loss: 1.1854 - accuracy: 0.5999 - val_loss: 6.1650 - val_accuracy: 0.0916\n",
      "Epoch 29/50\n",
      "1094/1094 - 1s - loss: 1.1821 - accuracy: 0.6020 - val_loss: 6.2181 - val_accuracy: 0.0914\n",
      "Epoch 30/50\n",
      "1094/1094 - 1s - loss: 1.1779 - accuracy: 0.6029 - val_loss: 6.3760 - val_accuracy: 0.0909\n",
      "Epoch 31/50\n",
      "1094/1094 - 1s - loss: 1.1767 - accuracy: 0.6043 - val_loss: 6.2240 - val_accuracy: 0.0913\n",
      "Epoch 32/50\n",
      "1094/1094 - 1s - loss: 1.1762 - accuracy: 0.6027 - val_loss: 6.2390 - val_accuracy: 0.0911\n",
      "Epoch 33/50\n",
      "1094/1094 - 1s - loss: 1.1689 - accuracy: 0.6071 - val_loss: 6.2967 - val_accuracy: 0.0901\n",
      "Epoch 34/50\n",
      "1094/1094 - 1s - loss: 1.1675 - accuracy: 0.6084 - val_loss: 6.3930 - val_accuracy: 0.0902\n",
      "Epoch 35/50\n",
      "1094/1094 - 1s - loss: 1.1673 - accuracy: 0.6083 - val_loss: 6.4044 - val_accuracy: 0.0895\n",
      "Epoch 36/50\n",
      "1094/1094 - 1s - loss: 1.1632 - accuracy: 0.6087 - val_loss: 6.2992 - val_accuracy: 0.0903\n",
      "Epoch 37/50\n",
      "1094/1094 - 1s - loss: 1.1635 - accuracy: 0.6094 - val_loss: 6.3469 - val_accuracy: 0.0907\n",
      "Epoch 38/50\n",
      "1094/1094 - 1s - loss: 1.1614 - accuracy: 0.6115 - val_loss: 6.3031 - val_accuracy: 0.0904\n",
      "Epoch 39/50\n",
      "1094/1094 - 1s - loss: 1.1652 - accuracy: 0.6096 - val_loss: 6.3065 - val_accuracy: 0.0904\n",
      "Epoch 40/50\n",
      "1094/1094 - 1s - loss: 1.1596 - accuracy: 0.6104 - val_loss: 6.3323 - val_accuracy: 0.0902\n",
      "Epoch 41/50\n",
      "1094/1094 - 1s - loss: 1.1591 - accuracy: 0.6125 - val_loss: 6.1947 - val_accuracy: 0.0910\n",
      "Epoch 42/50\n",
      "1094/1094 - 1s - loss: 1.1631 - accuracy: 0.6096 - val_loss: 6.4552 - val_accuracy: 0.0886\n",
      "Epoch 43/50\n",
      "1094/1094 - 1s - loss: 1.1572 - accuracy: 0.6121 - val_loss: 6.3663 - val_accuracy: 0.0899\n",
      "Epoch 44/50\n",
      "1094/1094 - 1s - loss: 1.1514 - accuracy: 0.6152 - val_loss: 6.2701 - val_accuracy: 0.0904\n",
      "Epoch 45/50\n",
      "1094/1094 - 1s - loss: 1.1524 - accuracy: 0.6165 - val_loss: 6.2509 - val_accuracy: 0.0910\n",
      "Epoch 46/50\n",
      "1094/1094 - 1s - loss: 1.1513 - accuracy: 0.6145 - val_loss: 6.3980 - val_accuracy: 0.0896\n",
      "Epoch 47/50\n",
      "1094/1094 - 1s - loss: 1.1545 - accuracy: 0.6144 - val_loss: 6.3086 - val_accuracy: 0.0908\n",
      "Epoch 48/50\n",
      "1094/1094 - 2s - loss: 1.1519 - accuracy: 0.6151 - val_loss: 6.3115 - val_accuracy: 0.0905\n",
      "Epoch 49/50\n",
      "1094/1094 - 2s - loss: 1.1469 - accuracy: 0.6180 - val_loss: 6.3274 - val_accuracy: 0.0908\n",
      "Epoch 50/50\n",
      "1094/1094 - 2s - loss: 1.1494 - accuracy: 0.6163 - val_loss: 6.3852 - val_accuracy: 0.0902\n",
      "Best: 0.7265142917633056 using {'batch_size': 64, 'epochs': 50, 'units': 32}\n",
      "Means: 0.7152142882347107, Stdev: 0.0063260037867725214 with: {'batch_size': 10, 'epochs': 20, 'units': 32}\n",
      "Means: 0.7231428503990174, Stdev: 0.003317536990134174 with: {'batch_size': 10, 'epochs': 50, 'units': 32}\n",
      "Means: 0.7198857069015503, Stdev: 0.004893904959524886 with: {'batch_size': 32, 'epochs': 20, 'units': 32}\n",
      "Means: 0.7217714190483093, Stdev: 0.005146704428823075 with: {'batch_size': 32, 'epochs': 50, 'units': 32}\n",
      "Means: 0.710699999332428, Stdev: 0.006818245706666747 with: {'batch_size': 64, 'epochs': 20, 'units': 32}\n",
      "Means: 0.7265142917633056, Stdev: 0.004341162230585409 with: {'batch_size': 64, 'epochs': 50, 'units': 32}\n",
      "Means: 0.3148571401834488, Stdev: 0.2121810084634621 with: {'batch_size': 512, 'epochs': 20, 'units': 32}\n",
      "Means: 0.7053285717964173, Stdev: 0.008373615998466526 with: {'batch_size': 512, 'epochs': 50, 'units': 32}\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "for hl1 in [250, 200, 150, 100, 75, 50, 25, 15, 10, 7]:\n",
    "    def baseline_model(units=32):\n",
    "        opt = tf.keras.optimizers.Adamax(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name=\"Adamax\")\n",
    "        model = Sequential()\n",
    "        model.add(Dense(hl1, input_dim=784, kernel_initializer='normal', activation='relu'))\n",
    "        model.add(Dropout(0.2,noise_shape=None, seed=None))\n",
    "        model.add(Dense(100, kernel_initializer='normal', activation='sigmoid'))\n",
    "        model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "        return model\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=baseline_model, verbose=1)\n",
    "\n",
    "# define the grid search parameters\n",
    "# batch_size = [10, 20, 40, 60, 80, 100]\n",
    "# param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "# define the grid search parameters\n",
    "param_grid = {'batch_size': [10,32,64,512],\n",
    "              'epochs': [20,50],\n",
    "              'units': [32],\n",
    "              # TODO add more params\n",
    "              }\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(X_train, y_train, validation_data=(X_test, y_test),verbose=2)\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model2(lr,opt):\n",
    "    \"\"\"Insert Doc String\"\"\"\n",
    "    opt = opt(lr)\n",
    "    model = Sequential([\n",
    "        Dense(32, activation=\"relu\", input_dim=784),\n",
    "        Dense(32, activation=\"relu\"),\n",
    "        Dense(10, activation=\"softmax\")\n",
    "      ])\n",
    "    model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't pickle _thread.RLock objects",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-f2c39ddfd718>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;31m# Create Grid Search\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mgrid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0mgrid_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;31m# Report Results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    653\u001b[0m         \u001b[0mn_splits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_n_splits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    654\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 655\u001b[1;33m         \u001b[0mbase_estimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m         parallel = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mclone\u001b[1;34m(estimator, safe)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[0mnew_object_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnew_object_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m         \u001b[0mnew_object_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m     \u001b[0mnew_object\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mnew_object_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[0mparams_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_object\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mclone\u001b[1;34m(estimator, safe)\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'get_params'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msafe\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m             raise TypeError(\"Cannot clone object '%s' (type %s): \"\n",
      "\u001b[1;32m~\\anaconda3\\envs\\U4-S1-NLP\\lib\\copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    178\u001b[0m                     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m                     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_reconstruct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[1;31m# If is its own copy, don't memoize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\U4-S1-NLP\\lib\\copy.py\u001b[0m in \u001b[0;36m_reconstruct\u001b[1;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[0;32m    278\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 280\u001b[1;33m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    281\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__setstate__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__setstate__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\U4-S1-NLP\\lib\\copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\U4-S1-NLP\\lib\\copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[1;34m(x, memo, deepcopy)\u001b[0m\n\u001b[0;32m    238\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 240\u001b[1;33m         \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    241\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\U4-S1-NLP\\lib\\copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\U4-S1-NLP\\lib\\copy.py\u001b[0m in \u001b[0;36m_deepcopy_list\u001b[1;34m(x, memo, deepcopy)\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[0mappend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 215\u001b[1;33m         \u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    216\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_deepcopy_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\U4-S1-NLP\\lib\\copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    178\u001b[0m                     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m                     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_reconstruct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[1;31m# If is its own copy, don't memoize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\U4-S1-NLP\\lib\\copy.py\u001b[0m in \u001b[0;36m_reconstruct\u001b[1;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[0;32m    278\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 280\u001b[1;33m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    281\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__setstate__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__setstate__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\U4-S1-NLP\\lib\\copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\U4-S1-NLP\\lib\\copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[1;34m(x, memo, deepcopy)\u001b[0m\n\u001b[0;32m    238\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 240\u001b[1;33m         \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    241\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\U4-S1-NLP\\lib\\copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\U4-S1-NLP\\lib\\copy.py\u001b[0m in \u001b[0;36m_deepcopy_list\u001b[1;34m(x, memo, deepcopy)\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[0mappend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 215\u001b[1;33m         \u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    216\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_deepcopy_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\U4-S1-NLP\\lib\\copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    178\u001b[0m                     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m                     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_reconstruct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[1;31m# If is its own copy, don't memoize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\U4-S1-NLP\\lib\\copy.py\u001b[0m in \u001b[0;36m_reconstruct\u001b[1;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[0;32m    278\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 280\u001b[1;33m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    281\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__setstate__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__setstate__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\U4-S1-NLP\\lib\\copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\U4-S1-NLP\\lib\\copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[1;34m(x, memo, deepcopy)\u001b[0m\n\u001b[0;32m    238\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 240\u001b[1;33m         \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    241\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\U4-S1-NLP\\lib\\copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    178\u001b[0m                     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m                     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_reconstruct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[1;31m# If is its own copy, don't memoize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\U4-S1-NLP\\lib\\copy.py\u001b[0m in \u001b[0;36m_reconstruct\u001b[1;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[0;32m    278\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 280\u001b[1;33m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    281\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__setstate__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__setstate__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\U4-S1-NLP\\lib\\copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\U4-S1-NLP\\lib\\copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[1;34m(x, memo, deepcopy)\u001b[0m\n\u001b[0;32m    238\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 240\u001b[1;33m         \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    241\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\U4-S1-NLP\\lib\\copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    178\u001b[0m                     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m                     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_reconstruct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[1;31m# If is its own copy, don't memoize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\U4-S1-NLP\\lib\\copy.py\u001b[0m in \u001b[0;36m_reconstruct\u001b[1;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[0;32m    278\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 280\u001b[1;33m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    281\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__setstate__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__setstate__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\U4-S1-NLP\\lib\\copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\U4-S1-NLP\\lib\\copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[1;34m(x, memo, deepcopy)\u001b[0m\n\u001b[0;32m    238\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 240\u001b[1;33m         \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    241\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\U4-S1-NLP\\lib\\copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    178\u001b[0m                     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m                     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_reconstruct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[1;31m# If is its own copy, don't memoize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\U4-S1-NLP\\lib\\copy.py\u001b[0m in \u001b[0;36m_reconstruct\u001b[1;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[0;32m    278\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 280\u001b[1;33m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    281\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__setstate__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__setstate__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\U4-S1-NLP\\lib\\copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\U4-S1-NLP\\lib\\copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[1;34m(x, memo, deepcopy)\u001b[0m\n\u001b[0;32m    238\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 240\u001b[1;33m         \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    241\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\U4-S1-NLP\\lib\\copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    167\u001b[0m                     \u001b[0mreductor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"__reduce_ex__\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mreductor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m                         \u001b[0mrv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreductor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m                         \u001b[0mreductor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"__reduce__\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: can't pickle _thread.RLock objects"
     ]
    }
   ],
   "source": [
    "def create_model3(lr,opt):\n",
    "    \"\"\"Insert Doc String\"\"\"\n",
    "    opt = opt(lr)\n",
    "    model = Sequential([\n",
    "        Dense(32, activation=\"relu\", input_dim=784),\n",
    "        Dense(32, activation=\"relu\"),\n",
    "        Dense(10, activation=\"softmax\")\n",
    "      ])\n",
    "    model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model3(.001, SGD))\n",
    "\n",
    "# define the grid search parameters\n",
    "# batch_size = [10, 20, 40, 60, 80, 100]\n",
    "# param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "# define the grid search parameters\n",
    "param_grid = {'batch_size': [100,512],\n",
    "              'epochs': [20],\n",
    "              'units': [32],\n",
    "              # TODO add more params\n",
    "              }\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(X_train, y_train, validation_data=(X_test, y_test))\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create Experiment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "HP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([16,32]))\n",
    "HP_LEARNING_RATE = hp.HParam('learning_rate', hp.RealInterval(0.001,.01))\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'Adagrad']))\n",
    "\n",
    "METRIC_ACCURACY = 'accuracy'\n",
    "\n",
    "with tf.summary.create_file_writer('logs/hparam_tuning').as_default():\n",
    "  hp.hparams_config(\n",
    "      hparams=[HP_NUM_UNITS, HP_LEARNING_RATE, HP_OPTIMIZER],\n",
    "      metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')]\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Adapt Model Function with HParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(hparams):\n",
    "  \n",
    "  model = tf.keras.Sequential(\n",
    "      [tf.keras.layers.Dense(hparams[HP_NUM_UNITS], activation='relu'),\n",
    "       tf.keras.layers.Dense(10, activation='softmax')      \n",
    "  ])\n",
    "\n",
    "  opt_name = hparams[HP_OPTIMIZER]\n",
    "  lr = hparams[HP_LEARNING_RATE]\n",
    "\n",
    "  if opt_name == 'adam':\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "  elif opt_name == 'Adagrad':\n",
    "    opt = tf.keras.optimizers.Adagrad(learning_rate=lr, initial_accumulator_value=0.1, epsilon=1e-07)\n",
    "  else:\n",
    "    raise ValueError(f'Unexpected optimizer: {opt_name}')\n",
    "\n",
    "  model.compile(\n",
    "      optimizer=opt,\n",
    "      loss='sparse_categorical_crossentropy',\n",
    "      metrics=['accuracy']\n",
    "  )\n",
    "\n",
    "  model.fit(X_train, y_train, epochs=5)\n",
    "  _, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "  # Python convention: if a variable doesn't need a name, give it _\n",
    "  # ten_ones = [1 for _ in range(10)]\n",
    "\n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(run_dir, hparams):\n",
    "  with tf.summary.create_file_writer(run_dir).as_default():\n",
    "    hp.hparams(hparams)  # record the values used in this trial\n",
    "    accuracy = train_test_model(hparams)\n",
    "    tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting trial: run-0\n",
      "{'num_units': 16, 'learning_rate': 0.001, 'optimizer': 'Adagrad'}\n",
      "Epoch 1/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 2.2148 - accuracy: 0.06 - ETA: 1s - loss: 1.2655 - accuracy: 0.08 - ETA: 1s - loss: 0.9208 - accuracy: 0.09 - ETA: 1s - loss: 0.7309 - accuracy: 0.09 - ETA: 1s - loss: 0.6096 - accuracy: 0.09 - ETA: 1s - loss: 0.5236 - accuracy: 0.09 - ETA: 1s - loss: 0.4626 - accuracy: 0.09 - ETA: 1s - loss: 0.4151 - accuracy: 0.10 - ETA: 0s - loss: 0.3775 - accuracy: 0.10 - ETA: 0s - loss: 0.3461 - accuracy: 0.10 - ETA: 0s - loss: 0.3200 - accuracy: 0.09 - ETA: 0s - loss: 0.2998 - accuracy: 0.09 - ETA: 0s - loss: 0.2798 - accuracy: 0.09 - ETA: 0s - loss: 0.2641 - accuracy: 0.09 - ETA: 0s - loss: 0.2496 - accuracy: 0.10 - ETA: 0s - loss: 0.2364 - accuracy: 0.10 - ETA: 0s - loss: 0.2248 - accuracy: 0.10 - ETA: 0s - loss: 0.2140 - accuracy: 0.10 - ETA: 0s - loss: 0.2043 - accuracy: 0.09 - ETA: 0s - loss: 0.1959 - accuracy: 0.09 - ETA: 0s - loss: 0.1880 - accuracy: 0.10 - ETA: 0s - loss: 0.1805 - accuracy: 0.09 - ETA: 0s - loss: 0.1738 - accuracy: 0.09 - ETA: 0s - loss: 0.1673 - accuracy: 0.09 - ETA: 0s - loss: 0.1614 - accuracy: 0.09 - ETA: 0s - loss: 0.1562 - accuracy: 0.10 - ETA: 0s - loss: 0.1511 - accuracy: 0.09 - ETA: 0s - loss: 0.1465 - accuracy: 0.09 - 1s 624us/step - loss: 0.1458 - accuracy: 0.0997\n",
      "Epoch 2/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 0.0127 - accuracy: 0.06 - ETA: 1s - loss: 0.0253 - accuracy: 0.10 - ETA: 1s - loss: 0.0254 - accuracy: 0.10 - ETA: 1s - loss: 0.0248 - accuracy: 0.09 - ETA: 1s - loss: 0.0246 - accuracy: 0.09 - ETA: 1s - loss: 0.0240 - accuracy: 0.09 - ETA: 1s - loss: 0.0235 - accuracy: 0.09 - ETA: 1s - loss: 0.0232 - accuracy: 0.09 - ETA: 0s - loss: 0.0230 - accuracy: 0.09 - ETA: 0s - loss: 0.0228 - accuracy: 0.10 - ETA: 0s - loss: 0.0223 - accuracy: 0.09 - ETA: 0s - loss: 0.0220 - accuracy: 0.09 - ETA: 0s - loss: 0.0216 - accuracy: 0.09 - ETA: 0s - loss: 0.0214 - accuracy: 0.09 - ETA: 0s - loss: 0.0212 - accuracy: 0.10 - ETA: 0s - loss: 0.0208 - accuracy: 0.10 - ETA: 0s - loss: 0.0206 - accuracy: 0.10 - ETA: 0s - loss: 0.0203 - accuracy: 0.10 - ETA: 0s - loss: 0.0200 - accuracy: 0.09 - ETA: 0s - loss: 0.0198 - accuracy: 0.10 - ETA: 0s - loss: 0.0195 - accuracy: 0.10 - ETA: 0s - loss: 0.0193 - accuracy: 0.09 - ETA: 0s - loss: 0.0190 - accuracy: 0.10 - ETA: 0s - loss: 0.0188 - accuracy: 0.10 - ETA: 0s - loss: 0.0186 - accuracy: 0.10 - ETA: 0s - loss: 0.0184 - accuracy: 0.10 - ETA: 0s - loss: 0.0182 - accuracy: 0.10 - ETA: 0s - loss: 0.0181 - accuracy: 0.10 - 1s 619us/step - loss: 0.0181 - accuracy: 0.1001\n",
      "Epoch 3/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 0.0085 - accuracy: 0.03 - ETA: 1s - loss: 0.0129 - accuracy: 0.09 - ETA: 1s - loss: 0.0133 - accuracy: 0.09 - ETA: 1s - loss: 0.0132 - accuracy: 0.10 - ETA: 1s - loss: 0.0128 - accuracy: 0.10 - ETA: 1s - loss: 0.0126 - accuracy: 0.10 - ETA: 1s - loss: 0.0124 - accuracy: 0.10 - ETA: 0s - loss: 0.0123 - accuracy: 0.10 - ETA: 0s - loss: 0.0120 - accuracy: 0.10 - ETA: 0s - loss: 0.0119 - accuracy: 0.10 - ETA: 0s - loss: 0.0117 - accuracy: 0.10 - ETA: 0s - loss: 0.0116 - accuracy: 0.10 - ETA: 0s - loss: 0.0115 - accuracy: 0.10 - ETA: 0s - loss: 0.0113 - accuracy: 0.10 - ETA: 0s - loss: 0.0112 - accuracy: 0.10 - ETA: 0s - loss: 0.0112 - accuracy: 0.09 - ETA: 0s - loss: 0.0111 - accuracy: 0.09 - ETA: 0s - loss: 0.0110 - accuracy: 0.10 - ETA: 0s - loss: 0.0109 - accuracy: 0.10 - ETA: 0s - loss: 0.0108 - accuracy: 0.10 - ETA: 0s - loss: 0.0107 - accuracy: 0.10 - ETA: 0s - loss: 0.0107 - accuracy: 0.10 - ETA: 0s - loss: 0.0106 - accuracy: 0.10 - ETA: 0s - loss: 0.0105 - accuracy: 0.10 - ETA: 0s - loss: 0.0105 - accuracy: 0.10 - ETA: 0s - loss: 0.0104 - accuracy: 0.10 - ETA: 0s - loss: 0.0103 - accuracy: 0.10 - ETA: 0s - loss: 0.0103 - accuracy: 0.10 - 1s 620us/step - loss: 0.0103 - accuracy: 0.1001\n",
      "Epoch 4/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 0.0036 - accuracy: 0.09 - ETA: 1s - loss: 0.0079 - accuracy: 0.09 - ETA: 1s - loss: 0.0087 - accuracy: 0.10 - ETA: 1s - loss: 0.0085 - accuracy: 0.10 - ETA: 1s - loss: 0.0082 - accuracy: 0.10 - ETA: 1s - loss: 0.0082 - accuracy: 0.10 - ETA: 1s - loss: 0.0081 - accuracy: 0.09 - ETA: 1s - loss: 0.0079 - accuracy: 0.10 - ETA: 0s - loss: 0.0079 - accuracy: 0.10 - ETA: 0s - loss: 0.0079 - accuracy: 0.10 - ETA: 0s - loss: 0.0079 - accuracy: 0.10 - ETA: 0s - loss: 0.0080 - accuracy: 0.10 - ETA: 0s - loss: 0.0079 - accuracy: 0.10 - ETA: 0s - loss: 0.0078 - accuracy: 0.10 - ETA: 0s - loss: 0.0077 - accuracy: 0.10 - ETA: 0s - loss: 0.0076 - accuracy: 0.10 - ETA: 0s - loss: 0.0076 - accuracy: 0.10 - ETA: 0s - loss: 0.0075 - accuracy: 0.10 - ETA: 0s - loss: 0.0075 - accuracy: 0.10 - ETA: 0s - loss: 0.0074 - accuracy: 0.10 - ETA: 0s - loss: 0.0074 - accuracy: 0.10 - ETA: 0s - loss: 0.0074 - accuracy: 0.10 - ETA: 0s - loss: 0.0074 - accuracy: 0.10 - ETA: 0s - loss: 0.0073 - accuracy: 0.09 - ETA: 0s - loss: 0.0073 - accuracy: 0.09 - ETA: 0s - loss: 0.0073 - accuracy: 0.09 - ETA: 0s - loss: 0.0072 - accuracy: 0.10 - ETA: 0s - loss: 0.0072 - accuracy: 0.10 - 1s 622us/step - loss: 0.0072 - accuracy: 0.1001\n",
      "Epoch 5/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 0.0030 - accuracy: 0.06 - ETA: 1s - loss: 0.0052 - accuracy: 0.09 - ETA: 1s - loss: 0.0056 - accuracy: 0.10 - ETA: 1s - loss: 0.0058 - accuracy: 0.10 - ETA: 1s - loss: 0.0058 - accuracy: 0.09 - ETA: 1s - loss: 0.0059 - accuracy: 0.09 - ETA: 1s - loss: 0.0059 - accuracy: 0.09 - ETA: 1s - loss: 0.0061 - accuracy: 0.10 - ETA: 1s - loss: 0.0061 - accuracy: 0.10 - ETA: 0s - loss: 0.0061 - accuracy: 0.09 - ETA: 0s - loss: 0.0061 - accuracy: 0.09 - ETA: 0s - loss: 0.0060 - accuracy: 0.09 - ETA: 0s - loss: 0.0060 - accuracy: 0.09 - ETA: 0s - loss: 0.0059 - accuracy: 0.09 - ETA: 0s - loss: 0.0059 - accuracy: 0.09 - ETA: 0s - loss: 0.0059 - accuracy: 0.09 - ETA: 0s - loss: 0.0058 - accuracy: 0.09 - ETA: 0s - loss: 0.0058 - accuracy: 0.10 - ETA: 0s - loss: 0.0057 - accuracy: 0.10 - ETA: 0s - loss: 0.0057 - accuracy: 0.10 - ETA: 0s - loss: 0.0057 - accuracy: 0.10 - ETA: 0s - loss: 0.0057 - accuracy: 0.10 - ETA: 0s - loss: 0.0056 - accuracy: 0.10 - ETA: 0s - loss: 0.0056 - accuracy: 0.10 - ETA: 0s - loss: 0.0056 - accuracy: 0.10 - ETA: 0s - loss: 0.0056 - accuracy: 0.10 - ETA: 0s - loss: 0.0056 - accuracy: 0.10 - ETA: 0s - loss: 0.0055 - accuracy: 0.10 - 1s 625us/step - loss: 0.0055 - accuracy: 0.1001\n",
      "938/938 [==============================] - ETA: 0s - loss: 0.0020 - accuracy: 0.12 - ETA: 0s - loss: 0.0047 - accuracy: 0.09 - ETA: 0s - loss: 0.0048 - accuracy: 0.09 - ETA: 0s - loss: 0.0049 - accuracy: 0.09 - ETA: 0s - loss: 0.0048 - accuracy: 0.09 - ETA: 0s - loss: 0.0049 - accuracy: 0.09 - ETA: 0s - loss: 0.0049 - accuracy: 0.09 - ETA: 0s - loss: 0.0050 - accuracy: 0.10 - ETA: 0s - loss: 0.0050 - accuracy: 0.09 - ETA: 0s - loss: 0.0050 - accuracy: 0.09 - 0s 498us/step - loss: 0.0050 - accuracy: 0.0997\n",
      "--- Starting trial: run-1\n",
      "{'num_units': 16, 'learning_rate': 0.001, 'optimizer': 'adam'}\n",
      "Epoch 1/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 2.2816 - accuracy: 0.03 - ETA: 1s - loss: 0.2786 - accuracy: 0.10 - ETA: 1s - loss: 0.1430 - accuracy: 0.10 - ETA: 1s - loss: 0.0934 - accuracy: 0.10 - ETA: 1s - loss: 0.0707 - accuracy: 0.09 - ETA: 1s - loss: 0.0570 - accuracy: 0.09 - ETA: 1s - loss: 0.0474 - accuracy: 0.09 - ETA: 1s - loss: 0.0407 - accuracy: 0.09 - ETA: 1s - loss: 0.0356 - accuracy: 0.09 - ETA: 1s - loss: 0.0316 - accuracy: 0.09 - ETA: 1s - loss: 0.0285 - accuracy: 0.09 - ETA: 0s - loss: 0.0260 - accuracy: 0.09 - ETA: 0s - loss: 0.0238 - accuracy: 0.09 - ETA: 0s - loss: 0.0219 - accuracy: 0.09 - ETA: 0s - loss: 0.0203 - accuracy: 0.09 - ETA: 0s - loss: 0.0190 - accuracy: 0.09 - ETA: 0s - loss: 0.0178 - accuracy: 0.09 - ETA: 0s - loss: 0.0168 - accuracy: 0.09 - ETA: 0s - loss: 0.0159 - accuracy: 0.09 - ETA: 0s - loss: 0.0150 - accuracy: 0.09 - ETA: 0s - loss: 0.0143 - accuracy: 0.09 - ETA: 0s - loss: 0.0136 - accuracy: 0.09 - ETA: 0s - loss: 0.0130 - accuracy: 0.09 - ETA: 0s - loss: 0.0125 - accuracy: 0.09 - ETA: 0s - loss: 0.0120 - accuracy: 0.09 - ETA: 0s - loss: 0.0115 - accuracy: 0.09 - ETA: 0s - loss: 0.0110 - accuracy: 0.10 - ETA: 0s - loss: 0.0106 - accuracy: 0.10 - ETA: 0s - loss: 0.0103 - accuracy: 0.09 - ETA: 0s - loss: 0.0099 - accuracy: 0.10 - ETA: 0s - loss: 0.0096 - accuracy: 0.10 - 2s 697us/step - loss: 0.0095 - accuracy: 0.1000\n",
      "Epoch 2/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 5.4075e-05 - accuracy: 0.03 - ETA: 1s - loss: 4.1126e-05 - accuracy: 0.09 - ETA: 1s - loss: 6.5588e-05 - accuracy: 0.09 - ETA: 1s - loss: 6.8624e-05 - accuracy: 0.09 - ETA: 1s - loss: 5.7887e-05 - accuracy: 0.10 - ETA: 1s - loss: 5.0706e-05 - accuracy: 0.10 - ETA: 1s - loss: 4.6776e-05 - accuracy: 0.10 - ETA: 1s - loss: 4.4370e-05 - accuracy: 0.09 - ETA: 1s - loss: 5.9642e-05 - accuracy: 0.09 - ETA: 1s - loss: 5.7186e-05 - accuracy: 0.09 - ETA: 1s - loss: 5.5917e-05 - accuracy: 0.09 - ETA: 0s - loss: 5.3493e-05 - accuracy: 0.09 - ETA: 0s - loss: 5.0994e-05 - accuracy: 0.10 - ETA: 0s - loss: 4.8975e-05 - accuracy: 0.10 - ETA: 0s - loss: 4.9310e-05 - accuracy: 0.10 - ETA: 0s - loss: 4.7832e-05 - accuracy: 0.10 - ETA: 0s - loss: 4.5696e-05 - accuracy: 0.10 - ETA: 0s - loss: 4.4164e-05 - accuracy: 0.10 - ETA: 0s - loss: 4.2344e-05 - accuracy: 0.10 - ETA: 0s - loss: 4.0636e-05 - accuracy: 0.10 - ETA: 0s - loss: 3.9474e-05 - accuracy: 0.10 - ETA: 0s - loss: 6.6619e-05 - accuracy: 0.10 - ETA: 0s - loss: 6.4335e-05 - accuracy: 0.10 - ETA: 0s - loss: 6.2091e-05 - accuracy: 0.09 - ETA: 0s - loss: 7.7244e-05 - accuracy: 0.09 - ETA: 0s - loss: 7.4517e-05 - accuracy: 0.09 - ETA: 0s - loss: 8.9891e-05 - accuracy: 0.10 - ETA: 0s - loss: 8.6759e-05 - accuracy: 0.10 - ETA: 0s - loss: 8.3807e-05 - accuracy: 0.10 - ETA: 0s - loss: 8.1093e-05 - accuracy: 0.10 - ETA: 0s - loss: 8.4849e-05 - accuracy: 0.10 - 2s 698us/step - loss: 8.3664e-05 - accuracy: 0.1001\n",
      "Epoch 3/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 6.3330e-08 - accuracy: 0.06 - ETA: 1s - loss: 1.3259e-05 - accuracy: 0.09 - ETA: 1s - loss: 9.7021e-06 - accuracy: 0.09 - ETA: 1s - loss: 8.7480e-06 - accuracy: 0.10 - ETA: 1s - loss: 7.5369e-06 - accuracy: 0.10 - ETA: 1s - loss: 7.2914e-06 - accuracy: 0.09 - ETA: 1s - loss: 9.9746e-06 - accuracy: 0.10 - ETA: 1s - loss: 9.0391e-06 - accuracy: 0.09 - ETA: 1s - loss: 8.1955e-06 - accuracy: 0.09 - ETA: 1s - loss: 7.6966e-06 - accuracy: 0.10 - ETA: 1s - loss: 7.2583e-06 - accuracy: 0.10 - ETA: 0s - loss: 7.1043e-06 - accuracy: 0.09 - ETA: 0s - loss: 7.4752e-06 - accuracy: 0.09 - ETA: 0s - loss: 7.8467e-06 - accuracy: 0.09 - ETA: 0s - loss: 4.4732e-05 - accuracy: 0.09 - ETA: 0s - loss: 4.1966e-05 - accuracy: 0.09 - ETA: 0s - loss: 3.9433e-05 - accuracy: 0.09 - ETA: 0s - loss: 3.7268e-05 - accuracy: 0.09 - ETA: 0s - loss: 3.5353e-05 - accuracy: 0.09 - ETA: 0s - loss: 3.3506e-05 - accuracy: 0.10 - ETA: 0s - loss: 3.1954e-05 - accuracy: 0.10 - ETA: 0s - loss: 4.8511e-05 - accuracy: 0.10 - ETA: 0s - loss: 4.8015e-05 - accuracy: 0.10 - ETA: 0s - loss: 4.5919e-05 - accuracy: 0.10 - ETA: 0s - loss: 4.4007e-05 - accuracy: 0.10 - ETA: 0s - loss: 4.2343e-05 - accuracy: 0.10 - ETA: 0s - loss: 4.0697e-05 - accuracy: 0.10 - ETA: 0s - loss: 3.9221e-05 - accuracy: 0.10 - ETA: 0s - loss: 3.9637e-05 - accuracy: 0.10 - ETA: 0s - loss: 3.8343e-05 - accuracy: 0.10 - ETA: 0s - loss: 3.7043e-05 - accuracy: 0.10 - 2s 697us/step - loss: 3.6527e-05 - accuracy: 0.1001\n",
      "Epoch 4/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 7.4506e-09 - accuracy: 0.09 - ETA: 1s - loss: 1.5861e-06 - accuracy: 0.09 - ETA: 1s - loss: 1.2149e-04 - accuracy: 0.10 - ETA: 1s - loss: 8.7791e-05 - accuracy: 0.10 - ETA: 1s - loss: 6.6953e-05 - accuracy: 0.09 - ETA: 1s - loss: 5.3450e-05 - accuracy: 0.10 - ETA: 1s - loss: 4.4738e-05 - accuracy: 0.10 - ETA: 1s - loss: 4.8606e-05 - accuracy: 0.09 - ETA: 1s - loss: 4.2851e-05 - accuracy: 0.09 - ETA: 1s - loss: 3.8222e-05 - accuracy: 0.10 - ETA: 1s - loss: 3.4416e-05 - accuracy: 0.10 - ETA: 0s - loss: 3.1381e-05 - accuracy: 0.10 - ETA: 0s - loss: 2.8712e-05 - accuracy: 0.10 - ETA: 0s - loss: 2.6610e-05 - accuracy: 0.10 - ETA: 0s - loss: 2.4746e-05 - accuracy: 0.10 - ETA: 0s - loss: 2.3158e-05 - accuracy: 0.10 - ETA: 0s - loss: 2.2812e-05 - accuracy: 0.10 - ETA: 0s - loss: 2.1468e-05 - accuracy: 0.10 - ETA: 0s - loss: 2.0285e-05 - accuracy: 0.10 - ETA: 0s - loss: 1.9228e-05 - accuracy: 0.10 - ETA: 0s - loss: 1.8263e-05 - accuracy: 0.10 - ETA: 0s - loss: 1.7426e-05 - accuracy: 0.10 - ETA: 0s - loss: 1.6625e-05 - accuracy: 0.10 - ETA: 0s - loss: 1.5926e-05 - accuracy: 0.10 - ETA: 0s - loss: 1.5292e-05 - accuracy: 0.10 - ETA: 0s - loss: 1.4672e-05 - accuracy: 0.10 - ETA: 0s - loss: 1.4132e-05 - accuracy: 0.10 - ETA: 0s - loss: 1.3601e-05 - accuracy: 0.10 - ETA: 0s - loss: 1.3122e-05 - accuracy: 0.10 - ETA: 0s - loss: 1.2687e-05 - accuracy: 0.10 - ETA: 0s - loss: 1.6520e-05 - accuracy: 0.10 - 2s 696us/step - loss: 1.6284e-05 - accuracy: 0.1001\n",
      "Epoch 5/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 7.4506e-09 - accuracy: 0.12 - ETA: 1s - loss: 5.5764e-06 - accuracy: 0.10 - ETA: 1s - loss: 2.7640e-06 - accuracy: 0.10 - ETA: 1s - loss: 8.6622e-06 - accuracy: 0.10 - ETA: 1s - loss: 6.5747e-06 - accuracy: 0.09 - ETA: 1s - loss: 5.2820e-06 - accuracy: 0.09 - ETA: 1s - loss: 4.4386e-06 - accuracy: 0.09 - ETA: 1s - loss: 3.8575e-06 - accuracy: 0.09 - ETA: 1s - loss: 3.4205e-06 - accuracy: 0.09 - ETA: 1s - loss: 3.0569e-06 - accuracy: 0.09 - ETA: 1s - loss: 2.7626e-06 - accuracy: 0.09 - ETA: 0s - loss: 2.5202e-06 - accuracy: 0.09 - ETA: 0s - loss: 2.3095e-06 - accuracy: 0.09 - ETA: 0s - loss: 2.1391e-06 - accuracy: 0.09 - ETA: 0s - loss: 2.0112e-06 - accuracy: 0.09 - ETA: 0s - loss: 1.9046e-06 - accuracy: 0.09 - ETA: 0s - loss: 1.8144e-06 - accuracy: 0.09 - ETA: 0s - loss: 1.7280e-06 - accuracy: 0.09 - ETA: 0s - loss: 1.6386e-06 - accuracy: 0.09 - ETA: 0s - loss: 1.5579e-06 - accuracy: 0.09 - ETA: 0s - loss: 1.5662e-06 - accuracy: 0.09 - ETA: 0s - loss: 1.4930e-06 - accuracy: 0.09 - ETA: 0s - loss: 1.4273e-06 - accuracy: 0.09 - ETA: 0s - loss: 7.8542e-06 - accuracy: 0.09 - ETA: 0s - loss: 7.5251e-06 - accuracy: 0.09 - ETA: 0s - loss: 7.2190e-06 - accuracy: 0.09 - ETA: 0s - loss: 6.9449e-06 - accuracy: 0.10 - ETA: 0s - loss: 6.7092e-06 - accuracy: 0.09 - ETA: 0s - loss: 6.4849e-06 - accuracy: 0.09 - ETA: 0s - loss: 6.3840e-06 - accuracy: 0.09 - ETA: 0s - loss: 6.1842e-06 - accuracy: 0.09 - ETA: 0s - loss: 5.9902e-06 - accuracy: 0.09 - ETA: 0s - loss: 5.7951e-06 - accuracy: 0.10 - 2s 739us/step - loss: 5.7489e-06 - accuracy: 0.1001\n",
      "938/938 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 0.12 - ETA: 0s - loss: 8.4233e-07 - accuracy: 0.09 - ETA: 0s - loss: 4.3250e-07 - accuracy: 0.09 - ETA: 0s - loss: 2.9429e-07 - accuracy: 0.09 - ETA: 0s - loss: 2.2643e-07 - accuracy: 0.09 - ETA: 0s - loss: 1.9204e-07 - accuracy: 0.09 - ETA: 0s - loss: 2.4057e-07 - accuracy: 0.09 - ETA: 0s - loss: 2.0827e-07 - accuracy: 0.10 - ETA: 0s - loss: 1.4766e-05 - accuracy: 0.09 - ETA: 0s - loss: 1.9493e-05 - accuracy: 0.09 - ETA: 0s - loss: 1.7341e-05 - accuracy: 0.09 - 1s 544us/step - loss: 1.7073e-05 - accuracy: 0.0997\n",
      "--- Starting trial: run-2\n",
      "{'num_units': 16, 'learning_rate': 0.01, 'optimizer': 'Adagrad'}\n",
      "Epoch 1/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 2.5718 - accuracy: 0.0000e+ - ETA: 1s - loss: 0.3879 - accuracy: 0.0970   - ETA: 1s - loss: 0.2018 - accuracy: 0.09 - ETA: 1s - loss: 0.1410 - accuracy: 0.10 - ETA: 1s - loss: 0.1091 - accuracy: 0.10 - ETA: 1s - loss: 0.0876 - accuracy: 0.10 - ETA: 1s - loss: 0.0741 - accuracy: 0.10 - ETA: 1s - loss: 0.0646 - accuracy: 0.10 - ETA: 0s - loss: 0.0574 - accuracy: 0.10 - ETA: 0s - loss: 0.0514 - accuracy: 0.10 - ETA: 0s - loss: 0.0465 - accuracy: 0.10 - ETA: 0s - loss: 0.0425 - accuracy: 0.10 - ETA: 0s - loss: 0.0391 - accuracy: 0.09 - ETA: 0s - loss: 0.0363 - accuracy: 0.09 - ETA: 0s - loss: 0.0339 - accuracy: 0.09 - ETA: 0s - loss: 0.0316 - accuracy: 0.09 - ETA: 0s - loss: 0.0298 - accuracy: 0.10 - ETA: 0s - loss: 0.0282 - accuracy: 0.10 - ETA: 0s - loss: 0.0267 - accuracy: 0.09 - ETA: 0s - loss: 0.0254 - accuracy: 0.09 - ETA: 0s - loss: 0.0242 - accuracy: 0.09 - ETA: 0s - loss: 0.0231 - accuracy: 0.09 - ETA: 0s - loss: 0.0222 - accuracy: 0.09 - ETA: 0s - loss: 0.0213 - accuracy: 0.09 - ETA: 0s - loss: 0.0204 - accuracy: 0.09 - ETA: 0s - loss: 0.0196 - accuracy: 0.10 - ETA: 0s - loss: 0.0189 - accuracy: 0.09 - ETA: 0s - loss: 0.0183 - accuracy: 0.09 - 1s 628us/step - loss: 0.0181 - accuracy: 0.0999\n",
      "Epoch 2/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 3.3165e-04 - accuracy: 0.12 - ETA: 1s - loss: 8.7211e-04 - accuracy: 0.10 - ETA: 1s - loss: 0.0012 - accuracy: 0.1004   - ETA: 1s - loss: 0.0011 - accuracy: 0.10 - ETA: 1s - loss: 0.0011 - accuracy: 0.10 - ETA: 1s - loss: 0.0011 - accuracy: 0.10 - ETA: 1s - loss: 0.0010 - accuracy: 0.09 - ETA: 1s - loss: 0.0011 - accuracy: 0.09 - ETA: 0s - loss: 0.0010 - accuracy: 0.09 - ETA: 0s - loss: 0.0011 - accuracy: 0.09 - ETA: 0s - loss: 0.0011 - accuracy: 0.09 - ETA: 0s - loss: 0.0011 - accuracy: 0.09 - ETA: 0s - loss: 0.0011 - accuracy: 0.09 - ETA: 0s - loss: 0.0010 - accuracy: 0.10 - ETA: 0s - loss: 0.0010 - accuracy: 0.10 - ETA: 0s - loss: 9.9279e-04 - accuracy: 0.10 - ETA: 0s - loss: 9.6792e-04 - accuracy: 0.10 - ETA: 0s - loss: 9.5634e-04 - accuracy: 0.10 - ETA: 0s - loss: 9.7294e-04 - accuracy: 0.10 - ETA: 0s - loss: 9.6366e-04 - accuracy: 0.10 - ETA: 0s - loss: 9.4993e-04 - accuracy: 0.09 - ETA: 0s - loss: 9.3446e-04 - accuracy: 0.09 - ETA: 0s - loss: 9.4086e-04 - accuracy: 0.09 - ETA: 0s - loss: 9.3125e-04 - accuracy: 0.09 - ETA: 0s - loss: 9.2595e-04 - accuracy: 0.09 - ETA: 0s - loss: 9.1231e-04 - accuracy: 0.10 - ETA: 0s - loss: 8.9866e-04 - accuracy: 0.09 - ETA: 0s - loss: 8.8560e-04 - accuracy: 0.10 - 1s 639us/step - loss: 8.7263e-04 - accuracy: 0.1001\n",
      "Epoch 3/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 2.9474e-04 - accuracy: 0.12 - ETA: 1s - loss: 4.5581e-04 - accuracy: 0.10 - ETA: 1s - loss: 4.9584e-04 - accuracy: 0.10 - ETA: 1s - loss: 5.0987e-04 - accuracy: 0.10 - ETA: 1s - loss: 4.8368e-04 - accuracy: 0.10 - ETA: 1s - loss: 5.4357e-04 - accuracy: 0.10 - ETA: 1s - loss: 5.3241e-04 - accuracy: 0.10 - ETA: 1s - loss: 5.2725e-04 - accuracy: 0.09 - ETA: 0s - loss: 5.1529e-04 - accuracy: 0.10 - ETA: 0s - loss: 5.1307e-04 - accuracy: 0.10 - ETA: 0s - loss: 5.0815e-04 - accuracy: 0.09 - ETA: 0s - loss: 5.5673e-04 - accuracy: 0.09 - ETA: 0s - loss: 5.3667e-04 - accuracy: 0.09 - ETA: 0s - loss: 5.3212e-04 - accuracy: 0.09 - ETA: 0s - loss: 5.2794e-04 - accuracy: 0.09 - ETA: 0s - loss: 5.4179e-04 - accuracy: 0.09 - ETA: 0s - loss: 5.3345e-04 - accuracy: 0.09 - ETA: 0s - loss: 5.5499e-04 - accuracy: 0.09 - ETA: 0s - loss: 5.8473e-04 - accuracy: 0.09 - ETA: 0s - loss: 5.7359e-04 - accuracy: 0.09 - ETA: 0s - loss: 5.6390e-04 - accuracy: 0.09 - ETA: 0s - loss: 5.5672e-04 - accuracy: 0.09 - ETA: 0s - loss: 5.5427e-04 - accuracy: 0.09 - ETA: 0s - loss: 5.4538e-04 - accuracy: 0.09 - ETA: 0s - loss: 5.3532e-04 - accuracy: 0.09 - ETA: 0s - loss: 5.7087e-04 - accuracy: 0.09 - ETA: 0s - loss: 5.5971e-04 - accuracy: 0.09 - ETA: 0s - loss: 5.5005e-04 - accuracy: 0.10 - 1s 632us/step - loss: 5.4591e-04 - accuracy: 0.1001\n",
      "Epoch 4/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 0.0017 - accuracy: 0.12 - ETA: 1s - loss: 3.0085e-04 - accuracy: 0.10 - ETA: 1s - loss: 3.2659e-04 - accuracy: 0.09 - ETA: 1s - loss: 3.7791e-04 - accuracy: 0.09 - ETA: 1s - loss: 3.7737e-04 - accuracy: 0.09 - ETA: 1s - loss: 3.7819e-04 - accuracy: 0.09 - ETA: 1s - loss: 3.6102e-04 - accuracy: 0.09 - ETA: 1s - loss: 3.4737e-04 - accuracy: 0.10 - ETA: 0s - loss: 3.4197e-04 - accuracy: 0.09 - ETA: 0s - loss: 3.3553e-04 - accuracy: 0.10 - ETA: 0s - loss: 3.3017e-04 - accuracy: 0.10 - ETA: 0s - loss: 3.3781e-04 - accuracy: 0.10 - ETA: 0s - loss: 3.8452e-04 - accuracy: 0.10 - ETA: 0s - loss: 3.7990e-04 - accuracy: 0.10 - ETA: 0s - loss: 3.7778e-04 - accuracy: 0.10 - ETA: 0s - loss: 3.9082e-04 - accuracy: 0.10 - ETA: 0s - loss: 4.3838e-04 - accuracy: 0.10 - ETA: 0s - loss: 4.2873e-04 - accuracy: 0.10 - ETA: 0s - loss: 4.1722e-04 - accuracy: 0.10 - ETA: 0s - loss: 4.3667e-04 - accuracy: 0.10 - ETA: 0s - loss: 4.5542e-04 - accuracy: 0.10 - ETA: 0s - loss: 4.4732e-04 - accuracy: 0.10 - ETA: 0s - loss: 4.3913e-04 - accuracy: 0.10 - ETA: 0s - loss: 4.3005e-04 - accuracy: 0.10 - ETA: 0s - loss: 4.2885e-04 - accuracy: 0.10 - ETA: 0s - loss: 4.2372e-04 - accuracy: 0.10 - ETA: 0s - loss: 4.2281e-04 - accuracy: 0.10 - ETA: 0s - loss: 4.1534e-04 - accuracy: 0.10 - 1s 630us/step - loss: 4.1134e-04 - accuracy: 0.1001\n",
      "Epoch 5/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 2.3717e-04 - accuracy: 0.18 - ETA: 1s - loss: 2.1535e-04 - accuracy: 0.10 - ETA: 1s - loss: 3.3888e-04 - accuracy: 0.09 - ETA: 1s - loss: 3.2695e-04 - accuracy: 0.09 - ETA: 1s - loss: 3.1455e-04 - accuracy: 0.09 - ETA: 1s - loss: 3.1048e-04 - accuracy: 0.09 - ETA: 1s - loss: 3.0016e-04 - accuracy: 0.09 - ETA: 1s - loss: 2.9121e-04 - accuracy: 0.09 - ETA: 0s - loss: 2.7926e-04 - accuracy: 0.09 - ETA: 0s - loss: 3.4913e-04 - accuracy: 0.09 - ETA: 0s - loss: 3.4183e-04 - accuracy: 0.09 - ETA: 0s - loss: 3.3767e-04 - accuracy: 0.09 - ETA: 0s - loss: 3.2336e-04 - accuracy: 0.09 - ETA: 0s - loss: 3.1464e-04 - accuracy: 0.09 - ETA: 0s - loss: 3.4924e-04 - accuracy: 0.09 - ETA: 0s - loss: 3.5155e-04 - accuracy: 0.09 - ETA: 0s - loss: 3.4645e-04 - accuracy: 0.09 - ETA: 0s - loss: 4.0396e-04 - accuracy: 0.09 - ETA: 0s - loss: 3.9741e-04 - accuracy: 0.09 - ETA: 0s - loss: 3.8810e-04 - accuracy: 0.09 - ETA: 0s - loss: 3.7875e-04 - accuracy: 0.09 - ETA: 0s - loss: 3.7174e-04 - accuracy: 0.09 - ETA: 0s - loss: 3.6307e-04 - accuracy: 0.09 - ETA: 0s - loss: 3.5538e-04 - accuracy: 0.09 - ETA: 0s - loss: 3.5180e-04 - accuracy: 0.09 - ETA: 0s - loss: 3.4746e-04 - accuracy: 0.09 - ETA: 0s - loss: 3.4317e-04 - accuracy: 0.09 - ETA: 0s - loss: 3.3937e-04 - accuracy: 0.10 - 1s 624us/step - loss: 3.3679e-04 - accuracy: 0.1001\n",
      "938/938 [==============================] - ETA: 0s - loss: 3.5489e-05 - accuracy: 0.12 - ETA: 0s - loss: 3.8893e-04 - accuracy: 0.09 - ETA: 0s - loss: 3.1139e-04 - accuracy: 0.10 - ETA: 0s - loss: 2.8488e-04 - accuracy: 0.09 - ETA: 0s - loss: 2.6156e-04 - accuracy: 0.09 - ETA: 0s - loss: 2.7045e-04 - accuracy: 0.09 - ETA: 0s - loss: 2.6353e-04 - accuracy: 0.09 - ETA: 0s - loss: 3.2689e-04 - accuracy: 0.09 - ETA: 0s - loss: 3.5789e-04 - accuracy: 0.10 - ETA: 0s - loss: 3.3948e-04 - accuracy: 0.09 - 0s 506us/step - loss: 3.3332e-04 - accuracy: 0.0997\n",
      "--- Starting trial: run-3\n",
      "{'num_units': 16, 'learning_rate': 0.01, 'optimizer': 'adam'}\n",
      "Epoch 1/5\n",
      "2188/2188 [==============================] - ETA: 2s - loss: 2.4312 - accuracy: 0.0000e+ - ETA: 1s - loss: 0.0434 - accuracy: 0.0967   - ETA: 1s - loss: 0.0220 - accuracy: 0.09 - ETA: 1s - loss: 0.0149 - accuracy: 0.09 - ETA: 1s - loss: 0.0111 - accuracy: 0.09 - ETA: 1s - loss: 0.0088 - accuracy: 0.10 - ETA: 1s - loss: 0.0074 - accuracy: 0.09 - ETA: 1s - loss: 0.0064 - accuracy: 0.09 - ETA: 1s - loss: 0.0056 - accuracy: 0.09 - ETA: 1s - loss: 0.0050 - accuracy: 0.09 - ETA: 1s - loss: 0.0045 - accuracy: 0.09 - ETA: 0s - loss: 0.0041 - accuracy: 0.09 - ETA: 0s - loss: 0.0038 - accuracy: 0.09 - ETA: 0s - loss: 0.0035 - accuracy: 0.09 - ETA: 0s - loss: 0.0033 - accuracy: 0.09 - ETA: 0s - loss: 0.0030 - accuracy: 0.09 - ETA: 0s - loss: 0.0029 - accuracy: 0.09 - ETA: 0s - loss: 0.0027 - accuracy: 0.09 - ETA: 0s - loss: 0.0025 - accuracy: 0.09 - ETA: 0s - loss: 0.0024 - accuracy: 0.09 - ETA: 0s - loss: 0.0023 - accuracy: 0.09 - ETA: 0s - loss: 0.0022 - accuracy: 0.09 - ETA: 0s - loss: 0.0021 - accuracy: 0.09 - ETA: 0s - loss: 0.0020 - accuracy: 0.09 - ETA: 0s - loss: 0.0019 - accuracy: 0.09 - ETA: 0s - loss: 0.0018 - accuracy: 0.09 - ETA: 0s - loss: 0.0017 - accuracy: 0.10 - ETA: 0s - loss: 0.0017 - accuracy: 0.10 - ETA: 0s - loss: 0.0016 - accuracy: 0.10 - ETA: 0s - loss: 0.0016 - accuracy: 0.10 - ETA: 0s - loss: 0.0015 - accuracy: 0.10 - 2s 690us/step - loss: 0.0015 - accuracy: 0.1001\n",
      "Epoch 2/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 0.06 - ETA: 1s - loss: 1.5557e-04 - accuracy: 0.08 - ETA: 1s - loss: 7.8865e-05 - accuracy: 0.08 - ETA: 1s - loss: 5.2833e-05 - accuracy: 0.08 - ETA: 1s - loss: 3.9211e-05 - accuracy: 0.09 - ETA: 1s - loss: 3.1332e-05 - accuracy: 0.09 - ETA: 1s - loss: 2.6164e-05 - accuracy: 0.09 - ETA: 1s - loss: 2.2418e-05 - accuracy: 0.09 - ETA: 1s - loss: 1.9657e-05 - accuracy: 0.09 - ETA: 1s - loss: 2.1329e-05 - accuracy: 0.09 - ETA: 0s - loss: 1.9199e-05 - accuracy: 0.09 - ETA: 0s - loss: 1.8744e-05 - accuracy: 0.09 - ETA: 0s - loss: 1.7193e-05 - accuracy: 0.09 - ETA: 0s - loss: 1.5862e-05 - accuracy: 0.09 - ETA: 0s - loss: 1.4723e-05 - accuracy: 0.09 - ETA: 0s - loss: 1.3748e-05 - accuracy: 0.09 - ETA: 0s - loss: 1.3464e-05 - accuracy: 0.10 - ETA: 0s - loss: 1.2677e-05 - accuracy: 0.10 - ETA: 0s - loss: 1.2006e-05 - accuracy: 0.10 - ETA: 0s - loss: 1.1385e-05 - accuracy: 0.10 - ETA: 0s - loss: 1.0810e-05 - accuracy: 0.10 - ETA: 0s - loss: 1.0304e-05 - accuracy: 0.10 - ETA: 0s - loss: 9.8368e-06 - accuracy: 0.10 - ETA: 0s - loss: 1.1757e-05 - accuracy: 0.10 - ETA: 0s - loss: 1.1268e-05 - accuracy: 0.10 - ETA: 0s - loss: 1.0825e-05 - accuracy: 0.10 - ETA: 0s - loss: 1.0410e-05 - accuracy: 0.10 - ETA: 0s - loss: 1.0019e-05 - accuracy: 0.09 - ETA: 0s - loss: 9.6623e-06 - accuracy: 0.10 - ETA: 0s - loss: 9.3325e-06 - accuracy: 0.10 - 1s 685us/step - loss: 9.0446e-06 - accuracy: 0.1001\n",
      "Epoch 3/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 0.06 - ETA: 1s - loss: 2.9978e-06 - accuracy: 0.10 - ETA: 1s - loss: 1.5107e-06 - accuracy: 0.10 - ETA: 1s - loss: 1.0009e-06 - accuracy: 0.09 - ETA: 1s - loss: 7.5150e-07 - accuracy: 0.10 - ETA: 1s - loss: 5.9855e-07 - accuracy: 0.09 - ETA: 1s - loss: 5.0351e-07 - accuracy: 0.09 - ETA: 1s - loss: 4.3126e-07 - accuracy: 0.09 - ETA: 1s - loss: 3.8031e-07 - accuracy: 0.09 - ETA: 1s - loss: 3.4024e-07 - accuracy: 0.09 - ETA: 1s - loss: 3.0653e-07 - accuracy: 0.09 - ETA: 0s - loss: 2.7883e-07 - accuracy: 0.10 - ETA: 0s - loss: 2.5621e-07 - accuracy: 0.09 - ETA: 0s - loss: 2.3636e-07 - accuracy: 0.09 - ETA: 0s - loss: 2.3597e-07 - accuracy: 0.09 - ETA: 0s - loss: 2.2494e-07 - accuracy: 0.09 - ETA: 0s - loss: 2.1137e-07 - accuracy: 0.09 - ETA: 0s - loss: 1.9922e-07 - accuracy: 0.09 - ETA: 0s - loss: 1.8782e-07 - accuracy: 0.09 - ETA: 0s - loss: 1.7805e-07 - accuracy: 0.09 - ETA: 0s - loss: 7.4257e-07 - accuracy: 0.09 - ETA: 0s - loss: 8.5716e-07 - accuracy: 0.09 - ETA: 0s - loss: 8.1774e-07 - accuracy: 0.10 - ETA: 0s - loss: 7.9213e-07 - accuracy: 0.10 - ETA: 0s - loss: 7.6188e-07 - accuracy: 0.10 - ETA: 0s - loss: 7.3199e-07 - accuracy: 0.10 - ETA: 0s - loss: 7.0383e-07 - accuracy: 0.10 - ETA: 0s - loss: 6.7704e-07 - accuracy: 0.10 - ETA: 0s - loss: 6.5320e-07 - accuracy: 0.10 - ETA: 0s - loss: 6.4847e-07 - accuracy: 0.10 - ETA: 0s - loss: 6.2661e-07 - accuracy: 0.10 - 2s 694us/step - loss: 6.1932e-07 - accuracy: 0.1001\n",
      "Epoch 4/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 0.03 - ETA: 1s - loss: 2.4190e-10 - accuracy: 0.10 - ETA: 1s - loss: 4.9671e-10 - accuracy: 0.09 - ETA: 1s - loss: 4.0093e-10 - accuracy: 0.09 - ETA: 1s - loss: 4.6090e-07 - accuracy: 0.09 - ETA: 1s - loss: 4.1796e-07 - accuracy: 0.09 - ETA: 1s - loss: 3.4984e-07 - accuracy: 0.09 - ETA: 1s - loss: 2.9900e-07 - accuracy: 0.09 - ETA: 1s - loss: 2.6362e-07 - accuracy: 0.09 - ETA: 1s - loss: 2.3451e-07 - accuracy: 0.09 - ETA: 0s - loss: 2.1120e-07 - accuracy: 0.09 - ETA: 0s - loss: 1.9210e-07 - accuracy: 0.09 - ETA: 0s - loss: 1.7640e-07 - accuracy: 0.09 - ETA: 0s - loss: 1.6272e-07 - accuracy: 0.09 - ETA: 0s - loss: 2.6775e-07 - accuracy: 0.09 - ETA: 0s - loss: 2.4997e-07 - accuracy: 0.09 - ETA: 0s - loss: 2.3401e-07 - accuracy: 0.09 - ETA: 0s - loss: 2.2068e-07 - accuracy: 0.09 - ETA: 0s - loss: 2.0911e-07 - accuracy: 0.09 - ETA: 0s - loss: 1.9826e-07 - accuracy: 0.09 - ETA: 0s - loss: 1.8821e-07 - accuracy: 0.09 - ETA: 0s - loss: 1.7949e-07 - accuracy: 0.09 - ETA: 0s - loss: 1.7111e-07 - accuracy: 0.09 - ETA: 0s - loss: 1.6357e-07 - accuracy: 0.09 - ETA: 0s - loss: 1.5678e-07 - accuracy: 0.09 - ETA: 0s - loss: 1.5062e-07 - accuracy: 0.09 - ETA: 0s - loss: 1.4491e-07 - accuracy: 0.09 - ETA: 0s - loss: 1.4322e-07 - accuracy: 0.10 - ETA: 0s - loss: 1.3819e-07 - accuracy: 0.10 - ETA: 0s - loss: 1.3343e-07 - accuracy: 0.10 - 1s 685us/step - loss: 1.2932e-07 - accuracy: 0.1001\n",
      "Epoch 5/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 0.03 - ETA: 1s - loss: 7.6288e-07 - accuracy: 0.10 - ETA: 1s - loss: 3.8154e-07 - accuracy: 0.10 - ETA: 1s - loss: 2.7327e-07 - accuracy: 0.10 - ETA: 1s - loss: 2.0426e-07 - accuracy: 0.10 - ETA: 1s - loss: 2.8377e-07 - accuracy: 0.10 - ETA: 1s - loss: 2.3648e-07 - accuracy: 0.09 - ETA: 1s - loss: 2.0349e-07 - accuracy: 0.10 - ETA: 1s - loss: 1.7827e-07 - accuracy: 0.09 - ETA: 1s - loss: 1.6066e-07 - accuracy: 0.09 - ETA: 1s - loss: 1.4481e-07 - accuracy: 0.09 - ETA: 0s - loss: 1.3157e-07 - accuracy: 0.10 - ETA: 0s - loss: 1.2068e-07 - accuracy: 0.10 - ETA: 0s - loss: 1.1122e-07 - accuracy: 0.10 - ETA: 0s - loss: 1.0334e-07 - accuracy: 0.10 - ETA: 0s - loss: 9.6412e-08 - accuracy: 0.10 - ETA: 0s - loss: 9.0433e-08 - accuracy: 0.10 - ETA: 0s - loss: 8.5291e-08 - accuracy: 0.09 - ETA: 0s - loss: 8.0516e-08 - accuracy: 0.10 - ETA: 0s - loss: 7.6401e-08 - accuracy: 0.10 - ETA: 0s - loss: 7.2707e-08 - accuracy: 0.10 - ETA: 0s - loss: 6.9307e-08 - accuracy: 0.10 - ETA: 0s - loss: 6.6188e-08 - accuracy: 0.10 - ETA: 0s - loss: 6.3334e-08 - accuracy: 0.10 - ETA: 0s - loss: 6.0671e-08 - accuracy: 0.10 - ETA: 0s - loss: 5.9623e-08 - accuracy: 0.10 - ETA: 0s - loss: 5.7371e-08 - accuracy: 0.10 - ETA: 0s - loss: 5.5229e-08 - accuracy: 0.10 - ETA: 0s - loss: 5.3212e-08 - accuracy: 0.10 - ETA: 0s - loss: 5.1338e-08 - accuracy: 0.10 - ETA: 0s - loss: 4.9614e-08 - accuracy: 0.10 - 2s 692us/step - loss: 4.9308e-08 - accuracy: 0.1001\n",
      "938/938 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 0.12 - ETA: 0s - loss: 2.2130e-10 - accuracy: 0.09 - ETA: 0s - loss: 1.1120e-10 - accuracy: 0.10 - ETA: 0s - loss: 7.4012e-11 - accuracy: 0.09 - ETA: 0s - loss: 2.0286e-10 - accuracy: 0.09 - ETA: 0s - loss: 7.0453e-09 - accuracy: 0.09 - ETA: 0s - loss: 5.8827e-09 - accuracy: 0.09 - ETA: 0s - loss: 7.5922e-06 - accuracy: 0.09 - ETA: 0s - loss: 8.1924e-06 - accuracy: 0.09 - ETA: 0s - loss: 7.3024e-06 - accuracy: 0.09 - 0s 500us/step - loss: 7.0258e-06 - accuracy: 0.0997\n",
      "--- Starting trial: run-4\n",
      "{'num_units': 32, 'learning_rate': 0.001, 'optimizer': 'Adagrad'}\n",
      "Epoch 1/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 1.7713 - accuracy: 0.03 - ETA: 1s - loss: 0.8581 - accuracy: 0.09 - ETA: 1s - loss: 0.5743 - accuracy: 0.10 - ETA: 1s - loss: 0.4333 - accuracy: 0.10 - ETA: 1s - loss: 0.3504 - accuracy: 0.09 - ETA: 1s - loss: 0.2955 - accuracy: 0.09 - ETA: 1s - loss: 0.2571 - accuracy: 0.09 - ETA: 1s - loss: 0.2286 - accuracy: 0.09 - ETA: 1s - loss: 0.2049 - accuracy: 0.09 - ETA: 1s - loss: 0.1875 - accuracy: 0.09 - ETA: 0s - loss: 0.1722 - accuracy: 0.09 - ETA: 0s - loss: 0.1600 - accuracy: 0.09 - ETA: 0s - loss: 0.1489 - accuracy: 0.09 - ETA: 0s - loss: 0.1395 - accuracy: 0.09 - ETA: 0s - loss: 0.1314 - accuracy: 0.09 - ETA: 0s - loss: 0.1241 - accuracy: 0.09 - ETA: 0s - loss: 0.1178 - accuracy: 0.10 - ETA: 0s - loss: 0.1122 - accuracy: 0.10 - ETA: 0s - loss: 0.1070 - accuracy: 0.10 - ETA: 0s - loss: 0.1025 - accuracy: 0.09 - ETA: 0s - loss: 0.0981 - accuracy: 0.09 - ETA: 0s - loss: 0.0944 - accuracy: 0.09 - ETA: 0s - loss: 0.0909 - accuracy: 0.09 - ETA: 0s - loss: 0.0875 - accuracy: 0.09 - ETA: 0s - loss: 0.0845 - accuracy: 0.09 - ETA: 0s - loss: 0.0817 - accuracy: 0.09 - ETA: 0s - loss: 0.0789 - accuracy: 0.09 - ETA: 0s - loss: 0.0764 - accuracy: 0.09 - ETA: 0s - loss: 0.0740 - accuracy: 0.09 - ETA: 0s - loss: 0.0719 - accuracy: 0.10 - 1s 668us/step - loss: 0.0715 - accuracy: 0.0999\n",
      "Epoch 2/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 0.0176 - accuracy: 0.15 - ETA: 1s - loss: 0.0122 - accuracy: 0.09 - ETA: 1s - loss: 0.0110 - accuracy: 0.10 - ETA: 1s - loss: 0.0110 - accuracy: 0.09 - ETA: 1s - loss: 0.0104 - accuracy: 0.09 - ETA: 1s - loss: 0.0103 - accuracy: 0.10 - ETA: 1s - loss: 0.0103 - accuracy: 0.09 - ETA: 1s - loss: 0.0101 - accuracy: 0.10 - ETA: 1s - loss: 0.0100 - accuracy: 0.10 - ETA: 0s - loss: 0.0098 - accuracy: 0.10 - ETA: 0s - loss: 0.0099 - accuracy: 0.10 - ETA: 0s - loss: 0.0097 - accuracy: 0.10 - ETA: 0s - loss: 0.0095 - accuracy: 0.10 - ETA: 0s - loss: 0.0094 - accuracy: 0.10 - ETA: 0s - loss: 0.0093 - accuracy: 0.10 - ETA: 0s - loss: 0.0092 - accuracy: 0.10 - ETA: 0s - loss: 0.0090 - accuracy: 0.10 - ETA: 0s - loss: 0.0088 - accuracy: 0.10 - ETA: 0s - loss: 0.0087 - accuracy: 0.10 - ETA: 0s - loss: 0.0086 - accuracy: 0.10 - ETA: 0s - loss: 0.0085 - accuracy: 0.10 - ETA: 0s - loss: 0.0084 - accuracy: 0.10 - ETA: 0s - loss: 0.0083 - accuracy: 0.10 - ETA: 0s - loss: 0.0083 - accuracy: 0.10 - ETA: 0s - loss: 0.0082 - accuracy: 0.10 - ETA: 0s - loss: 0.0081 - accuracy: 0.10 - ETA: 0s - loss: 0.0080 - accuracy: 0.10 - ETA: 0s - loss: 0.0079 - accuracy: 0.10 - ETA: 0s - loss: 0.0078 - accuracy: 0.10 - ETA: 0s - loss: 0.0077 - accuracy: 0.10 - 1s 669us/step - loss: 0.0077 - accuracy: 0.1001\n",
      "Epoch 3/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 0.0073 - accuracy: 0.06 - ETA: 1s - loss: 0.0057 - accuracy: 0.10 - ETA: 1s - loss: 0.0060 - accuracy: 0.10 - ETA: 1s - loss: 0.0061 - accuracy: 0.10 - ETA: 1s - loss: 0.0057 - accuracy: 0.10 - ETA: 1s - loss: 0.0056 - accuracy: 0.10 - ETA: 1s - loss: 0.0055 - accuracy: 0.10 - ETA: 1s - loss: 0.0054 - accuracy: 0.10 - ETA: 1s - loss: 0.0054 - accuracy: 0.10 - ETA: 0s - loss: 0.0054 - accuracy: 0.10 - ETA: 0s - loss: 0.0053 - accuracy: 0.10 - ETA: 0s - loss: 0.0051 - accuracy: 0.10 - ETA: 0s - loss: 0.0051 - accuracy: 0.10 - ETA: 0s - loss: 0.0050 - accuracy: 0.10 - ETA: 0s - loss: 0.0050 - accuracy: 0.10 - ETA: 0s - loss: 0.0049 - accuracy: 0.10 - ETA: 0s - loss: 0.0049 - accuracy: 0.10 - ETA: 0s - loss: 0.0049 - accuracy: 0.10 - ETA: 0s - loss: 0.0048 - accuracy: 0.09 - ETA: 0s - loss: 0.0048 - accuracy: 0.10 - ETA: 0s - loss: 0.0047 - accuracy: 0.10 - ETA: 0s - loss: 0.0047 - accuracy: 0.10 - ETA: 0s - loss: 0.0047 - accuracy: 0.10 - ETA: 0s - loss: 0.0046 - accuracy: 0.10 - ETA: 0s - loss: 0.0046 - accuracy: 0.10 - ETA: 0s - loss: 0.0045 - accuracy: 0.10 - ETA: 0s - loss: 0.0045 - accuracy: 0.09 - ETA: 0s - loss: 0.0045 - accuracy: 0.09 - ETA: 0s - loss: 0.0045 - accuracy: 0.10 - 1s 662us/step - loss: 0.0045 - accuracy: 0.1001\n",
      "Epoch 4/5\n",
      "2188/2188 [==============================] - ETA: 2s - loss: 9.8486e-04 - accuracy: 0.18 - ETA: 1s - loss: 0.0036 - accuracy: 0.1096   - ETA: 1s - loss: 0.0035 - accuracy: 0.10 - ETA: 1s - loss: 0.0036 - accuracy: 0.10 - ETA: 1s - loss: 0.0035 - accuracy: 0.10 - ETA: 1s - loss: 0.0035 - accuracy: 0.10 - ETA: 1s - loss: 0.0034 - accuracy: 0.10 - ETA: 1s - loss: 0.0034 - accuracy: 0.10 - ETA: 1s - loss: 0.0035 - accuracy: 0.10 - ETA: 0s - loss: 0.0036 - accuracy: 0.10 - ETA: 0s - loss: 0.0036 - accuracy: 0.10 - ETA: 0s - loss: 0.0035 - accuracy: 0.09 - ETA: 0s - loss: 0.0035 - accuracy: 0.09 - ETA: 0s - loss: 0.0034 - accuracy: 0.09 - ETA: 0s - loss: 0.0034 - accuracy: 0.09 - ETA: 0s - loss: 0.0034 - accuracy: 0.09 - ETA: 0s - loss: 0.0034 - accuracy: 0.10 - ETA: 0s - loss: 0.0034 - accuracy: 0.10 - ETA: 0s - loss: 0.0034 - accuracy: 0.10 - ETA: 0s - loss: 0.0033 - accuracy: 0.10 - ETA: 0s - loss: 0.0033 - accuracy: 0.10 - ETA: 0s - loss: 0.0033 - accuracy: 0.10 - ETA: 0s - loss: 0.0033 - accuracy: 0.09 - ETA: 0s - loss: 0.0033 - accuracy: 0.09 - ETA: 0s - loss: 0.0033 - accuracy: 0.09 - ETA: 0s - loss: 0.0032 - accuracy: 0.10 - ETA: 0s - loss: 0.0032 - accuracy: 0.10 - ETA: 0s - loss: 0.0032 - accuracy: 0.10 - ETA: 0s - loss: 0.0032 - accuracy: 0.09 - 1s 661us/step - loss: 0.0032 - accuracy: 0.1001\n",
      "Epoch 5/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 0.0052 - accuracy: 0.12 - ETA: 1s - loss: 0.0032 - accuracy: 0.09 - ETA: 1s - loss: 0.0029 - accuracy: 0.09 - ETA: 1s - loss: 0.0029 - accuracy: 0.09 - ETA: 1s - loss: 0.0029 - accuracy: 0.10 - ETA: 1s - loss: 0.0027 - accuracy: 0.09 - ETA: 1s - loss: 0.0029 - accuracy: 0.09 - ETA: 1s - loss: 0.0028 - accuracy: 0.10 - ETA: 1s - loss: 0.0028 - accuracy: 0.10 - ETA: 0s - loss: 0.0027 - accuracy: 0.10 - ETA: 0s - loss: 0.0027 - accuracy: 0.09 - ETA: 0s - loss: 0.0027 - accuracy: 0.10 - ETA: 0s - loss: 0.0027 - accuracy: 0.10 - ETA: 0s - loss: 0.0026 - accuracy: 0.10 - ETA: 0s - loss: 0.0026 - accuracy: 0.10 - ETA: 0s - loss: 0.0026 - accuracy: 0.10 - ETA: 0s - loss: 0.0026 - accuracy: 0.10 - ETA: 0s - loss: 0.0026 - accuracy: 0.10 - ETA: 0s - loss: 0.0026 - accuracy: 0.10 - ETA: 0s - loss: 0.0026 - accuracy: 0.10 - ETA: 0s - loss: 0.0026 - accuracy: 0.10 - ETA: 0s - loss: 0.0026 - accuracy: 0.10 - ETA: 0s - loss: 0.0025 - accuracy: 0.10 - ETA: 0s - loss: 0.0025 - accuracy: 0.10 - ETA: 0s - loss: 0.0025 - accuracy: 0.10 - ETA: 0s - loss: 0.0025 - accuracy: 0.10 - ETA: 0s - loss: 0.0025 - accuracy: 0.10 - ETA: 0s - loss: 0.0025 - accuracy: 0.10 - ETA: 0s - loss: 0.0025 - accuracy: 0.10 - 1s 661us/step - loss: 0.0025 - accuracy: 0.1001\n",
      "  1/938 [..............................] - ETA: 0s - loss: 6.9771e-04 - accuracy: 0.1250WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_end` time: 0.0010s). Check your callbacks.\n",
      "938/938 [==============================] - ETA: 0s - loss: 0.0021 - accuracy: 0.0982   - ETA: 0s - loss: 0.0022 - accuracy: 0.09 - ETA: 0s - loss: 0.0023 - accuracy: 0.09 - ETA: 0s - loss: 0.0022 - accuracy: 0.09 - ETA: 0s - loss: 0.0022 - accuracy: 0.09 - ETA: 0s - loss: 0.0022 - accuracy: 0.09 - ETA: 0s - loss: 0.0023 - accuracy: 0.09 - ETA: 0s - loss: 0.0023 - accuracy: 0.10 - ETA: 0s - loss: 0.0023 - accuracy: 0.09 - 0s 508us/step - loss: 0.0023 - accuracy: 0.0997\n",
      "--- Starting trial: run-5\n",
      "{'num_units': 32, 'learning_rate': 0.001, 'optimizer': 'adam'}\n",
      "Epoch 1/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 2.2607 - accuracy: 0.03 - ETA: 1s - loss: 0.2032 - accuracy: 0.10 - ETA: 1s - loss: 0.1030 - accuracy: 0.10 - ETA: 1s - loss: 0.0699 - accuracy: 0.09 - ETA: 1s - loss: 0.0529 - accuracy: 0.10 - ETA: 1s - loss: 0.0425 - accuracy: 0.10 - ETA: 1s - loss: 0.0358 - accuracy: 0.10 - ETA: 1s - loss: 0.0308 - accuracy: 0.10 - ETA: 1s - loss: 0.0270 - accuracy: 0.09 - ETA: 1s - loss: 0.0240 - accuracy: 0.09 - ETA: 1s - loss: 0.0217 - accuracy: 0.09 - ETA: 1s - loss: 0.0198 - accuracy: 0.09 - ETA: 1s - loss: 0.0182 - accuracy: 0.09 - ETA: 0s - loss: 0.0168 - accuracy: 0.09 - ETA: 0s - loss: 0.0156 - accuracy: 0.09 - ETA: 0s - loss: 0.0145 - accuracy: 0.09 - ETA: 0s - loss: 0.0137 - accuracy: 0.10 - ETA: 0s - loss: 0.0129 - accuracy: 0.10 - ETA: 0s - loss: 0.0122 - accuracy: 0.10 - ETA: 0s - loss: 0.0116 - accuracy: 0.10 - ETA: 0s - loss: 0.0110 - accuracy: 0.10 - ETA: 0s - loss: 0.0105 - accuracy: 0.10 - ETA: 0s - loss: 0.0101 - accuracy: 0.10 - ETA: 0s - loss: 0.0096 - accuracy: 0.10 - ETA: 0s - loss: 0.0093 - accuracy: 0.10 - ETA: 0s - loss: 0.0089 - accuracy: 0.10 - ETA: 0s - loss: 0.0086 - accuracy: 0.10 - ETA: 0s - loss: 0.0082 - accuracy: 0.10 - ETA: 0s - loss: 0.0079 - accuracy: 0.10 - ETA: 0s - loss: 0.0077 - accuracy: 0.10 - ETA: 0s - loss: 0.0074 - accuracy: 0.10 - ETA: 0s - loss: 0.0072 - accuracy: 0.10 - ETA: 0s - loss: 0.0070 - accuracy: 0.10 - 2s 742us/step - loss: 0.0069 - accuracy: 0.1000\n",
      "Epoch 2/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 2.2791e-05 - accuracy: 0.03 - ETA: 1s - loss: 1.8044e-05 - accuracy: 0.09 - ETA: 1s - loss: 2.9300e-05 - accuracy: 0.09 - ETA: 1s - loss: 2.6457e-05 - accuracy: 0.10 - ETA: 1s - loss: 2.3627e-05 - accuracy: 0.09 - ETA: 1s - loss: 2.1980e-05 - accuracy: 0.09 - ETA: 1s - loss: 1.1042e-04 - accuracy: 0.10 - ETA: 1s - loss: 9.6264e-05 - accuracy: 0.09 - ETA: 1s - loss: 8.5444e-05 - accuracy: 0.09 - ETA: 1s - loss: 1.0352e-04 - accuracy: 0.09 - ETA: 1s - loss: 9.3775e-05 - accuracy: 0.10 - ETA: 1s - loss: 9.4680e-05 - accuracy: 0.10 - ETA: 0s - loss: 1.3489e-04 - accuracy: 0.10 - ETA: 0s - loss: 1.2536e-04 - accuracy: 0.10 - ETA: 0s - loss: 1.1680e-04 - accuracy: 0.09 - ETA: 0s - loss: 1.0965e-04 - accuracy: 0.09 - ETA: 0s - loss: 1.0319e-04 - accuracy: 0.09 - ETA: 0s - loss: 9.7627e-05 - accuracy: 0.09 - ETA: 0s - loss: 9.2435e-05 - accuracy: 0.09 - ETA: 0s - loss: 8.7854e-05 - accuracy: 0.09 - ETA: 0s - loss: 8.3585e-05 - accuracy: 0.10 - ETA: 0s - loss: 7.9729e-05 - accuracy: 0.10 - ETA: 0s - loss: 7.6515e-05 - accuracy: 0.10 - ETA: 0s - loss: 7.3459e-05 - accuracy: 0.10 - ETA: 0s - loss: 7.1483e-05 - accuracy: 0.10 - ETA: 0s - loss: 9.0573e-05 - accuracy: 0.09 - ETA: 0s - loss: 8.7613e-05 - accuracy: 0.09 - ETA: 0s - loss: 8.4403e-05 - accuracy: 0.09 - ETA: 0s - loss: 8.1488e-05 - accuracy: 0.09 - ETA: 0s - loss: 7.8790e-05 - accuracy: 0.09 - ETA: 0s - loss: 7.6288e-05 - accuracy: 0.10 - ETA: 0s - loss: 7.3929e-05 - accuracy: 0.10 - 2s 725us/step - loss: 7.2326e-05 - accuracy: 0.1001\n",
      "Epoch 3/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 3.7253e-09 - accuracy: 0.03 - ETA: 1s - loss: 1.7183e-06 - accuracy: 0.10 - ETA: 1s - loss: 1.9264e-06 - accuracy: 0.09 - ETA: 1s - loss: 1.6724e-06 - accuracy: 0.09 - ETA: 1s - loss: 5.0059e-06 - accuracy: 0.09 - ETA: 1s - loss: 4.6708e-06 - accuracy: 0.09 - ETA: 1s - loss: 4.2121e-06 - accuracy: 0.09 - ETA: 1s - loss: 3.8447e-06 - accuracy: 0.09 - ETA: 1s - loss: 3.7857e-06 - accuracy: 0.09 - ETA: 1s - loss: 3.6524e-06 - accuracy: 0.09 - ETA: 1s - loss: 3.5759e-06 - accuracy: 0.09 - ETA: 1s - loss: 3.3829e-06 - accuracy: 0.09 - ETA: 0s - loss: 3.2241e-06 - accuracy: 0.09 - ETA: 0s - loss: 3.3584e-06 - accuracy: 0.09 - ETA: 0s - loss: 3.2104e-06 - accuracy: 0.09 - ETA: 0s - loss: 8.4577e-06 - accuracy: 0.09 - ETA: 0s - loss: 8.0320e-06 - accuracy: 0.09 - ETA: 0s - loss: 7.5973e-06 - accuracy: 0.09 - ETA: 0s - loss: 7.1972e-06 - accuracy: 0.09 - ETA: 0s - loss: 2.8640e-05 - accuracy: 0.10 - ETA: 0s - loss: 2.7235e-05 - accuracy: 0.10 - ETA: 0s - loss: 2.5984e-05 - accuracy: 0.10 - ETA: 0s - loss: 2.4806e-05 - accuracy: 0.10 - ETA: 0s - loss: 4.0074e-05 - accuracy: 0.10 - ETA: 0s - loss: 3.8442e-05 - accuracy: 0.10 - ETA: 0s - loss: 3.6938e-05 - accuracy: 0.10 - ETA: 0s - loss: 3.5513e-05 - accuracy: 0.10 - ETA: 0s - loss: 3.9444e-05 - accuracy: 0.10 - ETA: 0s - loss: 3.8057e-05 - accuracy: 0.10 - ETA: 0s - loss: 3.6710e-05 - accuracy: 0.10 - ETA: 0s - loss: 3.5454e-05 - accuracy: 0.10 - ETA: 0s - loss: 3.4282e-05 - accuracy: 0.10 - ETA: 0s - loss: 3.3205e-05 - accuracy: 0.10 - 2s 736us/step - loss: 3.3091e-05 - accuracy: 0.1001\n",
      "Epoch 4/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 0.09 - ETA: 1s - loss: 2.4534e-08 - accuracy: 0.09 - ETA: 1s - loss: 6.7403e-08 - accuracy: 0.09 - ETA: 1s - loss: 3.1861e-06 - accuracy: 0.10 - ETA: 1s - loss: 5.1859e-05 - accuracy: 0.09 - ETA: 1s - loss: 4.1485e-05 - accuracy: 0.09 - ETA: 1s - loss: 3.4579e-05 - accuracy: 0.09 - ETA: 1s - loss: 2.9711e-05 - accuracy: 0.09 - ETA: 1s - loss: 2.5990e-05 - accuracy: 0.09 - ETA: 1s - loss: 2.3067e-05 - accuracy: 0.09 - ETA: 1s - loss: 2.0793e-05 - accuracy: 0.09 - ETA: 1s - loss: 1.8905e-05 - accuracy: 0.09 - ETA: 0s - loss: 1.7331e-05 - accuracy: 0.09 - ETA: 0s - loss: 1.6005e-05 - accuracy: 0.09 - ETA: 0s - loss: 1.4877e-05 - accuracy: 0.09 - ETA: 0s - loss: 1.3876e-05 - accuracy: 0.09 - ETA: 0s - loss: 1.3002e-05 - accuracy: 0.09 - ETA: 0s - loss: 1.2229e-05 - accuracy: 0.09 - ETA: 0s - loss: 1.1555e-05 - accuracy: 0.09 - ETA: 0s - loss: 1.0950e-05 - accuracy: 0.09 - ETA: 0s - loss: 1.0998e-05 - accuracy: 0.09 - ETA: 0s - loss: 1.0480e-05 - accuracy: 0.09 - ETA: 0s - loss: 1.0013e-05 - accuracy: 0.09 - ETA: 0s - loss: 1.2431e-05 - accuracy: 0.10 - ETA: 0s - loss: 1.6651e-05 - accuracy: 0.10 - ETA: 0s - loss: 1.5987e-05 - accuracy: 0.10 - ETA: 0s - loss: 1.5381e-05 - accuracy: 0.10 - ETA: 0s - loss: 1.4820e-05 - accuracy: 0.10 - ETA: 0s - loss: 1.4290e-05 - accuracy: 0.10 - ETA: 0s - loss: 1.3790e-05 - accuracy: 0.10 - ETA: 0s - loss: 1.3353e-05 - accuracy: 0.10 - ETA: 0s - loss: 1.2922e-05 - accuracy: 0.10 - 2s 725us/step - loss: 1.2630e-05 - accuracy: 0.1001\n",
      "Epoch 5/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 0.18 - ETA: 1s - loss: 2.7370e-08 - accuracy: 0.10 - ETA: 1s - loss: 2.6949e-08 - accuracy: 0.09 - ETA: 1s - loss: 2.2993e-08 - accuracy: 0.09 - ETA: 1s - loss: 2.2579e-08 - accuracy: 0.09 - ETA: 1s - loss: 3.5833e-08 - accuracy: 0.09 - ETA: 1s - loss: 3.2836e-08 - accuracy: 0.09 - ETA: 1s - loss: 3.2643e-08 - accuracy: 0.10 - ETA: 1s - loss: 3.5558e-08 - accuracy: 0.09 - ETA: 1s - loss: 9.9675e-06 - accuracy: 0.10 - ETA: 1s - loss: 9.0310e-06 - accuracy: 0.09 - ETA: 1s - loss: 8.2213e-06 - accuracy: 0.09 - ETA: 1s - loss: 7.5346e-06 - accuracy: 0.10 - ETA: 1s - loss: 7.1208e-06 - accuracy: 0.10 - ETA: 0s - loss: 6.5983e-06 - accuracy: 0.10 - ETA: 0s - loss: 6.1410e-06 - accuracy: 0.10 - ETA: 0s - loss: 5.7746e-06 - accuracy: 0.10 - ETA: 0s - loss: 5.4372e-06 - accuracy: 0.10 - ETA: 0s - loss: 5.8615e-06 - accuracy: 0.10 - ETA: 0s - loss: 5.5331e-06 - accuracy: 0.10 - ETA: 0s - loss: 5.2564e-06 - accuracy: 0.10 - ETA: 0s - loss: 5.0163e-06 - accuracy: 0.10 - ETA: 0s - loss: 6.7884e-06 - accuracy: 0.10 - ETA: 0s - loss: 6.4912e-06 - accuracy: 0.10 - ETA: 0s - loss: 6.2151e-06 - accuracy: 0.10 - ETA: 0s - loss: 5.9506e-06 - accuracy: 0.10 - ETA: 0s - loss: 5.7160e-06 - accuracy: 0.10 - ETA: 0s - loss: 5.4978e-06 - accuracy: 0.10 - ETA: 0s - loss: 5.2898e-06 - accuracy: 0.10 - ETA: 0s - loss: 5.1051e-06 - accuracy: 0.09 - ETA: 0s - loss: 4.9278e-06 - accuracy: 0.09 - ETA: 0s - loss: 4.7670e-06 - accuracy: 0.09 - ETA: 0s - loss: 4.6378e-06 - accuracy: 0.09 - 2s 753us/step - loss: 4.4970e-06 - accuracy: 0.1001\n",
      "938/938 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 0.12 - ETA: 0s - loss: 2.6107e-07 - accuracy: 0.09 - ETA: 0s - loss: 1.2725e-07 - accuracy: 0.09 - ETA: 0s - loss: 8.4747e-08 - accuracy: 0.09 - ETA: 0s - loss: 6.3619e-08 - accuracy: 0.09 - ETA: 0s - loss: 8.6633e-08 - accuracy: 0.09 - ETA: 0s - loss: 7.4507e-08 - accuracy: 0.09 - ETA: 0s - loss: 1.6665e-05 - accuracy: 0.09 - ETA: 0s - loss: 2.0811e-05 - accuracy: 0.10 - ETA: 0s - loss: 1.8485e-05 - accuracy: 0.09 - 0s 510us/step - loss: 1.7470e-05 - accuracy: 0.0997\n",
      "--- Starting trial: run-6\n",
      "{'num_units': 32, 'learning_rate': 0.01, 'optimizer': 'Adagrad'}\n",
      "Epoch 1/5\n",
      "   1/2188 [..............................] - ETA: 0s - loss: 2.3086 - accuracy: 0.0000e+00WARNING:tensorflow:Callbacks method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_begin` time: 0.0010s). Check your callbacks.\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0010s). Check your callbacks.\n",
      "2188/2188 [==============================] - ETA: 1s - loss: 0.2398 - accuracy: 0.0985   - ETA: 1s - loss: 0.1271 - accuracy: 0.09 - ETA: 1s - loss: 0.0868 - accuracy: 0.09 - ETA: 1s - loss: 0.0662 - accuracy: 0.10 - ETA: 1s - loss: 0.0538 - accuracy: 0.10 - ETA: 1s - loss: 0.0453 - accuracy: 0.10 - ETA: 1s - loss: 0.0395 - accuracy: 0.10 - ETA: 1s - loss: 0.0350 - accuracy: 0.10 - ETA: 0s - loss: 0.0314 - accuracy: 0.10 - ETA: 0s - loss: 0.0286 - accuracy: 0.10 - ETA: 0s - loss: 0.0261 - accuracy: 0.10 - ETA: 0s - loss: 0.0241 - accuracy: 0.10 - ETA: 0s - loss: 0.0224 - accuracy: 0.10 - ETA: 0s - loss: 0.0209 - accuracy: 0.10 - ETA: 0s - loss: 0.0196 - accuracy: 0.09 - ETA: 0s - loss: 0.0184 - accuracy: 0.10 - ETA: 0s - loss: 0.0174 - accuracy: 0.09 - ETA: 0s - loss: 0.0166 - accuracy: 0.09 - ETA: 0s - loss: 0.0158 - accuracy: 0.09 - ETA: 0s - loss: 0.0151 - accuracy: 0.09 - ETA: 0s - loss: 0.0144 - accuracy: 0.09 - ETA: 0s - loss: 0.0138 - accuracy: 0.09 - ETA: 0s - loss: 0.0132 - accuracy: 0.10 - ETA: 0s - loss: 0.0128 - accuracy: 0.10 - ETA: 0s - loss: 0.0123 - accuracy: 0.09 - ETA: 0s - loss: 0.0119 - accuracy: 0.10 - ETA: 0s - loss: 0.0115 - accuracy: 0.09 - ETA: 0s - loss: 0.0111 - accuracy: 0.09 - 1s 657us/step - loss: 0.0108 - accuracy: 0.1000\n",
      "Epoch 2/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 4.3413e-04 - accuracy: 0.09 - ETA: 1s - loss: 0.0015 - accuracy: 0.0921   - ETA: 1s - loss: 0.0011 - accuracy: 0.09 - ETA: 1s - loss: 0.0012 - accuracy: 0.09 - ETA: 1s - loss: 0.0011 - accuracy: 0.09 - ETA: 1s - loss: 9.7078e-04 - accuracy: 0.09 - ETA: 1s - loss: 9.1334e-04 - accuracy: 0.10 - ETA: 1s - loss: 8.4661e-04 - accuracy: 0.10 - ETA: 1s - loss: 8.0401e-04 - accuracy: 0.09 - ETA: 1s - loss: 7.8852e-04 - accuracy: 0.09 - ETA: 0s - loss: 7.7128e-04 - accuracy: 0.09 - ETA: 0s - loss: 8.1006e-04 - accuracy: 0.09 - ETA: 0s - loss: 7.8442e-04 - accuracy: 0.09 - ETA: 0s - loss: 7.8668e-04 - accuracy: 0.10 - ETA: 0s - loss: 7.7213e-04 - accuracy: 0.10 - ETA: 0s - loss: 7.5337e-04 - accuracy: 0.10 - ETA: 0s - loss: 7.3345e-04 - accuracy: 0.10 - ETA: 0s - loss: 7.1920e-04 - accuracy: 0.10 - ETA: 0s - loss: 7.0645e-04 - accuracy: 0.09 - ETA: 0s - loss: 7.0202e-04 - accuracy: 0.09 - ETA: 0s - loss: 6.8468e-04 - accuracy: 0.10 - ETA: 0s - loss: 6.6890e-04 - accuracy: 0.09 - ETA: 0s - loss: 6.5758e-04 - accuracy: 0.09 - ETA: 0s - loss: 6.4930e-04 - accuracy: 0.09 - ETA: 0s - loss: 6.4622e-04 - accuracy: 0.10 - ETA: 0s - loss: 6.3674e-04 - accuracy: 0.09 - ETA: 0s - loss: 6.2509e-04 - accuracy: 0.09 - ETA: 0s - loss: 6.1920e-04 - accuracy: 0.09 - ETA: 0s - loss: 6.2618e-04 - accuracy: 0.10 - ETA: 0s - loss: 6.3704e-04 - accuracy: 0.10 - 1s 668us/step - loss: 6.3640e-04 - accuracy: 0.1001\n",
      "Epoch 3/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 1.7097e-04 - accuracy: 0.21 - ETA: 1s - loss: 4.2761e-04 - accuracy: 0.10 - ETA: 1s - loss: 3.7459e-04 - accuracy: 0.10 - ETA: 1s - loss: 5.0560e-04 - accuracy: 0.10 - ETA: 1s - loss: 4.4756e-04 - accuracy: 0.10 - ETA: 1s - loss: 4.1787e-04 - accuracy: 0.10 - ETA: 1s - loss: 4.7447e-04 - accuracy: 0.10 - ETA: 1s - loss: 4.6529e-04 - accuracy: 0.10 - ETA: 1s - loss: 5.3564e-04 - accuracy: 0.10 - ETA: 1s - loss: 5.2429e-04 - accuracy: 0.10 - ETA: 0s - loss: 4.9704e-04 - accuracy: 0.10 - ETA: 0s - loss: 5.4707e-04 - accuracy: 0.10 - ETA: 0s - loss: 5.4853e-04 - accuracy: 0.09 - ETA: 0s - loss: 5.3098e-04 - accuracy: 0.09 - ETA: 0s - loss: 5.1253e-04 - accuracy: 0.10 - ETA: 0s - loss: 4.9353e-04 - accuracy: 0.10 - ETA: 0s - loss: 4.7852e-04 - accuracy: 0.10 - ETA: 0s - loss: 4.6264e-04 - accuracy: 0.09 - ETA: 0s - loss: 4.5017e-04 - accuracy: 0.09 - ETA: 0s - loss: 4.4433e-04 - accuracy: 0.09 - ETA: 0s - loss: 4.3088e-04 - accuracy: 0.09 - ETA: 0s - loss: 4.3081e-04 - accuracy: 0.09 - ETA: 0s - loss: 4.2023e-04 - accuracy: 0.09 - ETA: 0s - loss: 4.1238e-04 - accuracy: 0.09 - ETA: 0s - loss: 4.0655e-04 - accuracy: 0.09 - ETA: 0s - loss: 4.0281e-04 - accuracy: 0.09 - ETA: 0s - loss: 3.9851e-04 - accuracy: 0.09 - ETA: 0s - loss: 3.9528e-04 - accuracy: 0.10 - ETA: 0s - loss: 3.9294e-04 - accuracy: 0.10 - ETA: 0s - loss: 4.0600e-04 - accuracy: 0.10 - 1s 667us/step - loss: 4.0500e-04 - accuracy: 0.1001\n",
      "Epoch 4/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 8.1624e-05 - accuracy: 0.09 - ETA: 1s - loss: 8.9624e-04 - accuracy: 0.08 - ETA: 1s - loss: 5.4120e-04 - accuracy: 0.09 - ETA: 1s - loss: 4.6057e-04 - accuracy: 0.09 - ETA: 1s - loss: 3.8770e-04 - accuracy: 0.09 - ETA: 1s - loss: 3.6699e-04 - accuracy: 0.09 - ETA: 1s - loss: 3.6457e-04 - accuracy: 0.09 - ETA: 1s - loss: 3.5530e-04 - accuracy: 0.09 - ETA: 1s - loss: 3.3736e-04 - accuracy: 0.09 - ETA: 1s - loss: 3.2089e-04 - accuracy: 0.10 - ETA: 1s - loss: 3.1472e-04 - accuracy: 0.09 - ETA: 1s - loss: 3.0257e-04 - accuracy: 0.09 - ETA: 0s - loss: 3.2906e-04 - accuracy: 0.09 - ETA: 0s - loss: 3.1538e-04 - accuracy: 0.10 - ETA: 0s - loss: 3.0660e-04 - accuracy: 0.10 - ETA: 0s - loss: 2.9555e-04 - accuracy: 0.10 - ETA: 0s - loss: 2.9541e-04 - accuracy: 0.10 - ETA: 0s - loss: 2.9277e-04 - accuracy: 0.10 - ETA: 0s - loss: 2.8573e-04 - accuracy: 0.10 - ETA: 0s - loss: 2.7938e-04 - accuracy: 0.10 - ETA: 0s - loss: 2.7586e-04 - accuracy: 0.10 - ETA: 0s - loss: 2.7398e-04 - accuracy: 0.10 - ETA: 0s - loss: 3.0651e-04 - accuracy: 0.10 - ETA: 0s - loss: 3.0403e-04 - accuracy: 0.10 - ETA: 0s - loss: 2.9872e-04 - accuracy: 0.10 - ETA: 0s - loss: 2.9340e-04 - accuracy: 0.10 - ETA: 0s - loss: 2.9479e-04 - accuracy: 0.10 - ETA: 0s - loss: 2.9088e-04 - accuracy: 0.10 - ETA: 0s - loss: 3.0763e-04 - accuracy: 0.10 - ETA: 0s - loss: 3.0804e-04 - accuracy: 0.10 - ETA: 0s - loss: 3.0846e-04 - accuracy: 0.10 - 2s 687us/step - loss: 3.0846e-04 - accuracy: 0.1001\n",
      "Epoch 5/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 2.4552e-04 - accuracy: 0.03 - ETA: 1s - loss: 6.8783e-04 - accuracy: 0.09 - ETA: 1s - loss: 4.3496e-04 - accuracy: 0.10 - ETA: 1s - loss: 3.4881e-04 - accuracy: 0.10 - ETA: 1s - loss: 2.9095e-04 - accuracy: 0.10 - ETA: 1s - loss: 2.8784e-04 - accuracy: 0.10 - ETA: 1s - loss: 2.8236e-04 - accuracy: 0.10 - ETA: 1s - loss: 2.8109e-04 - accuracy: 0.10 - ETA: 1s - loss: 2.6957e-04 - accuracy: 0.10 - ETA: 0s - loss: 2.6492e-04 - accuracy: 0.10 - ETA: 0s - loss: 2.5859e-04 - accuracy: 0.10 - ETA: 0s - loss: 3.2834e-04 - accuracy: 0.09 - ETA: 0s - loss: 3.1415e-04 - accuracy: 0.09 - ETA: 0s - loss: 3.0664e-04 - accuracy: 0.09 - ETA: 0s - loss: 2.9730e-04 - accuracy: 0.10 - ETA: 0s - loss: 2.8603e-04 - accuracy: 0.10 - ETA: 0s - loss: 2.8178e-04 - accuracy: 0.10 - ETA: 0s - loss: 2.7371e-04 - accuracy: 0.09 - ETA: 0s - loss: 2.6544e-04 - accuracy: 0.09 - ETA: 0s - loss: 2.5565e-04 - accuracy: 0.10 - ETA: 0s - loss: 2.4957e-04 - accuracy: 0.10 - ETA: 0s - loss: 2.4569e-04 - accuracy: 0.10 - ETA: 0s - loss: 2.4054e-04 - accuracy: 0.10 - ETA: 0s - loss: 2.3767e-04 - accuracy: 0.10 - ETA: 0s - loss: 2.3447e-04 - accuracy: 0.10 - ETA: 0s - loss: 2.2966e-04 - accuracy: 0.10 - ETA: 0s - loss: 2.3507e-04 - accuracy: 0.09 - ETA: 0s - loss: 2.6202e-04 - accuracy: 0.09 - ETA: 0s - loss: 2.5905e-04 - accuracy: 0.10 - ETA: 0s - loss: 2.5443e-04 - accuracy: 0.10 - 1s 665us/step - loss: 2.5442e-04 - accuracy: 0.1001\n",
      "938/938 [==============================] - ETA: 0s - loss: 1.8726e-05 - accuracy: 0.12 - ETA: 0s - loss: 3.8459e-04 - accuracy: 0.09 - ETA: 0s - loss: 2.6426e-04 - accuracy: 0.10 - ETA: 0s - loss: 2.3248e-04 - accuracy: 0.09 - ETA: 0s - loss: 2.0476e-04 - accuracy: 0.09 - ETA: 0s - loss: 2.0756e-04 - accuracy: 0.09 - ETA: 0s - loss: 2.0506e-04 - accuracy: 0.09 - ETA: 0s - loss: 2.6608e-04 - accuracy: 0.09 - ETA: 0s - loss: 3.0290e-04 - accuracy: 0.10 - ETA: 0s - loss: 2.8636e-04 - accuracy: 0.09 - 0s 514us/step - loss: 2.7524e-04 - accuracy: 0.0997\n",
      "--- Starting trial: run-7\n",
      "{'num_units': 32, 'learning_rate': 0.01, 'optimizer': 'adam'}\n",
      "Epoch 1/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 2.3108 - accuracy: 0.0000e+ - ETA: 1s - loss: 0.0379 - accuracy: 0.0947   - ETA: 1s - loss: 0.0204 - accuracy: 0.09 - ETA: 1s - loss: 0.0155 - accuracy: 0.09 - ETA: 1s - loss: 0.0115 - accuracy: 0.09 - ETA: 1s - loss: 0.0090 - accuracy: 0.09 - ETA: 1s - loss: 0.0074 - accuracy: 0.09 - ETA: 1s - loss: 0.0063 - accuracy: 0.09 - ETA: 1s - loss: 0.0054 - accuracy: 0.09 - ETA: 1s - loss: 0.0049 - accuracy: 0.09 - ETA: 1s - loss: 0.0044 - accuracy: 0.09 - ETA: 1s - loss: 0.0039 - accuracy: 0.09 - ETA: 1s - loss: 0.0035 - accuracy: 0.10 - ETA: 1s - loss: 0.0033 - accuracy: 0.10 - ETA: 1s - loss: 0.0030 - accuracy: 0.10 - ETA: 1s - loss: 0.0028 - accuracy: 0.10 - ETA: 1s - loss: 0.0027 - accuracy: 0.10 - ETA: 0s - loss: 0.0025 - accuracy: 0.10 - ETA: 0s - loss: 0.0024 - accuracy: 0.10 - ETA: 0s - loss: 0.0023 - accuracy: 0.10 - ETA: 0s - loss: 0.0022 - accuracy: 0.10 - ETA: 0s - loss: 0.0021 - accuracy: 0.10 - ETA: 0s - loss: 0.0020 - accuracy: 0.09 - ETA: 0s - loss: 0.0019 - accuracy: 0.09 - ETA: 0s - loss: 0.0018 - accuracy: 0.09 - ETA: 0s - loss: 0.0017 - accuracy: 0.09 - ETA: 0s - loss: 0.0016 - accuracy: 0.09 - ETA: 0s - loss: 0.0016 - accuracy: 0.09 - ETA: 0s - loss: 0.0015 - accuracy: 0.09 - ETA: 0s - loss: 0.0015 - accuracy: 0.09 - ETA: 0s - loss: 0.0014 - accuracy: 0.09 - ETA: 0s - loss: 0.0014 - accuracy: 0.09 - ETA: 0s - loss: 0.0013 - accuracy: 0.09 - ETA: 0s - loss: 0.0013 - accuracy: 0.09 - ETA: 0s - loss: 0.0013 - accuracy: 0.09 - ETA: 0s - loss: 0.0012 - accuracy: 0.09 - 2s 818us/step - loss: 0.0012 - accuracy: 0.1001\n",
      "Epoch 2/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 0.09 - ETA: 1s - loss: 0.0000e+00 - accuracy: 0.08 - ETA: 1s - loss: 0.0000e+00 - accuracy: 0.09 - ETA: 1s - loss: 6.3835e-09 - accuracy: 0.09 - ETA: 1s - loss: 4.8346e-09 - accuracy: 0.09 - ETA: 1s - loss: 4.6262e-07 - accuracy: 0.09 - ETA: 1s - loss: 4.2822e-07 - accuracy: 0.09 - ETA: 1s - loss: 4.3135e-07 - accuracy: 0.10 - ETA: 1s - loss: 3.7743e-07 - accuracy: 0.10 - ETA: 1s - loss: 3.3495e-07 - accuracy: 0.10 - ETA: 1s - loss: 3.0150e-07 - accuracy: 0.10 - ETA: 1s - loss: 2.7304e-07 - accuracy: 0.10 - ETA: 1s - loss: 2.5040e-07 - accuracy: 0.10 - ETA: 0s - loss: 2.3122e-07 - accuracy: 0.10 - ETA: 0s - loss: 2.1477e-07 - accuracy: 0.10 - ETA: 0s - loss: 1.9993e-07 - accuracy: 0.10 - ETA: 0s - loss: 1.8785e-07 - accuracy: 0.10 - ETA: 0s - loss: 1.7685e-07 - accuracy: 0.10 - ETA: 0s - loss: 1.6720e-07 - accuracy: 0.10 - ETA: 0s - loss: 1.5856e-07 - accuracy: 0.10 - ETA: 0s - loss: 1.5065e-07 - accuracy: 0.10 - ETA: 0s - loss: 1.4341e-07 - accuracy: 0.09 - ETA: 0s - loss: 1.3691e-07 - accuracy: 0.09 - ETA: 0s - loss: 1.3098e-07 - accuracy: 0.10 - ETA: 0s - loss: 1.3243e-07 - accuracy: 0.10 - ETA: 0s - loss: 1.2722e-07 - accuracy: 0.10 - ETA: 0s - loss: 1.2249e-07 - accuracy: 0.10 - ETA: 0s - loss: 1.1841e-07 - accuracy: 0.10 - ETA: 0s - loss: 1.1448e-07 - accuracy: 0.10 - ETA: 0s - loss: 1.1057e-07 - accuracy: 0.10 - ETA: 0s - loss: 1.0692e-07 - accuracy: 0.10 - ETA: 0s - loss: 1.0351e-07 - accuracy: 0.10 - ETA: 0s - loss: 1.0584e-07 - accuracy: 0.10 - 2s 740us/step - loss: 1.0480e-07 - accuracy: 0.1001\n",
      "Epoch 3/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 0.21 - ETA: 1s - loss: 0.0000e+00 - accuracy: 0.10 - ETA: 1s - loss: 2.7192e-11 - accuracy: 0.09 - ETA: 1s - loss: 5.3347e-09 - accuracy: 0.10 - ETA: 1s - loss: 3.9817e-09 - accuracy: 0.09 - ETA: 1s - loss: 3.1761e-09 - accuracy: 0.09 - ETA: 1s - loss: 2.6481e-09 - accuracy: 0.09 - ETA: 1s - loss: 2.2735e-09 - accuracy: 0.09 - ETA: 1s - loss: 2.7246e-08 - accuracy: 0.09 - ETA: 1s - loss: 2.4156e-08 - accuracy: 0.09 - ETA: 1s - loss: 1.9465e-07 - accuracy: 0.09 - ETA: 1s - loss: 1.7728e-07 - accuracy: 0.09 - ETA: 0s - loss: 1.6394e-07 - accuracy: 0.09 - ETA: 0s - loss: 1.5178e-07 - accuracy: 0.09 - ETA: 0s - loss: 1.4130e-07 - accuracy: 0.09 - ETA: 0s - loss: 1.3141e-07 - accuracy: 0.09 - ETA: 0s - loss: 1.3293e-07 - accuracy: 0.09 - ETA: 0s - loss: 1.2508e-07 - accuracy: 0.09 - ETA: 0s - loss: 1.1820e-07 - accuracy: 0.09 - ETA: 0s - loss: 1.1195e-07 - accuracy: 0.09 - ETA: 0s - loss: 1.0617e-07 - accuracy: 0.09 - ETA: 0s - loss: 1.0674e-07 - accuracy: 0.09 - ETA: 0s - loss: 1.0181e-07 - accuracy: 0.09 - ETA: 0s - loss: 9.7376e-08 - accuracy: 0.09 - ETA: 0s - loss: 9.6838e-08 - accuracy: 0.09 - ETA: 0s - loss: 9.3059e-08 - accuracy: 0.09 - ETA: 0s - loss: 8.9766e-08 - accuracy: 0.09 - ETA: 0s - loss: 8.6556e-08 - accuracy: 0.09 - ETA: 0s - loss: 8.3520e-08 - accuracy: 0.10 - ETA: 0s - loss: 8.0695e-08 - accuracy: 0.10 - ETA: 0s - loss: 7.8017e-08 - accuracy: 0.10 - ETA: 0s - loss: 7.5476e-08 - accuracy: 0.10 - ETA: 0s - loss: 7.3128e-08 - accuracy: 0.10 - 2s 733us/step - loss: 7.3078e-08 - accuracy: 0.1001\n",
      "Epoch 4/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 0.18 - ETA: 1s - loss: 0.0000e+00 - accuracy: 0.09 - ETA: 1s - loss: 0.0000e+00 - accuracy: 0.10 - ETA: 1s - loss: 0.0000e+00 - accuracy: 0.10 - ETA: 1s - loss: 0.0000e+00 - accuracy: 0.09 - ETA: 1s - loss: 1.9675e-07 - accuracy: 0.10 - ETA: 1s - loss: 1.6316e-07 - accuracy: 0.10 - ETA: 1s - loss: 1.3907e-07 - accuracy: 0.09 - ETA: 1s - loss: 1.2185e-07 - accuracy: 0.10 - ETA: 1s - loss: 1.1198e-07 - accuracy: 0.09 - ETA: 1s - loss: 1.0090e-07 - accuracy: 0.09 - ETA: 1s - loss: 9.1930e-08 - accuracy: 0.09 - ETA: 0s - loss: 8.3917e-08 - accuracy: 0.09 - ETA: 0s - loss: 7.7275e-08 - accuracy: 0.09 - ETA: 0s - loss: 7.1755e-08 - accuracy: 0.09 - ETA: 0s - loss: 6.6907e-08 - accuracy: 0.09 - ETA: 0s - loss: 6.7739e-08 - accuracy: 0.09 - ETA: 0s - loss: 6.3754e-08 - accuracy: 0.09 - ETA: 0s - loss: 6.0212e-08 - accuracy: 0.09 - ETA: 0s - loss: 5.8779e-08 - accuracy: 0.10 - ETA: 0s - loss: 5.5800e-08 - accuracy: 0.10 - ETA: 0s - loss: 5.3181e-08 - accuracy: 0.10 - ETA: 0s - loss: 5.0731e-08 - accuracy: 0.10 - ETA: 0s - loss: 4.8557e-08 - accuracy: 0.10 - ETA: 0s - loss: 4.6534e-08 - accuracy: 0.10 - ETA: 0s - loss: 4.4621e-08 - accuracy: 0.10 - ETA: 0s - loss: 4.2930e-08 - accuracy: 0.10 - ETA: 0s - loss: 4.1297e-08 - accuracy: 0.10 - ETA: 0s - loss: 3.9785e-08 - accuracy: 0.10 - ETA: 0s - loss: 3.8417e-08 - accuracy: 0.09 - ETA: 0s - loss: 3.9475e-08 - accuracy: 0.09 - ETA: 0s - loss: 3.8200e-08 - accuracy: 0.10 - 2s 723us/step - loss: 3.7475e-08 - accuracy: 0.1001\n",
      "Epoch 5/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 0.09 - ETA: 1s - loss: 0.0000e+00 - accuracy: 0.09 - ETA: 1s - loss: 0.0000e+00 - accuracy: 0.10 - ETA: 1s - loss: 0.0000e+00 - accuracy: 0.09 - ETA: 1s - loss: 0.0000e+00 - accuracy: 0.09 - ETA: 1s - loss: 0.0000e+00 - accuracy: 0.09 - ETA: 1s - loss: 0.0000e+00 - accuracy: 0.09 - ETA: 1s - loss: 0.0000e+00 - accuracy: 0.09 - ETA: 1s - loss: 5.3077e-08 - accuracy: 0.09 - ETA: 1s - loss: 4.7221e-08 - accuracy: 0.09 - ETA: 1s - loss: 4.2614e-08 - accuracy: 0.09 - ETA: 1s - loss: 3.8425e-08 - accuracy: 0.09 - ETA: 1s - loss: 3.4986e-08 - accuracy: 0.09 - ETA: 1s - loss: 3.2151e-08 - accuracy: 0.09 - ETA: 0s - loss: 2.9707e-08 - accuracy: 0.09 - ETA: 0s - loss: 2.7666e-08 - accuracy: 0.09 - ETA: 0s - loss: 2.6506e-08 - accuracy: 0.09 - ETA: 0s - loss: 2.4904e-08 - accuracy: 0.09 - ETA: 0s - loss: 2.3446e-08 - accuracy: 0.09 - ETA: 0s - loss: 2.2184e-08 - accuracy: 0.09 - ETA: 0s - loss: 2.1019e-08 - accuracy: 0.09 - ETA: 0s - loss: 1.9999e-08 - accuracy: 0.09 - ETA: 0s - loss: 1.9087e-08 - accuracy: 0.09 - ETA: 0s - loss: 1.8464e-08 - accuracy: 0.09 - ETA: 0s - loss: 1.7683e-08 - accuracy: 0.09 - ETA: 0s - loss: 1.8049e-08 - accuracy: 0.09 - ETA: 0s - loss: 1.7429e-08 - accuracy: 0.09 - ETA: 0s - loss: 1.6767e-08 - accuracy: 0.09 - ETA: 0s - loss: 1.6127e-08 - accuracy: 0.09 - ETA: 0s - loss: 1.6785e-08 - accuracy: 0.09 - ETA: 0s - loss: 1.6221e-08 - accuracy: 0.09 - ETA: 0s - loss: 1.5671e-08 - accuracy: 0.09 - ETA: 0s - loss: 1.5157e-08 - accuracy: 0.09 - 2s 750us/step - loss: 1.4794e-08 - accuracy: 0.1001\n",
      "938/938 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 0.12 - ETA: 0s - loss: 0.0000e+00 - accuracy: 0.09 - ETA: 0s - loss: 0.0000e+00 - accuracy: 0.10 - ETA: 0s - loss: 0.0000e+00 - accuracy: 0.09 - ETA: 0s - loss: 0.0000e+00 - accuracy: 0.09 - ETA: 0s - loss: 5.9666e-10 - accuracy: 0.09 - ETA: 0s - loss: 5.0012e-10 - accuracy: 0.09 - ETA: 0s - loss: 1.1705e-06 - accuracy: 0.09 - ETA: 0s - loss: 1.2758e-06 - accuracy: 0.10 - ETA: 0s - loss: 1.1426e-06 - accuracy: 0.09 - 0s 524us/step - loss: 1.0506e-06 - accuracy: 0.0997\n"
     ]
    }
   ],
   "source": [
    "session_num = 0\n",
    "\n",
    "# Basically a grid search\n",
    "for num_units in HP_NUM_UNITS.domain.values:\n",
    "  for learning_rate in (HP_LEARNING_RATE.domain.min_value,\n",
    "                        HP_LEARNING_RATE.domain.max_value):\n",
    "    for optimizer in HP_OPTIMIZER.domain.values:\n",
    "      hparams = {\n",
    "          HP_NUM_UNITS: num_units,\n",
    "          HP_LEARNING_RATE: learning_rate,\n",
    "          HP_OPTIMIZER: optimizer\n",
    "      }\n",
    "\n",
    "      run_name = f'run-{session_num}'\n",
    "      print(f'--- Starting trial: {run_name}')\n",
    "      print({param.name: hparams[param] for param in hparams})\n",
    "      run('logs/hparams_tuning/' + run_name, hparams)\n",
    "      session_num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Visualize the Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 13036), started 0:21:51 ago. (Use '!kill 13036' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-86bf5fb91538151e\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-86bf5fb91538151e\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/hparams_tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Hyperparameters with RandomSearchCV (Learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from kerastuner.tuners import RandomSearch\n",
    "\n",
    "\"\"\"\n",
    "This model Tunes:\n",
    "- Number of Neurons in the Hidden Layer\n",
    "- Learning Rate in Adam\n",
    "\n",
    "\"\"\"\n",
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Dense(units=hp.Int('units',\n",
    "                                        min_value=32,\n",
    "                                        max_value=512,\n",
    "                                        step=32),\n",
    "                           activation='relu'))\n",
    "    model.add(Dense(32, input_shape=(16,), kernel_initializer = 'he_uniform', kernel_regularizer = None, kernel_constraint = 'MaxNorm', activation = 'relu')) \n",
    "    model.add(Dense(50, activation = 'relu')) \n",
    "    model.add(Dense(8))\n",
    "    model.add(Dense(50))\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adagrad(\n",
    "            hp.Choice('learning_rate',\n",
    "                      values=[1e-2, 1e-3, 1e-4])),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=5,\n",
    "    executions_per_trial=3,\n",
    "    directory='keras-tuner-trial4',\n",
    "    project_name='helloworld')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Search space summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Default search space size: 2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">units (Int)</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-default: None</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-max_value: 512</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-min_value: 32</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-sampling: None</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-step: 32</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">learning_rate (Choice)</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-default: 0.01</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-ordered: True</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-values: [0.01, 0.001, 0.0001]</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 2.3287 - accuracy: 0.0000e+ - ETA: 4s - loss: 2.2166 - accuracy: 0.0247   - ETA: 4s - loss: 2.0815 - accuracy: 0.05 - ETA: 4s - loss: 1.8900 - accuracy: 0.07 - ETA: 4s - loss: 1.6451 - accuracy: 0.08 - ETA: 4s - loss: 1.4238 - accuracy: 0.08 - ETA: 4s - loss: 1.2396 - accuracy: 0.08 - ETA: 4s - loss: 1.0744 - accuracy: 0.08 - ETA: 4s - loss: 0.9543 - accuracy: 0.08 - ETA: 4s - loss: 0.8654 - accuracy: 0.08 - ETA: 4s - loss: 0.7883 - accuracy: 0.08 - ETA: 4s - loss: 0.7290 - accuracy: 0.08 - ETA: 4s - loss: 0.6712 - accuracy: 0.09 - ETA: 4s - loss: 0.6199 - accuracy: 0.09 - ETA: 4s - loss: 0.5776 - accuracy: 0.09 - ETA: 4s - loss: 0.5390 - accuracy: 0.09 - ETA: 4s - loss: 0.5067 - accuracy: 0.09 - ETA: 3s - loss: 0.4780 - accuracy: 0.09 - ETA: 3s - loss: 0.4514 - accuracy: 0.09 - ETA: 3s - loss: 0.4286 - accuracy: 0.09 - ETA: 3s - loss: 0.4071 - accuracy: 0.09 - ETA: 3s - loss: 0.3885 - accuracy: 0.09 - ETA: 3s - loss: 0.3700 - accuracy: 0.09 - ETA: 3s - loss: 0.3538 - accuracy: 0.09 - ETA: 3s - loss: 0.3397 - accuracy: 0.09 - ETA: 3s - loss: 0.3261 - accuracy: 0.09 - ETA: 3s - loss: 0.3136 - accuracy: 0.09 - ETA: 3s - loss: 0.3025 - accuracy: 0.09 - ETA: 3s - loss: 0.2922 - accuracy: 0.09 - ETA: 3s - loss: 0.2821 - accuracy: 0.09 - ETA: 3s - loss: 0.2731 - accuracy: 0.09 - ETA: 3s - loss: 0.2646 - accuracy: 0.09 - ETA: 3s - loss: 0.2571 - accuracy: 0.09 - ETA: 3s - loss: 0.2492 - accuracy: 0.09 - ETA: 3s - loss: 0.2422 - accuracy: 0.09 - ETA: 3s - loss: 0.2353 - accuracy: 0.09 - ETA: 2s - loss: 0.2290 - accuracy: 0.09 - ETA: 2s - loss: 0.2231 - accuracy: 0.09 - ETA: 2s - loss: 0.2174 - accuracy: 0.09 - ETA: 2s - loss: 0.2118 - accuracy: 0.09 - ETA: 2s - loss: 0.2065 - accuracy: 0.09 - ETA: 2s - loss: 0.2015 - accuracy: 0.09 - ETA: 2s - loss: 0.1967 - accuracy: 0.09 - ETA: 2s - loss: 0.1923 - accuracy: 0.09 - ETA: 2s - loss: 0.1881 - accuracy: 0.09 - ETA: 2s - loss: 0.1840 - accuracy: 0.09 - ETA: 2s - loss: 0.1800 - accuracy: 0.09 - ETA: 2s - loss: 0.1764 - accuracy: 0.09 - ETA: 2s - loss: 0.1728 - accuracy: 0.09 - ETA: 2s - loss: 0.1694 - accuracy: 0.09 - ETA: 2s - loss: 0.1661 - accuracy: 0.09 - ETA: 2s - loss: 0.1628 - accuracy: 0.09 - ETA: 2s - loss: 0.1598 - accuracy: 0.09 - ETA: 2s - loss: 0.1569 - accuracy: 0.09 - ETA: 2s - loss: 0.1541 - accuracy: 0.09 - ETA: 1s - loss: 0.1512 - accuracy: 0.09 - ETA: 1s - loss: 0.1486 - accuracy: 0.09 - ETA: 1s - loss: 0.1460 - accuracy: 0.09 - ETA: 1s - loss: 0.1436 - accuracy: 0.09 - ETA: 1s - loss: 0.1412 - accuracy: 0.09 - ETA: 1s - loss: 0.1390 - accuracy: 0.09 - ETA: 1s - loss: 0.1367 - accuracy: 0.09 - ETA: 1s - loss: 0.1345 - accuracy: 0.09 - ETA: 1s - loss: 0.1324 - accuracy: 0.09 - ETA: 1s - loss: 0.1305 - accuracy: 0.09 - ETA: 1s - loss: 0.1285 - accuracy: 0.10 - ETA: 1s - loss: 0.1266 - accuracy: 0.10 - ETA: 1s - loss: 0.1247 - accuracy: 0.10 - ETA: 1s - loss: 0.1229 - accuracy: 0.09 - ETA: 1s - loss: 0.1211 - accuracy: 0.09 - ETA: 1s - loss: 0.1194 - accuracy: 0.09 - ETA: 1s - loss: 0.1178 - accuracy: 0.09 - ETA: 1s - loss: 0.1161 - accuracy: 0.09 - ETA: 1s - loss: 0.1145 - accuracy: 0.09 - ETA: 1s - loss: 0.1130 - accuracy: 0.09 - ETA: 0s - loss: 0.1116 - accuracy: 0.09 - ETA: 0s - loss: 0.1101 - accuracy: 0.09 - ETA: 0s - loss: 0.1088 - accuracy: 0.09 - ETA: 0s - loss: 0.1074 - accuracy: 0.09 - ETA: 0s - loss: 0.1061 - accuracy: 0.09 - ETA: 0s - loss: 0.1047 - accuracy: 0.09 - ETA: 0s - loss: 0.1035 - accuracy: 0.09 - ETA: 0s - loss: 0.1022 - accuracy: 0.09 - ETA: 0s - loss: 0.1010 - accuracy: 0.09 - ETA: 0s - loss: 0.0998 - accuracy: 0.09 - ETA: 0s - loss: 0.0986 - accuracy: 0.09 - ETA: 0s - loss: 0.0975 - accuracy: 0.09 - ETA: 0s - loss: 0.0964 - accuracy: 0.09 - ETA: 0s - loss: 0.0954 - accuracy: 0.09 - ETA: 0s - loss: 0.0944 - accuracy: 0.09 - ETA: 0s - loss: 0.0934 - accuracy: 0.09 - ETA: 0s - loss: 0.0925 - accuracy: 0.09 - ETA: 0s - loss: 0.0916 - accuracy: 0.09 - ETA: 0s - loss: 0.0907 - accuracy: 0.09 - ETA: 0s - loss: 0.0898 - accuracy: 0.09 - 6s 3ms/step - loss: 0.0897 - accuracy: 0.0991 - val_loss: 0.0023 - val_accuracy: 0.0997\n",
      "Epoch 2/5\n",
      "2188/2188 [==============================] - ETA: 2s - loss: 8.8028e-04 - accuracy: 0.03 - ETA: 4s - loss: 0.0018 - accuracy: 0.0807   - ETA: 4s - loss: 0.0020 - accuracy: 0.09 - ETA: 4s - loss: 0.0020 - accuracy: 0.08 - ETA: 4s - loss: 0.0020 - accuracy: 0.08 - ETA: 4s - loss: 0.0020 - accuracy: 0.09 - ETA: 4s - loss: 0.0020 - accuracy: 0.09 - ETA: 4s - loss: 0.0020 - accuracy: 0.09 - ETA: 4s - loss: 0.0019 - accuracy: 0.09 - ETA: 4s - loss: 0.0021 - accuracy: 0.09 - ETA: 4s - loss: 0.0020 - accuracy: 0.09 - ETA: 4s - loss: 0.0020 - accuracy: 0.09 - ETA: 4s - loss: 0.0019 - accuracy: 0.09 - ETA: 4s - loss: 0.0019 - accuracy: 0.09 - ETA: 4s - loss: 0.0019 - accuracy: 0.09 - ETA: 4s - loss: 0.0019 - accuracy: 0.09 - ETA: 3s - loss: 0.0019 - accuracy: 0.09 - ETA: 3s - loss: 0.0019 - accuracy: 0.09 - ETA: 3s - loss: 0.0019 - accuracy: 0.09 - ETA: 3s - loss: 0.0019 - accuracy: 0.09 - ETA: 3s - loss: 0.0020 - accuracy: 0.09 - ETA: 3s - loss: 0.0019 - accuracy: 0.09 - ETA: 3s - loss: 0.0019 - accuracy: 0.09 - ETA: 3s - loss: 0.0019 - accuracy: 0.10 - ETA: 3s - loss: 0.0019 - accuracy: 0.10 - ETA: 3s - loss: 0.0019 - accuracy: 0.10 - ETA: 3s - loss: 0.0019 - accuracy: 0.10 - ETA: 3s - loss: 0.0018 - accuracy: 0.10 - ETA: 3s - loss: 0.0018 - accuracy: 0.10 - ETA: 3s - loss: 0.0018 - accuracy: 0.10 - ETA: 3s - loss: 0.0018 - accuracy: 0.10 - ETA: 3s - loss: 0.0018 - accuracy: 0.10 - ETA: 3s - loss: 0.0018 - accuracy: 0.10 - ETA: 3s - loss: 0.0018 - accuracy: 0.10 - ETA: 3s - loss: 0.0017 - accuracy: 0.10 - ETA: 3s - loss: 0.0017 - accuracy: 0.10 - ETA: 3s - loss: 0.0017 - accuracy: 0.09 - ETA: 2s - loss: 0.0017 - accuracy: 0.09 - ETA: 2s - loss: 0.0017 - accuracy: 0.09 - ETA: 2s - loss: 0.0017 - accuracy: 0.09 - ETA: 2s - loss: 0.0017 - accuracy: 0.09 - ETA: 2s - loss: 0.0017 - accuracy: 0.09 - ETA: 2s - loss: 0.0017 - accuracy: 0.09 - ETA: 2s - loss: 0.0017 - accuracy: 0.09 - ETA: 2s - loss: 0.0017 - accuracy: 0.09 - ETA: 2s - loss: 0.0017 - accuracy: 0.09 - ETA: 2s - loss: 0.0017 - accuracy: 0.09 - ETA: 2s - loss: 0.0017 - accuracy: 0.09 - ETA: 2s - loss: 0.0017 - accuracy: 0.09 - ETA: 2s - loss: 0.0017 - accuracy: 0.09 - ETA: 2s - loss: 0.0017 - accuracy: 0.09 - ETA: 2s - loss: 0.0017 - accuracy: 0.09 - ETA: 2s - loss: 0.0017 - accuracy: 0.09 - ETA: 2s - loss: 0.0017 - accuracy: 0.09 - ETA: 2s - loss: 0.0017 - accuracy: 0.09 - ETA: 2s - loss: 0.0017 - accuracy: 0.09 - ETA: 2s - loss: 0.0017 - accuracy: 0.09 - ETA: 1s - loss: 0.0017 - accuracy: 0.09 - ETA: 1s - loss: 0.0016 - accuracy: 0.09 - ETA: 1s - loss: 0.0016 - accuracy: 0.09 - ETA: 1s - loss: 0.0016 - accuracy: 0.09 - ETA: 1s - loss: 0.0016 - accuracy: 0.09 - ETA: 1s - loss: 0.0016 - accuracy: 0.09 - ETA: 1s - loss: 0.0016 - accuracy: 0.09 - ETA: 1s - loss: 0.0017 - accuracy: 0.09 - ETA: 1s - loss: 0.0016 - accuracy: 0.09 - ETA: 1s - loss: 0.0016 - accuracy: 0.09 - ETA: 1s - loss: 0.0016 - accuracy: 0.09 - ETA: 1s - loss: 0.0016 - accuracy: 0.09 - ETA: 1s - loss: 0.0016 - accuracy: 0.09 - ETA: 1s - loss: 0.0016 - accuracy: 0.09 - ETA: 1s - loss: 0.0016 - accuracy: 0.09 - ETA: 1s - loss: 0.0016 - accuracy: 0.09 - ETA: 1s - loss: 0.0016 - accuracy: 0.10 - ETA: 1s - loss: 0.0016 - accuracy: 0.10 - ETA: 1s - loss: 0.0016 - accuracy: 0.10 - ETA: 1s - loss: 0.0016 - accuracy: 0.10 - ETA: 0s - loss: 0.0016 - accuracy: 0.10 - ETA: 0s - loss: 0.0016 - accuracy: 0.10 - ETA: 0s - loss: 0.0016 - accuracy: 0.10 - ETA: 0s - loss: 0.0016 - accuracy: 0.09 - ETA: 0s - loss: 0.0016 - accuracy: 0.09 - ETA: 0s - loss: 0.0016 - accuracy: 0.10 - ETA: 0s - loss: 0.0016 - accuracy: 0.10 - ETA: 0s - loss: 0.0016 - accuracy: 0.10 - ETA: 0s - loss: 0.0015 - accuracy: 0.09 - ETA: 0s - loss: 0.0015 - accuracy: 0.09 - ETA: 0s - loss: 0.0015 - accuracy: 0.09 - ETA: 0s - loss: 0.0015 - accuracy: 0.09 - ETA: 0s - loss: 0.0015 - accuracy: 0.09 - ETA: 0s - loss: 0.0015 - accuracy: 0.09 - ETA: 0s - loss: 0.0015 - accuracy: 0.09 - ETA: 0s - loss: 0.0015 - accuracy: 0.09 - ETA: 0s - loss: 0.0015 - accuracy: 0.09 - ETA: 0s - loss: 0.0015 - accuracy: 0.10 - ETA: 0s - loss: 0.0015 - accuracy: 0.10 - ETA: 0s - loss: 0.0015 - accuracy: 0.10 - 6s 3ms/step - loss: 0.0015 - accuracy: 0.1001 - val_loss: 0.0011 - val_accuracy: 0.0997\n",
      "Epoch 3/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 1.3811e-04 - accuracy: 0.12 - ETA: 4s - loss: 8.7425e-04 - accuracy: 0.10 - ETA: 4s - loss: 9.0821e-04 - accuracy: 0.09 - ETA: 4s - loss: 9.4210e-04 - accuracy: 0.09 - ETA: 4s - loss: 9.4551e-04 - accuracy: 0.09 - ETA: 4s - loss: 9.4253e-04 - accuracy: 0.09 - ETA: 4s - loss: 9.3515e-04 - accuracy: 0.09 - ETA: 4s - loss: 9.2787e-04 - accuracy: 0.09 - ETA: 4s - loss: 8.9991e-04 - accuracy: 0.09 - ETA: 4s - loss: 0.0011 - accuracy: 0.0989   - ETA: 4s - loss: 0.0010 - accuracy: 0.09 - ETA: 4s - loss: 0.0010 - accuracy: 0.10 - ETA: 4s - loss: 9.7324e-04 - accuracy: 0.10 - ETA: 4s - loss: 0.0010 - accuracy: 0.0999   - ETA: 4s - loss: 0.0010 - accuracy: 0.10 - ETA: 4s - loss: 0.0010 - accuracy: 0.10 - ETA: 4s - loss: 9.8578e-04 - accuracy: 0.09 - ETA: 3s - loss: 9.8958e-04 - accuracy: 0.10 - ETA: 3s - loss: 9.7439e-04 - accuracy: 0.10 - ETA: 3s - loss: 9.5391e-04 - accuracy: 0.10 - ETA: 3s - loss: 9.4304e-04 - accuracy: 0.10 - ETA: 3s - loss: 9.3862e-04 - accuracy: 0.10 - ETA: 3s - loss: 9.3906e-04 - accuracy: 0.09 - ETA: 3s - loss: 9.4863e-04 - accuracy: 0.10 - ETA: 3s - loss: 9.5286e-04 - accuracy: 0.09 - ETA: 3s - loss: 9.4533e-04 - accuracy: 0.09 - ETA: 3s - loss: 9.4241e-04 - accuracy: 0.09 - ETA: 3s - loss: 9.3288e-04 - accuracy: 0.10 - ETA: 3s - loss: 9.3011e-04 - accuracy: 0.10 - ETA: 3s - loss: 9.1683e-04 - accuracy: 0.09 - ETA: 3s - loss: 9.0722e-04 - accuracy: 0.09 - ETA: 3s - loss: 9.0481e-04 - accuracy: 0.09 - ETA: 3s - loss: 9.2595e-04 - accuracy: 0.09 - ETA: 3s - loss: 9.2207e-04 - accuracy: 0.09 - ETA: 3s - loss: 9.1569e-04 - accuracy: 0.09 - ETA: 3s - loss: 9.0793e-04 - accuracy: 0.09 - ETA: 3s - loss: 9.0672e-04 - accuracy: 0.10 - ETA: 2s - loss: 8.9860e-04 - accuracy: 0.09 - ETA: 2s - loss: 9.0290e-04 - accuracy: 0.09 - ETA: 2s - loss: 9.1168e-04 - accuracy: 0.09 - ETA: 2s - loss: 9.0524e-04 - accuracy: 0.10 - ETA: 2s - loss: 8.9751e-04 - accuracy: 0.09 - ETA: 2s - loss: 8.9319e-04 - accuracy: 0.09 - ETA: 2s - loss: 8.8638e-04 - accuracy: 0.10 - ETA: 2s - loss: 8.8463e-04 - accuracy: 0.10 - ETA: 2s - loss: 8.8462e-04 - accuracy: 0.10 - ETA: 2s - loss: 8.8651e-04 - accuracy: 0.10 - ETA: 2s - loss: 8.8347e-04 - accuracy: 0.10 - ETA: 2s - loss: 9.1423e-04 - accuracy: 0.10 - ETA: 2s - loss: 9.1347e-04 - accuracy: 0.10 - ETA: 2s - loss: 9.0557e-04 - accuracy: 0.10 - ETA: 2s - loss: 8.9882e-04 - accuracy: 0.10 - ETA: 2s - loss: 8.9293e-04 - accuracy: 0.10 - ETA: 2s - loss: 8.9023e-04 - accuracy: 0.10 - ETA: 2s - loss: 8.9194e-04 - accuracy: 0.10 - ETA: 2s - loss: 8.8920e-04 - accuracy: 0.10 - ETA: 2s - loss: 9.0868e-04 - accuracy: 0.10 - ETA: 1s - loss: 9.1393e-04 - accuracy: 0.10 - ETA: 1s - loss: 9.1765e-04 - accuracy: 0.10 - ETA: 1s - loss: 9.1600e-04 - accuracy: 0.10 - ETA: 1s - loss: 9.1051e-04 - accuracy: 0.10 - ETA: 1s - loss: 9.1004e-04 - accuracy: 0.10 - ETA: 1s - loss: 9.0596e-04 - accuracy: 0.10 - ETA: 1s - loss: 9.0175e-04 - accuracy: 0.10 - ETA: 1s - loss: 8.9628e-04 - accuracy: 0.10 - ETA: 1s - loss: 8.9022e-04 - accuracy: 0.10 - ETA: 1s - loss: 8.8210e-04 - accuracy: 0.10 - ETA: 1s - loss: 9.0605e-04 - accuracy: 0.10 - ETA: 1s - loss: 9.0151e-04 - accuracy: 0.10 - ETA: 1s - loss: 9.0031e-04 - accuracy: 0.10 - ETA: 1s - loss: 8.9566e-04 - accuracy: 0.10 - ETA: 1s - loss: 8.9432e-04 - accuracy: 0.10 - ETA: 1s - loss: 8.9528e-04 - accuracy: 0.10 - ETA: 1s - loss: 8.9645e-04 - accuracy: 0.10 - ETA: 1s - loss: 8.9481e-04 - accuracy: 0.10 - ETA: 1s - loss: 8.9127e-04 - accuracy: 0.10 - ETA: 1s - loss: 8.8812e-04 - accuracy: 0.10 - ETA: 0s - loss: 8.9151e-04 - accuracy: 0.10 - ETA: 0s - loss: 8.8515e-04 - accuracy: 0.10 - ETA: 0s - loss: 8.8206e-04 - accuracy: 0.10 - ETA: 0s - loss: 8.7739e-04 - accuracy: 0.10 - ETA: 0s - loss: 8.7810e-04 - accuracy: 0.10 - ETA: 0s - loss: 8.7243e-04 - accuracy: 0.10 - ETA: 0s - loss: 8.6995e-04 - accuracy: 0.10 - ETA: 0s - loss: 8.6667e-04 - accuracy: 0.10 - ETA: 0s - loss: 8.6626e-04 - accuracy: 0.10 - ETA: 0s - loss: 8.6389e-04 - accuracy: 0.10 - ETA: 0s - loss: 8.6283e-04 - accuracy: 0.10 - ETA: 0s - loss: 8.5826e-04 - accuracy: 0.10 - ETA: 0s - loss: 8.5548e-04 - accuracy: 0.10 - ETA: 0s - loss: 8.5529e-04 - accuracy: 0.10 - ETA: 0s - loss: 8.5349e-04 - accuracy: 0.10 - ETA: 0s - loss: 8.5184e-04 - accuracy: 0.10 - ETA: 0s - loss: 8.4872e-04 - accuracy: 0.10 - ETA: 0s - loss: 8.4480e-04 - accuracy: 0.10 - ETA: 0s - loss: 8.4076e-04 - accuracy: 0.10 - ETA: 0s - loss: 8.3656e-04 - accuracy: 0.10 - 6s 3ms/step - loss: 8.3417e-04 - accuracy: 0.1001 - val_loss: 7.0228e-04 - val_accuracy: 0.0997\n",
      "Epoch 4/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 1.4729e-04 - accuracy: 0.15 - ETA: 4s - loss: 4.6123e-04 - accuracy: 0.08 - ETA: 4s - loss: 6.0486e-04 - accuracy: 0.09 - ETA: 4s - loss: 7.4305e-04 - accuracy: 0.09 - ETA: 4s - loss: 7.7164e-04 - accuracy: 0.10 - ETA: 4s - loss: 7.2087e-04 - accuracy: 0.10 - ETA: 4s - loss: 6.9488e-04 - accuracy: 0.10 - ETA: 4s - loss: 8.9658e-04 - accuracy: 0.10 - ETA: 4s - loss: 8.4397e-04 - accuracy: 0.10 - ETA: 4s - loss: 8.5484e-04 - accuracy: 0.10 - ETA: 4s - loss: 8.9976e-04 - accuracy: 0.10 - ETA: 4s - loss: 8.5835e-04 - accuracy: 0.10 - ETA: 4s - loss: 8.5560e-04 - accuracy: 0.09 - ETA: 4s - loss: 8.3622e-04 - accuracy: 0.09 - ETA: 4s - loss: 8.1771e-04 - accuracy: 0.09 - ETA: 4s - loss: 7.9424e-04 - accuracy: 0.09 - ETA: 4s - loss: 7.7561e-04 - accuracy: 0.09 - ETA: 4s - loss: 7.6378e-04 - accuracy: 0.09 - ETA: 3s - loss: 7.4930e-04 - accuracy: 0.09 - ETA: 3s - loss: 7.3734e-04 - accuracy: 0.09 - ETA: 3s - loss: 7.2049e-04 - accuracy: 0.09 - ETA: 3s - loss: 7.0452e-04 - accuracy: 0.09 - ETA: 3s - loss: 6.9195e-04 - accuracy: 0.09 - ETA: 3s - loss: 6.8248e-04 - accuracy: 0.09 - ETA: 3s - loss: 6.7670e-04 - accuracy: 0.09 - ETA: 3s - loss: 6.7490e-04 - accuracy: 0.09 - ETA: 3s - loss: 6.7282e-04 - accuracy: 0.09 - ETA: 3s - loss: 6.7052e-04 - accuracy: 0.09 - ETA: 3s - loss: 6.6382e-04 - accuracy: 0.09 - ETA: 3s - loss: 6.5969e-04 - accuracy: 0.09 - ETA: 3s - loss: 6.5469e-04 - accuracy: 0.09 - ETA: 3s - loss: 6.5633e-04 - accuracy: 0.09 - ETA: 3s - loss: 6.5296e-04 - accuracy: 0.09 - ETA: 3s - loss: 6.4739e-04 - accuracy: 0.09 - ETA: 3s - loss: 6.3969e-04 - accuracy: 0.09 - ETA: 3s - loss: 6.3334e-04 - accuracy: 0.09 - ETA: 3s - loss: 6.3877e-04 - accuracy: 0.09 - ETA: 3s - loss: 6.3841e-04 - accuracy: 0.09 - ETA: 3s - loss: 6.4016e-04 - accuracy: 0.09 - ETA: 2s - loss: 6.7368e-04 - accuracy: 0.09 - ETA: 2s - loss: 6.6758e-04 - accuracy: 0.09 - ETA: 2s - loss: 6.7177e-04 - accuracy: 0.09 - ETA: 2s - loss: 6.6457e-04 - accuracy: 0.09 - ETA: 2s - loss: 6.5846e-04 - accuracy: 0.09 - ETA: 2s - loss: 6.5468e-04 - accuracy: 0.09 - ETA: 2s - loss: 6.4921e-04 - accuracy: 0.09 - ETA: 2s - loss: 6.4509e-04 - accuracy: 0.09 - ETA: 2s - loss: 6.4150e-04 - accuracy: 0.09 - ETA: 2s - loss: 6.3710e-04 - accuracy: 0.09 - ETA: 2s - loss: 6.4434e-04 - accuracy: 0.09 - ETA: 2s - loss: 6.4299e-04 - accuracy: 0.09 - ETA: 2s - loss: 6.3578e-04 - accuracy: 0.09 - ETA: 2s - loss: 6.3521e-04 - accuracy: 0.09 - ETA: 2s - loss: 6.3243e-04 - accuracy: 0.09 - ETA: 2s - loss: 6.2728e-04 - accuracy: 0.09 - ETA: 2s - loss: 6.2179e-04 - accuracy: 0.09 - ETA: 2s - loss: 6.1818e-04 - accuracy: 0.09 - ETA: 2s - loss: 6.1509e-04 - accuracy: 0.09 - ETA: 1s - loss: 6.1207e-04 - accuracy: 0.09 - ETA: 1s - loss: 6.0951e-04 - accuracy: 0.09 - ETA: 1s - loss: 6.0546e-04 - accuracy: 0.09 - ETA: 1s - loss: 6.0347e-04 - accuracy: 0.09 - ETA: 1s - loss: 5.9997e-04 - accuracy: 0.09 - ETA: 1s - loss: 6.0209e-04 - accuracy: 0.09 - ETA: 1s - loss: 6.0139e-04 - accuracy: 0.09 - ETA: 1s - loss: 5.9989e-04 - accuracy: 0.09 - ETA: 1s - loss: 5.9515e-04 - accuracy: 0.09 - ETA: 1s - loss: 5.9376e-04 - accuracy: 0.09 - ETA: 1s - loss: 5.9140e-04 - accuracy: 0.09 - ETA: 1s - loss: 5.8769e-04 - accuracy: 0.09 - ETA: 1s - loss: 5.9649e-04 - accuracy: 0.09 - ETA: 1s - loss: 5.9534e-04 - accuracy: 0.09 - ETA: 1s - loss: 5.9536e-04 - accuracy: 0.09 - ETA: 1s - loss: 5.9070e-04 - accuracy: 0.09 - ETA: 1s - loss: 5.8874e-04 - accuracy: 0.09 - ETA: 1s - loss: 5.8918e-04 - accuracy: 0.09 - ETA: 1s - loss: 5.8657e-04 - accuracy: 0.09 - ETA: 1s - loss: 5.8384e-04 - accuracy: 0.09 - ETA: 0s - loss: 5.8205e-04 - accuracy: 0.09 - ETA: 0s - loss: 5.7948e-04 - accuracy: 0.09 - ETA: 0s - loss: 5.8169e-04 - accuracy: 0.09 - ETA: 0s - loss: 5.7765e-04 - accuracy: 0.09 - ETA: 0s - loss: 5.9111e-04 - accuracy: 0.09 - ETA: 0s - loss: 6.1291e-04 - accuracy: 0.09 - ETA: 0s - loss: 6.1233e-04 - accuracy: 0.09 - ETA: 0s - loss: 6.0955e-04 - accuracy: 0.09 - ETA: 0s - loss: 6.0770e-04 - accuracy: 0.09 - ETA: 0s - loss: 6.0518e-04 - accuracy: 0.09 - ETA: 0s - loss: 6.0250e-04 - accuracy: 0.09 - ETA: 0s - loss: 6.0032e-04 - accuracy: 0.10 - ETA: 0s - loss: 5.9820e-04 - accuracy: 0.10 - ETA: 0s - loss: 5.9636e-04 - accuracy: 0.10 - ETA: 0s - loss: 5.9418e-04 - accuracy: 0.10 - ETA: 0s - loss: 5.9149e-04 - accuracy: 0.10 - ETA: 0s - loss: 5.8991e-04 - accuracy: 0.10 - ETA: 0s - loss: 5.8912e-04 - accuracy: 0.09 - ETA: 0s - loss: 5.8789e-04 - accuracy: 0.09 - ETA: 0s - loss: 5.8481e-04 - accuracy: 0.10 - 6s 3ms/step - loss: 5.8551e-04 - accuracy: 0.1001 - val_loss: 5.2983e-04 - val_accuracy: 0.0997\n",
      "Epoch 5/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 0.0016 - accuracy: 0.09 - ETA: 4s - loss: 4.6086e-04 - accuracy: 0.11 - ETA: 4s - loss: 4.2487e-04 - accuracy: 0.09 - ETA: 4s - loss: 9.3531e-04 - accuracy: 0.09 - ETA: 4s - loss: 8.9752e-04 - accuracy: 0.09 - ETA: 4s - loss: 7.9619e-04 - accuracy: 0.09 - ETA: 4s - loss: 7.0957e-04 - accuracy: 0.10 - ETA: 4s - loss: 6.5829e-04 - accuracy: 0.10 - ETA: 4s - loss: 6.1033e-04 - accuracy: 0.10 - ETA: 4s - loss: 6.0128e-04 - accuracy: 0.10 - ETA: 4s - loss: 5.8113e-04 - accuracy: 0.11 - ETA: 4s - loss: 5.8114e-04 - accuracy: 0.10 - ETA: 4s - loss: 5.5763e-04 - accuracy: 0.10 - ETA: 4s - loss: 5.3814e-04 - accuracy: 0.10 - ETA: 4s - loss: 5.2159e-04 - accuracy: 0.10 - ETA: 4s - loss: 5.1075e-04 - accuracy: 0.10 - ETA: 4s - loss: 4.9496e-04 - accuracy: 0.10 - ETA: 4s - loss: 4.8940e-04 - accuracy: 0.10 - ETA: 4s - loss: 4.8692e-04 - accuracy: 0.10 - ETA: 4s - loss: 4.7934e-04 - accuracy: 0.10 - ETA: 3s - loss: 4.7762e-04 - accuracy: 0.10 - ETA: 3s - loss: 4.7144e-04 - accuracy: 0.10 - ETA: 3s - loss: 4.7275e-04 - accuracy: 0.10 - ETA: 3s - loss: 4.6599e-04 - accuracy: 0.10 - ETA: 3s - loss: 4.6300e-04 - accuracy: 0.10 - ETA: 3s - loss: 4.7656e-04 - accuracy: 0.10 - ETA: 3s - loss: 5.2980e-04 - accuracy: 0.10 - ETA: 3s - loss: 5.2982e-04 - accuracy: 0.10 - ETA: 3s - loss: 5.2298e-04 - accuracy: 0.10 - ETA: 3s - loss: 5.2868e-04 - accuracy: 0.10 - ETA: 3s - loss: 5.2172e-04 - accuracy: 0.10 - ETA: 3s - loss: 5.1518e-04 - accuracy: 0.10 - ETA: 3s - loss: 5.1497e-04 - accuracy: 0.10 - ETA: 3s - loss: 5.2198e-04 - accuracy: 0.10 - ETA: 3s - loss: 5.2642e-04 - accuracy: 0.10 - ETA: 3s - loss: 5.1982e-04 - accuracy: 0.10 - ETA: 3s - loss: 5.1372e-04 - accuracy: 0.10 - ETA: 3s - loss: 5.1010e-04 - accuracy: 0.10 - ETA: 2s - loss: 5.0423e-04 - accuracy: 0.10 - ETA: 2s - loss: 4.9906e-04 - accuracy: 0.10 - ETA: 2s - loss: 4.9399e-04 - accuracy: 0.10 - ETA: 2s - loss: 4.9008e-04 - accuracy: 0.10 - ETA: 2s - loss: 4.9519e-04 - accuracy: 0.10 - ETA: 2s - loss: 4.9726e-04 - accuracy: 0.10 - ETA: 2s - loss: 4.9234e-04 - accuracy: 0.10 - ETA: 2s - loss: 4.9478e-04 - accuracy: 0.10 - ETA: 2s - loss: 4.8845e-04 - accuracy: 0.10 - ETA: 2s - loss: 4.8423e-04 - accuracy: 0.10 - ETA: 2s - loss: 4.7896e-04 - accuracy: 0.10 - ETA: 2s - loss: 4.8048e-04 - accuracy: 0.10 - ETA: 2s - loss: 4.7670e-04 - accuracy: 0.10 - ETA: 2s - loss: 4.7648e-04 - accuracy: 0.10 - ETA: 2s - loss: 4.7145e-04 - accuracy: 0.10 - ETA: 2s - loss: 4.7132e-04 - accuracy: 0.10 - ETA: 2s - loss: 4.7082e-04 - accuracy: 0.10 - ETA: 2s - loss: 4.6716e-04 - accuracy: 0.10 - ETA: 2s - loss: 4.8714e-04 - accuracy: 0.10 - ETA: 2s - loss: 5.1440e-04 - accuracy: 0.10 - ETA: 1s - loss: 5.1242e-04 - accuracy: 0.10 - ETA: 1s - loss: 5.1206e-04 - accuracy: 0.10 - ETA: 1s - loss: 5.0763e-04 - accuracy: 0.10 - ETA: 1s - loss: 5.0499e-04 - accuracy: 0.10 - ETA: 1s - loss: 5.0147e-04 - accuracy: 0.10 - ETA: 1s - loss: 5.0235e-04 - accuracy: 0.10 - ETA: 1s - loss: 4.9940e-04 - accuracy: 0.09 - ETA: 1s - loss: 5.0056e-04 - accuracy: 0.09 - ETA: 1s - loss: 4.9790e-04 - accuracy: 0.10 - ETA: 1s - loss: 4.9525e-04 - accuracy: 0.10 - ETA: 1s - loss: 4.9302e-04 - accuracy: 0.10 - ETA: 1s - loss: 4.9086e-04 - accuracy: 0.10 - ETA: 1s - loss: 4.8910e-04 - accuracy: 0.10 - ETA: 1s - loss: 4.8664e-04 - accuracy: 0.10 - ETA: 1s - loss: 4.8261e-04 - accuracy: 0.10 - ETA: 1s - loss: 4.8017e-04 - accuracy: 0.10 - ETA: 1s - loss: 4.7846e-04 - accuracy: 0.10 - ETA: 1s - loss: 4.7630e-04 - accuracy: 0.10 - ETA: 1s - loss: 4.7382e-04 - accuracy: 0.10 - ETA: 1s - loss: 4.7167e-04 - accuracy: 0.10 - ETA: 0s - loss: 4.7193e-04 - accuracy: 0.10 - ETA: 0s - loss: 4.6963e-04 - accuracy: 0.10 - ETA: 0s - loss: 4.6736e-04 - accuracy: 0.10 - ETA: 0s - loss: 4.6695e-04 - accuracy: 0.10 - ETA: 0s - loss: 4.7326e-04 - accuracy: 0.10 - ETA: 0s - loss: 4.7073e-04 - accuracy: 0.10 - ETA: 0s - loss: 4.6863e-04 - accuracy: 0.10 - ETA: 0s - loss: 4.6669e-04 - accuracy: 0.09 - ETA: 0s - loss: 4.6722e-04 - accuracy: 0.10 - ETA: 0s - loss: 4.6521e-04 - accuracy: 0.09 - ETA: 0s - loss: 4.6392e-04 - accuracy: 0.10 - ETA: 0s - loss: 4.6428e-04 - accuracy: 0.10 - ETA: 0s - loss: 4.6212e-04 - accuracy: 0.10 - ETA: 0s - loss: 4.6121e-04 - accuracy: 0.10 - ETA: 0s - loss: 4.6080e-04 - accuracy: 0.10 - ETA: 0s - loss: 4.5902e-04 - accuracy: 0.10 - ETA: 0s - loss: 4.5615e-04 - accuracy: 0.10 - ETA: 0s - loss: 4.5568e-04 - accuracy: 0.09 - ETA: 0s - loss: 4.5478e-04 - accuracy: 0.10 - 6s 3ms/step - loss: 4.5476e-04 - accuracy: 0.1001 - val_loss: 4.2923e-04 - val_accuracy: 0.0997\n",
      "Epoch 1/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 2.3249 - accuracy: 0.0000e+ - ETA: 4s - loss: 2.1828 - accuracy: 0.0190   - ETA: 4s - loss: 2.0175 - accuracy: 0.05 - ETA: 4s - loss: 1.8096 - accuracy: 0.05 - ETA: 5s - loss: 1.6436 - accuracy: 0.06 - ETA: 4s - loss: 1.4184 - accuracy: 0.07 - ETA: 4s - loss: 1.2438 - accuracy: 0.07 - ETA: 4s - loss: 1.0878 - accuracy: 0.08 - ETA: 4s - loss: 0.9637 - accuracy: 0.08 - ETA: 4s - loss: 0.8637 - accuracy: 0.08 - ETA: 4s - loss: 0.7827 - accuracy: 0.08 - ETA: 4s - loss: 0.7174 - accuracy: 0.09 - ETA: 4s - loss: 0.6599 - accuracy: 0.09 - ETA: 4s - loss: 0.6108 - accuracy: 0.09 - ETA: 4s - loss: 0.5685 - accuracy: 0.09 - ETA: 4s - loss: 0.5303 - accuracy: 0.09 - ETA: 4s - loss: 0.4982 - accuracy: 0.09 - ETA: 4s - loss: 0.4699 - accuracy: 0.09 - ETA: 4s - loss: 0.4435 - accuracy: 0.09 - ETA: 3s - loss: 0.4209 - accuracy: 0.09 - ETA: 3s - loss: 0.3996 - accuracy: 0.09 - ETA: 3s - loss: 0.3812 - accuracy: 0.09 - ETA: 3s - loss: 0.3644 - accuracy: 0.09 - ETA: 3s - loss: 0.3490 - accuracy: 0.09 - ETA: 3s - loss: 0.3348 - accuracy: 0.09 - ETA: 3s - loss: 0.3224 - accuracy: 0.09 - ETA: 3s - loss: 0.3113 - accuracy: 0.09 - ETA: 3s - loss: 0.3015 - accuracy: 0.09 - ETA: 3s - loss: 0.2918 - accuracy: 0.09 - ETA: 3s - loss: 0.2819 - accuracy: 0.09 - ETA: 3s - loss: 0.2727 - accuracy: 0.09 - ETA: 3s - loss: 0.2641 - accuracy: 0.09 - ETA: 3s - loss: 0.2559 - accuracy: 0.09 - ETA: 3s - loss: 0.2483 - accuracy: 0.09 - ETA: 3s - loss: 0.2411 - accuracy: 0.09 - ETA: 3s - loss: 0.2340 - accuracy: 0.09 - ETA: 3s - loss: 0.2276 - accuracy: 0.09 - ETA: 3s - loss: 0.2215 - accuracy: 0.09 - ETA: 2s - loss: 0.2158 - accuracy: 0.09 - ETA: 2s - loss: 0.2104 - accuracy: 0.09 - ETA: 2s - loss: 0.2050 - accuracy: 0.09 - ETA: 2s - loss: 0.2003 - accuracy: 0.09 - ETA: 2s - loss: 0.1956 - accuracy: 0.09 - ETA: 2s - loss: 0.1911 - accuracy: 0.09 - ETA: 2s - loss: 0.1869 - accuracy: 0.09 - ETA: 2s - loss: 0.1828 - accuracy: 0.09 - ETA: 2s - loss: 0.1789 - accuracy: 0.09 - ETA: 2s - loss: 0.1751 - accuracy: 0.09 - ETA: 2s - loss: 0.1715 - accuracy: 0.09 - ETA: 2s - loss: 0.1681 - accuracy: 0.09 - ETA: 2s - loss: 0.1648 - accuracy: 0.09 - ETA: 2s - loss: 0.1618 - accuracy: 0.09 - ETA: 2s - loss: 0.1587 - accuracy: 0.09 - ETA: 2s - loss: 0.1559 - accuracy: 0.09 - ETA: 2s - loss: 0.1530 - accuracy: 0.09 - ETA: 2s - loss: 0.1503 - accuracy: 0.09 - ETA: 2s - loss: 0.1477 - accuracy: 0.09 - ETA: 1s - loss: 0.1451 - accuracy: 0.09 - ETA: 1s - loss: 0.1426 - accuracy: 0.09 - ETA: 1s - loss: 0.1403 - accuracy: 0.09 - ETA: 1s - loss: 0.1380 - accuracy: 0.09 - ETA: 1s - loss: 0.1358 - accuracy: 0.09 - ETA: 1s - loss: 0.1337 - accuracy: 0.09 - ETA: 1s - loss: 0.1317 - accuracy: 0.09 - ETA: 1s - loss: 0.1297 - accuracy: 0.09 - ETA: 1s - loss: 0.1278 - accuracy: 0.09 - ETA: 1s - loss: 0.1259 - accuracy: 0.09 - ETA: 1s - loss: 0.1242 - accuracy: 0.09 - ETA: 1s - loss: 0.1224 - accuracy: 0.09 - ETA: 1s - loss: 0.1206 - accuracy: 0.09 - ETA: 1s - loss: 0.1189 - accuracy: 0.09 - ETA: 1s - loss: 0.1173 - accuracy: 0.09 - ETA: 1s - loss: 0.1157 - accuracy: 0.09 - ETA: 1s - loss: 0.1142 - accuracy: 0.09 - ETA: 1s - loss: 0.1127 - accuracy: 0.09 - ETA: 1s - loss: 0.1112 - accuracy: 0.09 - ETA: 1s - loss: 0.1099 - accuracy: 0.09 - ETA: 0s - loss: 0.1084 - accuracy: 0.09 - ETA: 0s - loss: 0.1070 - accuracy: 0.09 - ETA: 0s - loss: 0.1057 - accuracy: 0.09 - ETA: 0s - loss: 0.1044 - accuracy: 0.09 - ETA: 0s - loss: 0.1032 - accuracy: 0.09 - ETA: 0s - loss: 0.1020 - accuracy: 0.09 - ETA: 0s - loss: 0.1011 - accuracy: 0.09 - ETA: 0s - loss: 0.1000 - accuracy: 0.09 - ETA: 0s - loss: 0.0989 - accuracy: 0.09 - ETA: 0s - loss: 0.0978 - accuracy: 0.09 - ETA: 0s - loss: 0.0968 - accuracy: 0.09 - ETA: 0s - loss: 0.0957 - accuracy: 0.09 - ETA: 0s - loss: 0.0947 - accuracy: 0.09 - ETA: 0s - loss: 0.0936 - accuracy: 0.09 - ETA: 0s - loss: 0.0926 - accuracy: 0.09 - ETA: 0s - loss: 0.0916 - accuracy: 0.09 - ETA: 0s - loss: 0.0906 - accuracy: 0.09 - ETA: 0s - loss: 0.0896 - accuracy: 0.09 - ETA: 0s - loss: 0.0887 - accuracy: 0.09 - ETA: 0s - loss: 0.0878 - accuracy: 0.09 - 6s 3ms/step - loss: 0.0871 - accuracy: 0.0992 - val_loss: 0.0024 - val_accuracy: 0.0997\n",
      "Epoch 2/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 9.0057e-04 - accuracy: 0.12 - ETA: 4s - loss: 0.0026 - accuracy: 0.1013   - ETA: 4s - loss: 0.0027 - accuracy: 0.09 - ETA: 4s - loss: 0.0026 - accuracy: 0.09 - ETA: 4s - loss: 0.0025 - accuracy: 0.09 - ETA: 4s - loss: 0.0023 - accuracy: 0.09 - ETA: 4s - loss: 0.0023 - accuracy: 0.09 - ETA: 4s - loss: 0.0022 - accuracy: 0.09 - ETA: 4s - loss: 0.0022 - accuracy: 0.09 - ETA: 4s - loss: 0.0021 - accuracy: 0.10 - ETA: 4s - loss: 0.0021 - accuracy: 0.10 - ETA: 4s - loss: 0.0021 - accuracy: 0.10 - ETA: 4s - loss: 0.0021 - accuracy: 0.10 - ETA: 4s - loss: 0.0022 - accuracy: 0.10 - ETA: 4s - loss: 0.0023 - accuracy: 0.10 - ETA: 4s - loss: 0.0023 - accuracy: 0.10 - ETA: 4s - loss: 0.0024 - accuracy: 0.10 - ETA: 4s - loss: 0.0023 - accuracy: 0.10 - ETA: 3s - loss: 0.0023 - accuracy: 0.10 - ETA: 3s - loss: 0.0022 - accuracy: 0.10 - ETA: 3s - loss: 0.0022 - accuracy: 0.10 - ETA: 3s - loss: 0.0022 - accuracy: 0.09 - ETA: 3s - loss: 0.0022 - accuracy: 0.09 - ETA: 3s - loss: 0.0022 - accuracy: 0.09 - ETA: 3s - loss: 0.0022 - accuracy: 0.09 - ETA: 3s - loss: 0.0022 - accuracy: 0.09 - ETA: 3s - loss: 0.0021 - accuracy: 0.10 - ETA: 3s - loss: 0.0021 - accuracy: 0.09 - ETA: 3s - loss: 0.0021 - accuracy: 0.09 - ETA: 3s - loss: 0.0021 - accuracy: 0.09 - ETA: 3s - loss: 0.0021 - accuracy: 0.09 - ETA: 3s - loss: 0.0021 - accuracy: 0.09 - ETA: 3s - loss: 0.0020 - accuracy: 0.09 - ETA: 3s - loss: 0.0020 - accuracy: 0.09 - ETA: 3s - loss: 0.0020 - accuracy: 0.09 - ETA: 3s - loss: 0.0020 - accuracy: 0.09 - ETA: 3s - loss: 0.0020 - accuracy: 0.09 - ETA: 2s - loss: 0.0020 - accuracy: 0.09 - ETA: 2s - loss: 0.0020 - accuracy: 0.09 - ETA: 2s - loss: 0.0020 - accuracy: 0.09 - ETA: 2s - loss: 0.0019 - accuracy: 0.09 - ETA: 2s - loss: 0.0019 - accuracy: 0.09 - ETA: 2s - loss: 0.0019 - accuracy: 0.10 - ETA: 2s - loss: 0.0019 - accuracy: 0.10 - ETA: 2s - loss: 0.0019 - accuracy: 0.10 - ETA: 2s - loss: 0.0019 - accuracy: 0.10 - ETA: 2s - loss: 0.0019 - accuracy: 0.10 - ETA: 2s - loss: 0.0019 - accuracy: 0.10 - ETA: 2s - loss: 0.0019 - accuracy: 0.10 - ETA: 2s - loss: 0.0019 - accuracy: 0.10 - ETA: 2s - loss: 0.0019 - accuracy: 0.10 - ETA: 2s - loss: 0.0018 - accuracy: 0.10 - ETA: 2s - loss: 0.0018 - accuracy: 0.10 - ETA: 2s - loss: 0.0018 - accuracy: 0.10 - ETA: 2s - loss: 0.0018 - accuracy: 0.10 - ETA: 2s - loss: 0.0018 - accuracy: 0.10 - ETA: 2s - loss: 0.0018 - accuracy: 0.10 - ETA: 1s - loss: 0.0018 - accuracy: 0.10 - ETA: 1s - loss: 0.0018 - accuracy: 0.10 - ETA: 1s - loss: 0.0018 - accuracy: 0.10 - ETA: 1s - loss: 0.0017 - accuracy: 0.10 - ETA: 1s - loss: 0.0017 - accuracy: 0.10 - ETA: 1s - loss: 0.0017 - accuracy: 0.10 - ETA: 1s - loss: 0.0017 - accuracy: 0.10 - ETA: 1s - loss: 0.0017 - accuracy: 0.10 - ETA: 1s - loss: 0.0017 - accuracy: 0.10 - ETA: 1s - loss: 0.0017 - accuracy: 0.10 - ETA: 1s - loss: 0.0017 - accuracy: 0.10 - ETA: 1s - loss: 0.0017 - accuracy: 0.10 - ETA: 1s - loss: 0.0017 - accuracy: 0.10 - ETA: 1s - loss: 0.0017 - accuracy: 0.10 - ETA: 1s - loss: 0.0017 - accuracy: 0.10 - ETA: 1s - loss: 0.0017 - accuracy: 0.10 - ETA: 1s - loss: 0.0017 - accuracy: 0.10 - ETA: 1s - loss: 0.0017 - accuracy: 0.10 - ETA: 1s - loss: 0.0017 - accuracy: 0.10 - ETA: 1s - loss: 0.0017 - accuracy: 0.10 - ETA: 0s - loss: 0.0016 - accuracy: 0.10 - ETA: 0s - loss: 0.0016 - accuracy: 0.10 - ETA: 0s - loss: 0.0016 - accuracy: 0.10 - ETA: 0s - loss: 0.0016 - accuracy: 0.09 - ETA: 0s - loss: 0.0016 - accuracy: 0.09 - ETA: 0s - loss: 0.0016 - accuracy: 0.09 - ETA: 0s - loss: 0.0016 - accuracy: 0.09 - ETA: 0s - loss: 0.0016 - accuracy: 0.10 - ETA: 0s - loss: 0.0016 - accuracy: 0.09 - ETA: 0s - loss: 0.0016 - accuracy: 0.09 - ETA: 0s - loss: 0.0016 - accuracy: 0.09 - ETA: 0s - loss: 0.0016 - accuracy: 0.10 - ETA: 0s - loss: 0.0016 - accuracy: 0.10 - ETA: 0s - loss: 0.0016 - accuracy: 0.10 - ETA: 0s - loss: 0.0016 - accuracy: 0.09 - ETA: 0s - loss: 0.0015 - accuracy: 0.10 - ETA: 0s - loss: 0.0015 - accuracy: 0.09 - ETA: 0s - loss: 0.0015 - accuracy: 0.10 - ETA: 0s - loss: 0.0015 - accuracy: 0.10 - 6s 3ms/step - loss: 0.0015 - accuracy: 0.1001 - val_loss: 0.0011 - val_accuracy: 0.0997\n",
      "Epoch 3/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 0.0016 - accuracy: 0.09 - ETA: 4s - loss: 8.9780e-04 - accuracy: 0.10 - ETA: 4s - loss: 8.4227e-04 - accuracy: 0.09 - ETA: 4s - loss: 8.1924e-04 - accuracy: 0.09 - ETA: 4s - loss: 8.1873e-04 - accuracy: 0.10 - ETA: 4s - loss: 8.6140e-04 - accuracy: 0.09 - ETA: 4s - loss: 8.2917e-04 - accuracy: 0.10 - ETA: 4s - loss: 8.1444e-04 - accuracy: 0.10 - ETA: 4s - loss: 8.1233e-04 - accuracy: 0.09 - ETA: 4s - loss: 8.5259e-04 - accuracy: 0.09 - ETA: 4s - loss: 8.5160e-04 - accuracy: 0.10 - ETA: 4s - loss: 8.5169e-04 - accuracy: 0.10 - ETA: 4s - loss: 8.6376e-04 - accuracy: 0.10 - ETA: 4s - loss: 8.7647e-04 - accuracy: 0.10 - ETA: 4s - loss: 8.8807e-04 - accuracy: 0.10 - ETA: 4s - loss: 9.2321e-04 - accuracy: 0.10 - ETA: 4s - loss: 9.0667e-04 - accuracy: 0.10 - ETA: 3s - loss: 9.2997e-04 - accuracy: 0.10 - ETA: 3s - loss: 9.1576e-04 - accuracy: 0.10 - ETA: 3s - loss: 0.0010 - accuracy: 0.1035   - ETA: 3s - loss: 0.0010 - accuracy: 0.10 - ETA: 3s - loss: 0.0011 - accuracy: 0.10 - ETA: 3s - loss: 0.0011 - accuracy: 0.10 - ETA: 3s - loss: 0.0011 - accuracy: 0.10 - ETA: 3s - loss: 0.0010 - accuracy: 0.10 - ETA: 3s - loss: 0.0010 - accuracy: 0.10 - ETA: 3s - loss: 0.0010 - accuracy: 0.10 - ETA: 3s - loss: 0.0010 - accuracy: 0.10 - ETA: 3s - loss: 0.0010 - accuracy: 0.10 - ETA: 3s - loss: 0.0010 - accuracy: 0.10 - ETA: 3s - loss: 0.0010 - accuracy: 0.10 - ETA: 3s - loss: 0.0010 - accuracy: 0.10 - ETA: 3s - loss: 0.0011 - accuracy: 0.10 - ETA: 3s - loss: 0.0010 - accuracy: 0.10 - ETA: 3s - loss: 0.0010 - accuracy: 0.09 - ETA: 3s - loss: 0.0010 - accuracy: 0.09 - ETA: 2s - loss: 0.0010 - accuracy: 0.10 - ETA: 2s - loss: 0.0010 - accuracy: 0.09 - ETA: 2s - loss: 0.0010 - accuracy: 0.09 - ETA: 2s - loss: 0.0010 - accuracy: 0.09 - ETA: 2s - loss: 9.9853e-04 - accuracy: 0.09 - ETA: 2s - loss: 9.9329e-04 - accuracy: 0.09 - ETA: 2s - loss: 9.9189e-04 - accuracy: 0.09 - ETA: 2s - loss: 9.8127e-04 - accuracy: 0.09 - ETA: 2s - loss: 9.8359e-04 - accuracy: 0.09 - ETA: 2s - loss: 9.7731e-04 - accuracy: 0.09 - ETA: 2s - loss: 9.7845e-04 - accuracy: 0.09 - ETA: 2s - loss: 9.7464e-04 - accuracy: 0.09 - ETA: 2s - loss: 9.6888e-04 - accuracy: 0.09 - ETA: 2s - loss: 9.6741e-04 - accuracy: 0.09 - ETA: 2s - loss: 9.6095e-04 - accuracy: 0.09 - ETA: 2s - loss: 9.6255e-04 - accuracy: 0.09 - ETA: 2s - loss: 9.5715e-04 - accuracy: 0.09 - ETA: 2s - loss: 9.4812e-04 - accuracy: 0.09 - ETA: 2s - loss: 9.4180e-04 - accuracy: 0.09 - ETA: 2s - loss: 9.3522e-04 - accuracy: 0.09 - ETA: 2s - loss: 9.3103e-04 - accuracy: 0.09 - ETA: 1s - loss: 9.2856e-04 - accuracy: 0.09 - ETA: 1s - loss: 9.2222e-04 - accuracy: 0.09 - ETA: 1s - loss: 9.1824e-04 - accuracy: 0.09 - ETA: 1s - loss: 9.1433e-04 - accuracy: 0.09 - ETA: 1s - loss: 9.1296e-04 - accuracy: 0.09 - ETA: 1s - loss: 9.0631e-04 - accuracy: 0.09 - ETA: 1s - loss: 9.0238e-04 - accuracy: 0.09 - ETA: 1s - loss: 9.0380e-04 - accuracy: 0.09 - ETA: 1s - loss: 8.9836e-04 - accuracy: 0.09 - ETA: 1s - loss: 8.9345e-04 - accuracy: 0.09 - ETA: 1s - loss: 8.9079e-04 - accuracy: 0.09 - ETA: 1s - loss: 8.8849e-04 - accuracy: 0.09 - ETA: 1s - loss: 8.8416e-04 - accuracy: 0.09 - ETA: 1s - loss: 8.8142e-04 - accuracy: 0.09 - ETA: 1s - loss: 8.8073e-04 - accuracy: 0.09 - ETA: 1s - loss: 8.7404e-04 - accuracy: 0.09 - ETA: 1s - loss: 8.6965e-04 - accuracy: 0.09 - ETA: 1s - loss: 8.8552e-04 - accuracy: 0.09 - ETA: 1s - loss: 8.8033e-04 - accuracy: 0.09 - ETA: 1s - loss: 8.7543e-04 - accuracy: 0.09 - ETA: 0s - loss: 8.7457e-04 - accuracy: 0.09 - ETA: 0s - loss: 8.7205e-04 - accuracy: 0.09 - ETA: 0s - loss: 8.6783e-04 - accuracy: 0.09 - ETA: 0s - loss: 8.6403e-04 - accuracy: 0.09 - ETA: 0s - loss: 8.7086e-04 - accuracy: 0.09 - ETA: 0s - loss: 8.6479e-04 - accuracy: 0.09 - ETA: 0s - loss: 8.6562e-04 - accuracy: 0.09 - ETA: 0s - loss: 8.6142e-04 - accuracy: 0.09 - ETA: 0s - loss: 8.5862e-04 - accuracy: 0.09 - ETA: 0s - loss: 8.5704e-04 - accuracy: 0.09 - ETA: 0s - loss: 8.5687e-04 - accuracy: 0.09 - ETA: 0s - loss: 8.5210e-04 - accuracy: 0.10 - ETA: 0s - loss: 8.5062e-04 - accuracy: 0.10 - ETA: 0s - loss: 8.4994e-04 - accuracy: 0.10 - ETA: 0s - loss: 8.5038e-04 - accuracy: 0.10 - ETA: 0s - loss: 8.4797e-04 - accuracy: 0.10 - ETA: 0s - loss: 8.4819e-04 - accuracy: 0.10 - ETA: 0s - loss: 8.4466e-04 - accuracy: 0.10 - ETA: 0s - loss: 8.4073e-04 - accuracy: 0.10 - ETA: 0s - loss: 8.3805e-04 - accuracy: 0.10 - 6s 3ms/step - loss: 8.3750e-04 - accuracy: 0.1001 - val_loss: 7.0476e-04 - val_accuracy: 0.0997\n",
      "Epoch 4/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 2.1535e-04 - accuracy: 0.09 - ETA: 4s - loss: 5.6702e-04 - accuracy: 0.10 - ETA: 4s - loss: 5.6774e-04 - accuracy: 0.09 - ETA: 4s - loss: 5.3909e-04 - accuracy: 0.09 - ETA: 4s - loss: 5.1043e-04 - accuracy: 0.09 - ETA: 4s - loss: 5.5838e-04 - accuracy: 0.09 - ETA: 4s - loss: 6.0814e-04 - accuracy: 0.09 - ETA: 4s - loss: 6.2372e-04 - accuracy: 0.09 - ETA: 4s - loss: 6.1424e-04 - accuracy: 0.09 - ETA: 4s - loss: 5.9912e-04 - accuracy: 0.09 - ETA: 4s - loss: 5.9370e-04 - accuracy: 0.09 - ETA: 4s - loss: 5.9373e-04 - accuracy: 0.10 - ETA: 4s - loss: 5.7858e-04 - accuracy: 0.10 - ETA: 4s - loss: 5.9120e-04 - accuracy: 0.10 - ETA: 4s - loss: 6.1058e-04 - accuracy: 0.10 - ETA: 4s - loss: 5.9651e-04 - accuracy: 0.10 - ETA: 3s - loss: 5.8657e-04 - accuracy: 0.09 - ETA: 3s - loss: 5.7987e-04 - accuracy: 0.09 - ETA: 3s - loss: 5.8171e-04 - accuracy: 0.09 - ETA: 3s - loss: 5.7903e-04 - accuracy: 0.09 - ETA: 3s - loss: 5.8550e-04 - accuracy: 0.09 - ETA: 3s - loss: 5.9071e-04 - accuracy: 0.09 - ETA: 3s - loss: 5.8747e-04 - accuracy: 0.09 - ETA: 3s - loss: 6.5501e-04 - accuracy: 0.09 - ETA: 3s - loss: 6.5811e-04 - accuracy: 0.09 - ETA: 3s - loss: 6.5086e-04 - accuracy: 0.09 - ETA: 3s - loss: 6.4772e-04 - accuracy: 0.09 - ETA: 3s - loss: 6.4328e-04 - accuracy: 0.09 - ETA: 3s - loss: 6.4118e-04 - accuracy: 0.09 - ETA: 3s - loss: 6.3134e-04 - accuracy: 0.09 - ETA: 3s - loss: 6.2351e-04 - accuracy: 0.09 - ETA: 3s - loss: 6.1862e-04 - accuracy: 0.09 - ETA: 3s - loss: 6.2015e-04 - accuracy: 0.09 - ETA: 3s - loss: 6.2063e-04 - accuracy: 0.09 - ETA: 3s - loss: 6.1944e-04 - accuracy: 0.09 - ETA: 3s - loss: 6.1384e-04 - accuracy: 0.09 - ETA: 3s - loss: 6.1178e-04 - accuracy: 0.09 - ETA: 2s - loss: 6.0748e-04 - accuracy: 0.09 - ETA: 2s - loss: 6.0835e-04 - accuracy: 0.09 - ETA: 2s - loss: 6.1178e-04 - accuracy: 0.09 - ETA: 2s - loss: 6.0751e-04 - accuracy: 0.09 - ETA: 2s - loss: 6.0154e-04 - accuracy: 0.09 - ETA: 2s - loss: 5.9638e-04 - accuracy: 0.09 - ETA: 2s - loss: 6.0046e-04 - accuracy: 0.09 - ETA: 2s - loss: 6.0056e-04 - accuracy: 0.09 - ETA: 2s - loss: 5.9436e-04 - accuracy: 0.09 - ETA: 2s - loss: 5.9061e-04 - accuracy: 0.09 - ETA: 2s - loss: 5.9338e-04 - accuracy: 0.09 - ETA: 2s - loss: 5.9395e-04 - accuracy: 0.09 - ETA: 2s - loss: 5.8973e-04 - accuracy: 0.09 - ETA: 2s - loss: 5.8833e-04 - accuracy: 0.09 - ETA: 2s - loss: 5.9108e-04 - accuracy: 0.09 - ETA: 2s - loss: 5.8443e-04 - accuracy: 0.09 - ETA: 2s - loss: 5.8282e-04 - accuracy: 0.09 - ETA: 2s - loss: 5.7965e-04 - accuracy: 0.09 - ETA: 2s - loss: 5.7913e-04 - accuracy: 0.09 - ETA: 2s - loss: 5.7482e-04 - accuracy: 0.09 - ETA: 2s - loss: 5.7441e-04 - accuracy: 0.10 - ETA: 1s - loss: 5.8008e-04 - accuracy: 0.10 - ETA: 1s - loss: 5.9489e-04 - accuracy: 0.09 - ETA: 1s - loss: 5.9414e-04 - accuracy: 0.09 - ETA: 1s - loss: 6.1373e-04 - accuracy: 0.10 - ETA: 1s - loss: 6.1117e-04 - accuracy: 0.10 - ETA: 1s - loss: 6.0870e-04 - accuracy: 0.10 - ETA: 1s - loss: 6.0744e-04 - accuracy: 0.10 - ETA: 1s - loss: 6.1032e-04 - accuracy: 0.10 - ETA: 1s - loss: 6.0732e-04 - accuracy: 0.10 - ETA: 1s - loss: 6.0836e-04 - accuracy: 0.10 - ETA: 1s - loss: 6.0655e-04 - accuracy: 0.10 - ETA: 1s - loss: 6.0324e-04 - accuracy: 0.10 - ETA: 1s - loss: 5.9919e-04 - accuracy: 0.10 - ETA: 1s - loss: 5.9617e-04 - accuracy: 0.10 - ETA: 1s - loss: 5.9806e-04 - accuracy: 0.10 - ETA: 1s - loss: 5.9329e-04 - accuracy: 0.10 - ETA: 1s - loss: 5.9029e-04 - accuracy: 0.10 - ETA: 1s - loss: 5.8911e-04 - accuracy: 0.10 - ETA: 1s - loss: 5.9009e-04 - accuracy: 0.10 - ETA: 0s - loss: 5.8857e-04 - accuracy: 0.10 - ETA: 0s - loss: 5.8709e-04 - accuracy: 0.10 - ETA: 0s - loss: 5.8603e-04 - accuracy: 0.10 - ETA: 0s - loss: 5.8428e-04 - accuracy: 0.10 - ETA: 0s - loss: 5.8464e-04 - accuracy: 0.10 - ETA: 0s - loss: 6.0291e-04 - accuracy: 0.10 - ETA: 0s - loss: 5.9989e-04 - accuracy: 0.10 - ETA: 0s - loss: 5.9617e-04 - accuracy: 0.10 - ETA: 0s - loss: 5.9605e-04 - accuracy: 0.10 - ETA: 0s - loss: 5.9391e-04 - accuracy: 0.10 - ETA: 0s - loss: 5.9179e-04 - accuracy: 0.10 - ETA: 0s - loss: 5.9268e-04 - accuracy: 0.10 - ETA: 0s - loss: 5.8990e-04 - accuracy: 0.10 - ETA: 0s - loss: 5.8875e-04 - accuracy: 0.10 - ETA: 0s - loss: 5.9189e-04 - accuracy: 0.10 - ETA: 0s - loss: 5.8961e-04 - accuracy: 0.10 - ETA: 0s - loss: 5.8689e-04 - accuracy: 0.10 - ETA: 0s - loss: 5.8585e-04 - accuracy: 0.10 - ETA: 0s - loss: 5.8349e-04 - accuracy: 0.10 - ETA: 0s - loss: 5.8261e-04 - accuracy: 0.10 - 6s 3ms/step - loss: 5.8250e-04 - accuracy: 0.1001 - val_loss: 5.2745e-04 - val_accuracy: 0.0997\n",
      "Epoch 5/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 1.8835e-04 - accuracy: 0.15 - ETA: 4s - loss: 2.4313e-04 - accuracy: 0.10 - ETA: 4s - loss: 3.8697e-04 - accuracy: 0.10 - ETA: 4s - loss: 3.8407e-04 - accuracy: 0.10 - ETA: 4s - loss: 3.7377e-04 - accuracy: 0.11 - ETA: 4s - loss: 3.8501e-04 - accuracy: 0.10 - ETA: 4s - loss: 3.8748e-04 - accuracy: 0.10 - ETA: 4s - loss: 4.1916e-04 - accuracy: 0.10 - ETA: 4s - loss: 4.2058e-04 - accuracy: 0.10 - ETA: 4s - loss: 4.1405e-04 - accuracy: 0.10 - ETA: 4s - loss: 4.0354e-04 - accuracy: 0.10 - ETA: 4s - loss: 4.1005e-04 - accuracy: 0.10 - ETA: 4s - loss: 4.1045e-04 - accuracy: 0.10 - ETA: 4s - loss: 4.1841e-04 - accuracy: 0.10 - ETA: 4s - loss: 4.1279e-04 - accuracy: 0.10 - ETA: 4s - loss: 4.2704e-04 - accuracy: 0.10 - ETA: 4s - loss: 4.2896e-04 - accuracy: 0.10 - ETA: 3s - loss: 4.2403e-04 - accuracy: 0.10 - ETA: 3s - loss: 4.2332e-04 - accuracy: 0.10 - ETA: 3s - loss: 4.1945e-04 - accuracy: 0.10 - ETA: 3s - loss: 4.1734e-04 - accuracy: 0.10 - ETA: 3s - loss: 4.1326e-04 - accuracy: 0.09 - ETA: 3s - loss: 4.1045e-04 - accuracy: 0.09 - ETA: 3s - loss: 4.8294e-04 - accuracy: 0.09 - ETA: 3s - loss: 4.7736e-04 - accuracy: 0.09 - ETA: 3s - loss: 4.7495e-04 - accuracy: 0.09 - ETA: 3s - loss: 4.7136e-04 - accuracy: 0.09 - ETA: 3s - loss: 4.6592e-04 - accuracy: 0.09 - ETA: 3s - loss: 4.6604e-04 - accuracy: 0.10 - ETA: 3s - loss: 4.6218e-04 - accuracy: 0.09 - ETA: 3s - loss: 4.5873e-04 - accuracy: 0.09 - ETA: 3s - loss: 4.5554e-04 - accuracy: 0.09 - ETA: 3s - loss: 4.5522e-04 - accuracy: 0.10 - ETA: 3s - loss: 4.5450e-04 - accuracy: 0.10 - ETA: 3s - loss: 4.5328e-04 - accuracy: 0.09 - ETA: 2s - loss: 4.5086e-04 - accuracy: 0.10 - ETA: 2s - loss: 4.7977e-04 - accuracy: 0.10 - ETA: 2s - loss: 4.7747e-04 - accuracy: 0.09 - ETA: 2s - loss: 4.7480e-04 - accuracy: 0.09 - ETA: 2s - loss: 4.7289e-04 - accuracy: 0.09 - ETA: 2s - loss: 4.7054e-04 - accuracy: 0.09 - ETA: 2s - loss: 4.6996e-04 - accuracy: 0.09 - ETA: 2s - loss: 5.1155e-04 - accuracy: 0.09 - ETA: 2s - loss: 5.0915e-04 - accuracy: 0.09 - ETA: 2s - loss: 5.0771e-04 - accuracy: 0.09 - ETA: 2s - loss: 5.0515e-04 - accuracy: 0.09 - ETA: 2s - loss: 5.0094e-04 - accuracy: 0.09 - ETA: 2s - loss: 4.9875e-04 - accuracy: 0.09 - ETA: 2s - loss: 4.9527e-04 - accuracy: 0.09 - ETA: 2s - loss: 4.9302e-04 - accuracy: 0.09 - ETA: 2s - loss: 4.9313e-04 - accuracy: 0.09 - ETA: 2s - loss: 4.8922e-04 - accuracy: 0.09 - ETA: 2s - loss: 4.8734e-04 - accuracy: 0.09 - ETA: 2s - loss: 4.8676e-04 - accuracy: 0.09 - ETA: 2s - loss: 4.8320e-04 - accuracy: 0.09 - ETA: 1s - loss: 4.8083e-04 - accuracy: 0.09 - ETA: 1s - loss: 4.7861e-04 - accuracy: 0.09 - ETA: 1s - loss: 4.7536e-04 - accuracy: 0.09 - ETA: 1s - loss: 4.7344e-04 - accuracy: 0.09 - ETA: 1s - loss: 4.7387e-04 - accuracy: 0.09 - ETA: 1s - loss: 4.7084e-04 - accuracy: 0.09 - ETA: 1s - loss: 4.6925e-04 - accuracy: 0.09 - ETA: 1s - loss: 4.6935e-04 - accuracy: 0.09 - ETA: 1s - loss: 4.6817e-04 - accuracy: 0.09 - ETA: 1s - loss: 4.6760e-04 - accuracy: 0.09 - ETA: 1s - loss: 4.6562e-04 - accuracy: 0.09 - ETA: 1s - loss: 4.6319e-04 - accuracy: 0.09 - ETA: 1s - loss: 4.6067e-04 - accuracy: 0.09 - ETA: 1s - loss: 4.6141e-04 - accuracy: 0.09 - ETA: 1s - loss: 4.5876e-04 - accuracy: 0.09 - ETA: 1s - loss: 4.6312e-04 - accuracy: 0.09 - ETA: 1s - loss: 4.6160e-04 - accuracy: 0.09 - ETA: 1s - loss: 4.5961e-04 - accuracy: 0.09 - ETA: 1s - loss: 4.5913e-04 - accuracy: 0.09 - ETA: 1s - loss: 4.5830e-04 - accuracy: 0.09 - ETA: 0s - loss: 4.5556e-04 - accuracy: 0.09 - ETA: 0s - loss: 4.5251e-04 - accuracy: 0.09 - ETA: 0s - loss: 4.5229e-04 - accuracy: 0.09 - ETA: 0s - loss: 4.5091e-04 - accuracy: 0.09 - ETA: 0s - loss: 4.4972e-04 - accuracy: 0.09 - ETA: 0s - loss: 4.4877e-04 - accuracy: 0.09 - ETA: 0s - loss: 4.4987e-04 - accuracy: 0.09 - ETA: 0s - loss: 4.4906e-04 - accuracy: 0.09 - ETA: 0s - loss: 4.4851e-04 - accuracy: 0.09 - ETA: 0s - loss: 4.4607e-04 - accuracy: 0.09 - ETA: 0s - loss: 4.4521e-04 - accuracy: 0.10 - ETA: 0s - loss: 4.4368e-04 - accuracy: 0.10 - ETA: 0s - loss: 4.4276e-04 - accuracy: 0.10 - ETA: 0s - loss: 4.4544e-04 - accuracy: 0.10 - ETA: 0s - loss: 4.4425e-04 - accuracy: 0.10 - ETA: 0s - loss: 4.4230e-04 - accuracy: 0.10 - ETA: 0s - loss: 4.5612e-04 - accuracy: 0.10 - ETA: 0s - loss: 4.5300e-04 - accuracy: 0.10 - ETA: 0s - loss: 4.5237e-04 - accuracy: 0.10 - ETA: 0s - loss: 4.4976e-04 - accuracy: 0.10 - 5s 3ms/step - loss: 4.4949e-04 - accuracy: 0.1001 - val_loss: 4.2471e-04 - val_accuracy: 0.0997\n",
      "Epoch 1/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 2.1422 - accuracy: 0.03 - ETA: 4s - loss: 1.9337 - accuracy: 0.07 - ETA: 4s - loss: 1.6814 - accuracy: 0.09 - ETA: 4s - loss: 1.3821 - accuracy: 0.09 - ETA: 4s - loss: 1.1320 - accuracy: 0.09 - ETA: 4s - loss: 0.9464 - accuracy: 0.09 - ETA: 4s - loss: 0.8104 - accuracy: 0.09 - ETA: 4s - loss: 0.7043 - accuracy: 0.09 - ETA: 4s - loss: 0.6225 - accuracy: 0.09 - ETA: 4s - loss: 0.5580 - accuracy: 0.09 - ETA: 4s - loss: 0.5068 - accuracy: 0.09 - ETA: 4s - loss: 0.4633 - accuracy: 0.09 - ETA: 4s - loss: 0.4278 - accuracy: 0.09 - ETA: 4s - loss: 0.3960 - accuracy: 0.09 - ETA: 4s - loss: 0.3699 - accuracy: 0.09 - ETA: 3s - loss: 0.3461 - accuracy: 0.09 - ETA: 3s - loss: 0.3252 - accuracy: 0.09 - ETA: 3s - loss: 0.3075 - accuracy: 0.09 - ETA: 3s - loss: 0.2909 - accuracy: 0.09 - ETA: 3s - loss: 0.2766 - accuracy: 0.09 - ETA: 3s - loss: 0.2631 - accuracy: 0.09 - ETA: 3s - loss: 0.2513 - accuracy: 0.09 - ETA: 3s - loss: 0.2406 - accuracy: 0.09 - ETA: 3s - loss: 0.2305 - accuracy: 0.09 - ETA: 3s - loss: 0.2215 - accuracy: 0.09 - ETA: 3s - loss: 0.2129 - accuracy: 0.09 - ETA: 3s - loss: 0.2052 - accuracy: 0.09 - ETA: 3s - loss: 0.1981 - accuracy: 0.09 - ETA: 3s - loss: 0.1913 - accuracy: 0.09 - ETA: 3s - loss: 0.1851 - accuracy: 0.09 - ETA: 3s - loss: 0.1790 - accuracy: 0.09 - ETA: 3s - loss: 0.1736 - accuracy: 0.09 - ETA: 3s - loss: 0.1682 - accuracy: 0.09 - ETA: 3s - loss: 0.1634 - accuracy: 0.09 - ETA: 2s - loss: 0.1585 - accuracy: 0.09 - ETA: 2s - loss: 0.1541 - accuracy: 0.09 - ETA: 2s - loss: 0.1500 - accuracy: 0.09 - ETA: 2s - loss: 0.1460 - accuracy: 0.09 - ETA: 2s - loss: 0.1424 - accuracy: 0.09 - ETA: 2s - loss: 0.1388 - accuracy: 0.09 - ETA: 2s - loss: 0.1354 - accuracy: 0.09 - ETA: 2s - loss: 0.1322 - accuracy: 0.09 - ETA: 2s - loss: 0.1291 - accuracy: 0.09 - ETA: 2s - loss: 0.1261 - accuracy: 0.09 - ETA: 2s - loss: 0.1234 - accuracy: 0.09 - ETA: 2s - loss: 0.1209 - accuracy: 0.10 - ETA: 2s - loss: 0.1184 - accuracy: 0.09 - ETA: 2s - loss: 0.1160 - accuracy: 0.09 - ETA: 2s - loss: 0.1137 - accuracy: 0.09 - ETA: 2s - loss: 0.1115 - accuracy: 0.09 - ETA: 2s - loss: 0.1093 - accuracy: 0.09 - ETA: 2s - loss: 0.1073 - accuracy: 0.09 - ETA: 2s - loss: 0.1055 - accuracy: 0.09 - ETA: 2s - loss: 0.1036 - accuracy: 0.10 - ETA: 1s - loss: 0.1018 - accuracy: 0.10 - ETA: 1s - loss: 0.1000 - accuracy: 0.10 - ETA: 1s - loss: 0.0984 - accuracy: 0.10 - ETA: 1s - loss: 0.0968 - accuracy: 0.10 - ETA: 1s - loss: 0.0952 - accuracy: 0.10 - ETA: 1s - loss: 0.0937 - accuracy: 0.10 - ETA: 1s - loss: 0.0922 - accuracy: 0.09 - ETA: 1s - loss: 0.0908 - accuracy: 0.09 - ETA: 1s - loss: 0.0894 - accuracy: 0.10 - ETA: 1s - loss: 0.0880 - accuracy: 0.09 - ETA: 1s - loss: 0.0868 - accuracy: 0.09 - ETA: 1s - loss: 0.0856 - accuracy: 0.09 - ETA: 1s - loss: 0.0844 - accuracy: 0.09 - ETA: 1s - loss: 0.0832 - accuracy: 0.09 - ETA: 1s - loss: 0.0821 - accuracy: 0.09 - ETA: 1s - loss: 0.0809 - accuracy: 0.10 - ETA: 1s - loss: 0.0798 - accuracy: 0.09 - ETA: 1s - loss: 0.0787 - accuracy: 0.10 - ETA: 1s - loss: 0.0777 - accuracy: 0.10 - ETA: 1s - loss: 0.0767 - accuracy: 0.09 - ETA: 1s - loss: 0.0757 - accuracy: 0.09 - ETA: 0s - loss: 0.0747 - accuracy: 0.09 - ETA: 0s - loss: 0.0737 - accuracy: 0.09 - ETA: 0s - loss: 0.0728 - accuracy: 0.10 - ETA: 0s - loss: 0.0719 - accuracy: 0.09 - ETA: 0s - loss: 0.0710 - accuracy: 0.09 - ETA: 0s - loss: 0.0701 - accuracy: 0.09 - ETA: 0s - loss: 0.0693 - accuracy: 0.09 - ETA: 0s - loss: 0.0685 - accuracy: 0.09 - ETA: 0s - loss: 0.0677 - accuracy: 0.09 - ETA: 0s - loss: 0.0669 - accuracy: 0.09 - ETA: 0s - loss: 0.0661 - accuracy: 0.09 - ETA: 0s - loss: 0.0654 - accuracy: 0.09 - ETA: 0s - loss: 0.0647 - accuracy: 0.09 - ETA: 0s - loss: 0.0640 - accuracy: 0.09 - ETA: 0s - loss: 0.0633 - accuracy: 0.09 - ETA: 0s - loss: 0.0626 - accuracy: 0.09 - ETA: 0s - loss: 0.0620 - accuracy: 0.09 - ETA: 0s - loss: 0.0614 - accuracy: 0.09 - ETA: 0s - loss: 0.0607 - accuracy: 0.09 - ETA: 0s - loss: 0.0601 - accuracy: 0.09 - 6s 3ms/step - loss: 0.0600 - accuracy: 0.0999 - val_loss: 0.0020 - val_accuracy: 0.0997\n",
      "Epoch 2/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 0.0014 - accuracy: 0.06 - ETA: 4s - loss: 0.0017 - accuracy: 0.09 - ETA: 4s - loss: 0.0024 - accuracy: 0.09 - ETA: 4s - loss: 0.0024 - accuracy: 0.09 - ETA: 4s - loss: 0.0022 - accuracy: 0.09 - ETA: 4s - loss: 0.0020 - accuracy: 0.09 - ETA: 4s - loss: 0.0020 - accuracy: 0.10 - ETA: 4s - loss: 0.0020 - accuracy: 0.10 - ETA: 4s - loss: 0.0019 - accuracy: 0.09 - ETA: 4s - loss: 0.0019 - accuracy: 0.09 - ETA: 4s - loss: 0.0019 - accuracy: 0.09 - ETA: 4s - loss: 0.0019 - accuracy: 0.09 - ETA: 4s - loss: 0.0018 - accuracy: 0.09 - ETA: 4s - loss: 0.0018 - accuracy: 0.09 - ETA: 4s - loss: 0.0018 - accuracy: 0.09 - ETA: 4s - loss: 0.0018 - accuracy: 0.09 - ETA: 3s - loss: 0.0018 - accuracy: 0.09 - ETA: 3s - loss: 0.0017 - accuracy: 0.09 - ETA: 3s - loss: 0.0017 - accuracy: 0.09 - ETA: 3s - loss: 0.0017 - accuracy: 0.09 - ETA: 3s - loss: 0.0017 - accuracy: 0.09 - ETA: 3s - loss: 0.0017 - accuracy: 0.09 - ETA: 3s - loss: 0.0017 - accuracy: 0.09 - ETA: 3s - loss: 0.0017 - accuracy: 0.09 - ETA: 3s - loss: 0.0016 - accuracy: 0.09 - ETA: 3s - loss: 0.0016 - accuracy: 0.09 - ETA: 3s - loss: 0.0016 - accuracy: 0.10 - ETA: 3s - loss: 0.0016 - accuracy: 0.10 - ETA: 3s - loss: 0.0016 - accuracy: 0.10 - ETA: 3s - loss: 0.0016 - accuracy: 0.10 - ETA: 3s - loss: 0.0016 - accuracy: 0.10 - ETA: 3s - loss: 0.0016 - accuracy: 0.09 - ETA: 3s - loss: 0.0016 - accuracy: 0.09 - ETA: 3s - loss: 0.0016 - accuracy: 0.09 - ETA: 2s - loss: 0.0016 - accuracy: 0.10 - ETA: 2s - loss: 0.0016 - accuracy: 0.10 - ETA: 2s - loss: 0.0016 - accuracy: 0.10 - ETA: 2s - loss: 0.0016 - accuracy: 0.10 - ETA: 2s - loss: 0.0015 - accuracy: 0.10 - ETA: 2s - loss: 0.0015 - accuracy: 0.10 - ETA: 2s - loss: 0.0015 - accuracy: 0.10 - ETA: 2s - loss: 0.0015 - accuracy: 0.10 - ETA: 2s - loss: 0.0015 - accuracy: 0.09 - ETA: 2s - loss: 0.0015 - accuracy: 0.09 - ETA: 2s - loss: 0.0015 - accuracy: 0.09 - ETA: 2s - loss: 0.0015 - accuracy: 0.09 - ETA: 2s - loss: 0.0015 - accuracy: 0.09 - ETA: 2s - loss: 0.0015 - accuracy: 0.09 - ETA: 2s - loss: 0.0015 - accuracy: 0.09 - ETA: 2s - loss: 0.0015 - accuracy: 0.09 - ETA: 2s - loss: 0.0015 - accuracy: 0.09 - ETA: 2s - loss: 0.0015 - accuracy: 0.09 - ETA: 2s - loss: 0.0015 - accuracy: 0.09 - ETA: 2s - loss: 0.0015 - accuracy: 0.09 - ETA: 1s - loss: 0.0015 - accuracy: 0.09 - ETA: 1s - loss: 0.0015 - accuracy: 0.09 - ETA: 1s - loss: 0.0015 - accuracy: 0.09 - ETA: 1s - loss: 0.0015 - accuracy: 0.09 - ETA: 1s - loss: 0.0014 - accuracy: 0.09 - ETA: 1s - loss: 0.0014 - accuracy: 0.09 - ETA: 1s - loss: 0.0014 - accuracy: 0.09 - ETA: 1s - loss: 0.0014 - accuracy: 0.09 - ETA: 1s - loss: 0.0015 - accuracy: 0.09 - ETA: 1s - loss: 0.0014 - accuracy: 0.09 - ETA: 1s - loss: 0.0014 - accuracy: 0.09 - ETA: 1s - loss: 0.0014 - accuracy: 0.09 - ETA: 1s - loss: 0.0014 - accuracy: 0.09 - ETA: 1s - loss: 0.0014 - accuracy: 0.09 - ETA: 1s - loss: 0.0014 - accuracy: 0.09 - ETA: 1s - loss: 0.0014 - accuracy: 0.09 - ETA: 1s - loss: 0.0014 - accuracy: 0.09 - ETA: 1s - loss: 0.0014 - accuracy: 0.09 - ETA: 1s - loss: 0.0014 - accuracy: 0.09 - ETA: 1s - loss: 0.0014 - accuracy: 0.09 - ETA: 0s - loss: 0.0014 - accuracy: 0.09 - ETA: 0s - loss: 0.0014 - accuracy: 0.09 - ETA: 0s - loss: 0.0014 - accuracy: 0.09 - ETA: 0s - loss: 0.0014 - accuracy: 0.09 - ETA: 0s - loss: 0.0014 - accuracy: 0.09 - ETA: 0s - loss: 0.0014 - accuracy: 0.09 - ETA: 0s - loss: 0.0014 - accuracy: 0.09 - ETA: 0s - loss: 0.0014 - accuracy: 0.09 - ETA: 0s - loss: 0.0013 - accuracy: 0.09 - ETA: 0s - loss: 0.0013 - accuracy: 0.09 - ETA: 0s - loss: 0.0013 - accuracy: 0.09 - ETA: 0s - loss: 0.0013 - accuracy: 0.09 - ETA: 0s - loss: 0.0013 - accuracy: 0.09 - ETA: 0s - loss: 0.0013 - accuracy: 0.09 - ETA: 0s - loss: 0.0013 - accuracy: 0.10 - ETA: 0s - loss: 0.0013 - accuracy: 0.09 - ETA: 0s - loss: 0.0013 - accuracy: 0.09 - ETA: 0s - loss: 0.0013 - accuracy: 0.09 - ETA: 0s - loss: 0.0013 - accuracy: 0.10 - ETA: 0s - loss: 0.0013 - accuracy: 0.10 - 5s 2ms/step - loss: 0.0013 - accuracy: 0.1001 - val_loss: 9.5355e-04 - val_accuracy: 0.0997\n",
      "Epoch 3/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 1.5792e-04 - accuracy: 0.12 - ETA: 4s - loss: 7.3967e-04 - accuracy: 0.09 - ETA: 4s - loss: 7.6775e-04 - accuracy: 0.09 - ETA: 4s - loss: 7.7601e-04 - accuracy: 0.09 - ETA: 4s - loss: 8.2811e-04 - accuracy: 0.09 - ETA: 4s - loss: 8.5404e-04 - accuracy: 0.09 - ETA: 4s - loss: 8.2769e-04 - accuracy: 0.09 - ETA: 4s - loss: 7.8507e-04 - accuracy: 0.09 - ETA: 4s - loss: 7.9540e-04 - accuracy: 0.09 - ETA: 4s - loss: 8.1378e-04 - accuracy: 0.09 - ETA: 4s - loss: 8.1952e-04 - accuracy: 0.09 - ETA: 4s - loss: 8.4057e-04 - accuracy: 0.09 - ETA: 4s - loss: 8.4212e-04 - accuracy: 0.09 - ETA: 4s - loss: 8.2937e-04 - accuracy: 0.09 - ETA: 3s - loss: 8.2777e-04 - accuracy: 0.09 - ETA: 3s - loss: 8.2868e-04 - accuracy: 0.09 - ETA: 3s - loss: 8.1818e-04 - accuracy: 0.09 - ETA: 3s - loss: 8.1490e-04 - accuracy: 0.09 - ETA: 3s - loss: 8.2800e-04 - accuracy: 0.09 - ETA: 3s - loss: 8.1512e-04 - accuracy: 0.09 - ETA: 3s - loss: 8.0850e-04 - accuracy: 0.09 - ETA: 3s - loss: 8.9083e-04 - accuracy: 0.09 - ETA: 3s - loss: 8.8355e-04 - accuracy: 0.09 - ETA: 3s - loss: 8.7362e-04 - accuracy: 0.09 - ETA: 3s - loss: 8.6491e-04 - accuracy: 0.09 - ETA: 3s - loss: 8.5549e-04 - accuracy: 0.09 - ETA: 3s - loss: 8.4820e-04 - accuracy: 0.09 - ETA: 3s - loss: 8.4194e-04 - accuracy: 0.09 - ETA: 3s - loss: 8.3698e-04 - accuracy: 0.09 - ETA: 3s - loss: 8.3633e-04 - accuracy: 0.09 - ETA: 3s - loss: 8.3089e-04 - accuracy: 0.09 - ETA: 3s - loss: 8.3549e-04 - accuracy: 0.09 - ETA: 3s - loss: 8.3396e-04 - accuracy: 0.09 - ETA: 2s - loss: 8.2989e-04 - accuracy: 0.09 - ETA: 2s - loss: 8.1853e-04 - accuracy: 0.09 - ETA: 2s - loss: 8.1500e-04 - accuracy: 0.09 - ETA: 2s - loss: 8.1309e-04 - accuracy: 0.09 - ETA: 2s - loss: 8.4695e-04 - accuracy: 0.09 - ETA: 2s - loss: 8.4781e-04 - accuracy: 0.09 - ETA: 2s - loss: 8.5232e-04 - accuracy: 0.09 - ETA: 2s - loss: 8.5254e-04 - accuracy: 0.09 - ETA: 2s - loss: 8.4635e-04 - accuracy: 0.09 - ETA: 2s - loss: 8.4106e-04 - accuracy: 0.09 - ETA: 2s - loss: 8.4424e-04 - accuracy: 0.09 - ETA: 2s - loss: 8.4381e-04 - accuracy: 0.09 - ETA: 2s - loss: 8.3409e-04 - accuracy: 0.09 - ETA: 2s - loss: 8.3265e-04 - accuracy: 0.09 - ETA: 2s - loss: 8.2673e-04 - accuracy: 0.09 - ETA: 2s - loss: 8.3513e-04 - accuracy: 0.09 - ETA: 2s - loss: 8.3178e-04 - accuracy: 0.10 - ETA: 2s - loss: 8.2814e-04 - accuracy: 0.10 - ETA: 2s - loss: 8.2351e-04 - accuracy: 0.10 - ETA: 2s - loss: 8.5111e-04 - accuracy: 0.10 - ETA: 2s - loss: 8.4668e-04 - accuracy: 0.10 - ETA: 1s - loss: 8.4021e-04 - accuracy: 0.10 - ETA: 1s - loss: 8.3536e-04 - accuracy: 0.10 - ETA: 1s - loss: 8.3315e-04 - accuracy: 0.10 - ETA: 1s - loss: 8.2697e-04 - accuracy: 0.10 - ETA: 1s - loss: 8.2058e-04 - accuracy: 0.10 - ETA: 1s - loss: 8.1919e-04 - accuracy: 0.10 - ETA: 1s - loss: 8.1341e-04 - accuracy: 0.10 - ETA: 1s - loss: 8.0943e-04 - accuracy: 0.10 - ETA: 1s - loss: 8.0431e-04 - accuracy: 0.10 - ETA: 1s - loss: 8.0140e-04 - accuracy: 0.10 - ETA: 1s - loss: 7.9589e-04 - accuracy: 0.10 - ETA: 1s - loss: 7.9344e-04 - accuracy: 0.10 - ETA: 1s - loss: 7.8768e-04 - accuracy: 0.10 - ETA: 1s - loss: 7.8330e-04 - accuracy: 0.10 - ETA: 1s - loss: 7.7898e-04 - accuracy: 0.10 - ETA: 1s - loss: 7.7417e-04 - accuracy: 0.10 - ETA: 1s - loss: 7.7049e-04 - accuracy: 0.10 - ETA: 1s - loss: 7.7024e-04 - accuracy: 0.10 - ETA: 1s - loss: 7.6613e-04 - accuracy: 0.10 - ETA: 1s - loss: 7.6109e-04 - accuracy: 0.10 - ETA: 1s - loss: 7.5998e-04 - accuracy: 0.10 - ETA: 0s - loss: 7.6071e-04 - accuracy: 0.10 - ETA: 0s - loss: 7.5845e-04 - accuracy: 0.10 - ETA: 0s - loss: 7.5851e-04 - accuracy: 0.10 - ETA: 0s - loss: 7.7841e-04 - accuracy: 0.10 - ETA: 0s - loss: 7.7724e-04 - accuracy: 0.10 - ETA: 0s - loss: 7.7729e-04 - accuracy: 0.10 - ETA: 0s - loss: 7.7539e-04 - accuracy: 0.10 - ETA: 0s - loss: 7.7094e-04 - accuracy: 0.10 - ETA: 0s - loss: 7.7115e-04 - accuracy: 0.10 - ETA: 0s - loss: 7.6799e-04 - accuracy: 0.10 - ETA: 0s - loss: 7.6459e-04 - accuracy: 0.10 - ETA: 0s - loss: 7.6218e-04 - accuracy: 0.10 - ETA: 0s - loss: 7.5970e-04 - accuracy: 0.10 - ETA: 0s - loss: 7.5739e-04 - accuracy: 0.10 - ETA: 0s - loss: 7.5711e-04 - accuracy: 0.10 - ETA: 0s - loss: 7.5657e-04 - accuracy: 0.10 - ETA: 0s - loss: 7.5430e-04 - accuracy: 0.10 - ETA: 0s - loss: 7.5157e-04 - accuracy: 0.10 - ETA: 0s - loss: 7.4752e-04 - accuracy: 0.10 - ETA: 0s - loss: 7.4669e-04 - accuracy: 0.10 - 5s 2ms/step - loss: 7.4584e-04 - accuracy: 0.1001 - val_loss: 6.3616e-04 - val_accuracy: 0.0997\n",
      "Epoch 4/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 2.7165e-04 - accuracy: 0.06 - ETA: 4s - loss: 4.2212e-04 - accuracy: 0.12 - ETA: 4s - loss: 3.7751e-04 - accuracy: 0.11 - ETA: 4s - loss: 3.5331e-04 - accuracy: 0.11 - ETA: 4s - loss: 4.2320e-04 - accuracy: 0.10 - ETA: 4s - loss: 4.0030e-04 - accuracy: 0.10 - ETA: 4s - loss: 3.9883e-04 - accuracy: 0.10 - ETA: 4s - loss: 4.0271e-04 - accuracy: 0.10 - ETA: 4s - loss: 4.0818e-04 - accuracy: 0.10 - ETA: 4s - loss: 4.2082e-04 - accuracy: 0.10 - ETA: 4s - loss: 4.3399e-04 - accuracy: 0.10 - ETA: 4s - loss: 4.5752e-04 - accuracy: 0.10 - ETA: 4s - loss: 4.5048e-04 - accuracy: 0.10 - ETA: 3s - loss: 5.3836e-04 - accuracy: 0.10 - ETA: 3s - loss: 5.2476e-04 - accuracy: 0.10 - ETA: 3s - loss: 5.2326e-04 - accuracy: 0.10 - ETA: 3s - loss: 5.1708e-04 - accuracy: 0.10 - ETA: 3s - loss: 5.1780e-04 - accuracy: 0.10 - ETA: 3s - loss: 5.1528e-04 - accuracy: 0.10 - ETA: 3s - loss: 5.1063e-04 - accuracy: 0.10 - ETA: 3s - loss: 4.9844e-04 - accuracy: 0.09 - ETA: 3s - loss: 5.0305e-04 - accuracy: 0.10 - ETA: 3s - loss: 5.0273e-04 - accuracy: 0.09 - ETA: 3s - loss: 5.0497e-04 - accuracy: 0.10 - ETA: 3s - loss: 5.0347e-04 - accuracy: 0.09 - ETA: 3s - loss: 5.0758e-04 - accuracy: 0.10 - ETA: 3s - loss: 5.0952e-04 - accuracy: 0.10 - ETA: 3s - loss: 5.0442e-04 - accuracy: 0.10 - ETA: 3s - loss: 5.6222e-04 - accuracy: 0.10 - ETA: 3s - loss: 5.6012e-04 - accuracy: 0.10 - ETA: 3s - loss: 5.5476e-04 - accuracy: 0.10 - ETA: 3s - loss: 5.5026e-04 - accuracy: 0.10 - ETA: 3s - loss: 5.4684e-04 - accuracy: 0.09 - ETA: 2s - loss: 5.4268e-04 - accuracy: 0.09 - ETA: 2s - loss: 5.4290e-04 - accuracy: 0.10 - ETA: 2s - loss: 5.4435e-04 - accuracy: 0.10 - ETA: 2s - loss: 5.4330e-04 - accuracy: 0.10 - ETA: 2s - loss: 5.4422e-04 - accuracy: 0.10 - ETA: 2s - loss: 5.4165e-04 - accuracy: 0.10 - ETA: 2s - loss: 5.4007e-04 - accuracy: 0.09 - ETA: 2s - loss: 5.3606e-04 - accuracy: 0.09 - ETA: 2s - loss: 5.3282e-04 - accuracy: 0.09 - ETA: 2s - loss: 5.3577e-04 - accuracy: 0.09 - ETA: 2s - loss: 5.3504e-04 - accuracy: 0.10 - ETA: 2s - loss: 5.3349e-04 - accuracy: 0.09 - ETA: 2s - loss: 5.3004e-04 - accuracy: 0.10 - ETA: 2s - loss: 5.2991e-04 - accuracy: 0.10 - ETA: 2s - loss: 5.4047e-04 - accuracy: 0.10 - ETA: 2s - loss: 5.3728e-04 - accuracy: 0.09 - ETA: 2s - loss: 5.3500e-04 - accuracy: 0.10 - ETA: 2s - loss: 5.7094e-04 - accuracy: 0.10 - ETA: 2s - loss: 5.6741e-04 - accuracy: 0.10 - ETA: 2s - loss: 5.6556e-04 - accuracy: 0.10 - ETA: 2s - loss: 5.6354e-04 - accuracy: 0.09 - ETA: 1s - loss: 5.5997e-04 - accuracy: 0.10 - ETA: 1s - loss: 5.5580e-04 - accuracy: 0.10 - ETA: 1s - loss: 5.5345e-04 - accuracy: 0.10 - ETA: 1s - loss: 5.5519e-04 - accuracy: 0.10 - ETA: 1s - loss: 5.5523e-04 - accuracy: 0.10 - ETA: 1s - loss: 5.5070e-04 - accuracy: 0.10 - ETA: 1s - loss: 5.4916e-04 - accuracy: 0.10 - ETA: 1s - loss: 5.4805e-04 - accuracy: 0.10 - ETA: 1s - loss: 5.4579e-04 - accuracy: 0.10 - ETA: 1s - loss: 5.4342e-04 - accuracy: 0.10 - ETA: 1s - loss: 5.4022e-04 - accuracy: 0.10 - ETA: 1s - loss: 5.3867e-04 - accuracy: 0.10 - ETA: 1s - loss: 5.4342e-04 - accuracy: 0.10 - ETA: 1s - loss: 5.4207e-04 - accuracy: 0.10 - ETA: 1s - loss: 5.3963e-04 - accuracy: 0.10 - ETA: 1s - loss: 5.3596e-04 - accuracy: 0.10 - ETA: 1s - loss: 5.3387e-04 - accuracy: 0.10 - ETA: 1s - loss: 5.3056e-04 - accuracy: 0.10 - ETA: 1s - loss: 5.3046e-04 - accuracy: 0.10 - ETA: 1s - loss: 5.2893e-04 - accuracy: 0.10 - ETA: 0s - loss: 5.2742e-04 - accuracy: 0.10 - ETA: 0s - loss: 5.2890e-04 - accuracy: 0.10 - ETA: 0s - loss: 5.2792e-04 - accuracy: 0.10 - ETA: 0s - loss: 5.2658e-04 - accuracy: 0.10 - ETA: 0s - loss: 5.2708e-04 - accuracy: 0.09 - ETA: 0s - loss: 5.2956e-04 - accuracy: 0.09 - ETA: 0s - loss: 5.2818e-04 - accuracy: 0.09 - ETA: 0s - loss: 5.2643e-04 - accuracy: 0.09 - ETA: 0s - loss: 5.2470e-04 - accuracy: 0.09 - ETA: 0s - loss: 5.2101e-04 - accuracy: 0.09 - ETA: 0s - loss: 5.1703e-04 - accuracy: 0.09 - ETA: 0s - loss: 5.1441e-04 - accuracy: 0.09 - ETA: 0s - loss: 5.1472e-04 - accuracy: 0.09 - ETA: 0s - loss: 5.1407e-04 - accuracy: 0.09 - ETA: 0s - loss: 5.1177e-04 - accuracy: 0.09 - ETA: 0s - loss: 5.1226e-04 - accuracy: 0.10 - ETA: 0s - loss: 5.2974e-04 - accuracy: 0.10 - ETA: 0s - loss: 5.2873e-04 - accuracy: 0.10 - ETA: 0s - loss: 5.2696e-04 - accuracy: 0.10 - ETA: 0s - loss: 5.2810e-04 - accuracy: 0.10 - 5s 2ms/step - loss: 5.2774e-04 - accuracy: 0.1001 - val_loss: 4.8386e-04 - val_accuracy: 0.0997\n",
      "Epoch 5/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 1.0970e-04 - accuracy: 0.09 - ETA: 4s - loss: 3.2810e-04 - accuracy: 0.08 - ETA: 4s - loss: 3.7701e-04 - accuracy: 0.09 - ETA: 4s - loss: 3.8159e-04 - accuracy: 0.09 - ETA: 4s - loss: 4.4181e-04 - accuracy: 0.09 - ETA: 4s - loss: 4.3795e-04 - accuracy: 0.09 - ETA: 4s - loss: 4.3167e-04 - accuracy: 0.09 - ETA: 4s - loss: 4.3457e-04 - accuracy: 0.09 - ETA: 4s - loss: 4.2210e-04 - accuracy: 0.09 - ETA: 4s - loss: 4.1653e-04 - accuracy: 0.09 - ETA: 4s - loss: 4.0250e-04 - accuracy: 0.09 - ETA: 4s - loss: 4.2250e-04 - accuracy: 0.09 - ETA: 3s - loss: 4.1580e-04 - accuracy: 0.09 - ETA: 3s - loss: 4.1311e-04 - accuracy: 0.09 - ETA: 3s - loss: 4.0524e-04 - accuracy: 0.09 - ETA: 3s - loss: 3.9629e-04 - accuracy: 0.09 - ETA: 3s - loss: 3.9093e-04 - accuracy: 0.09 - ETA: 3s - loss: 3.9175e-04 - accuracy: 0.10 - ETA: 3s - loss: 3.8815e-04 - accuracy: 0.10 - ETA: 3s - loss: 3.8359e-04 - accuracy: 0.10 - ETA: 3s - loss: 3.8645e-04 - accuracy: 0.10 - ETA: 3s - loss: 3.9093e-04 - accuracy: 0.09 - ETA: 3s - loss: 3.8889e-04 - accuracy: 0.10 - ETA: 3s - loss: 3.8658e-04 - accuracy: 0.09 - ETA: 3s - loss: 3.9742e-04 - accuracy: 0.09 - ETA: 3s - loss: 3.9450e-04 - accuracy: 0.09 - ETA: 3s - loss: 3.9857e-04 - accuracy: 0.09 - ETA: 3s - loss: 3.9589e-04 - accuracy: 0.09 - ETA: 3s - loss: 3.9627e-04 - accuracy: 0.09 - ETA: 3s - loss: 4.0347e-04 - accuracy: 0.09 - ETA: 3s - loss: 4.0283e-04 - accuracy: 0.09 - ETA: 3s - loss: 3.9854e-04 - accuracy: 0.09 - ETA: 3s - loss: 3.9650e-04 - accuracy: 0.09 - ETA: 2s - loss: 3.9633e-04 - accuracy: 0.09 - ETA: 2s - loss: 3.9394e-04 - accuracy: 0.09 - ETA: 2s - loss: 3.9167e-04 - accuracy: 0.09 - ETA: 2s - loss: 4.0138e-04 - accuracy: 0.09 - ETA: 2s - loss: 4.4039e-04 - accuracy: 0.09 - ETA: 2s - loss: 4.3695e-04 - accuracy: 0.09 - ETA: 2s - loss: 4.3510e-04 - accuracy: 0.09 - ETA: 2s - loss: 4.3302e-04 - accuracy: 0.09 - ETA: 2s - loss: 4.2931e-04 - accuracy: 0.09 - ETA: 2s - loss: 4.2816e-04 - accuracy: 0.09 - ETA: 2s - loss: 4.2519e-04 - accuracy: 0.10 - ETA: 2s - loss: 4.2329e-04 - accuracy: 0.10 - ETA: 2s - loss: 4.2011e-04 - accuracy: 0.10 - ETA: 2s - loss: 4.1725e-04 - accuracy: 0.10 - ETA: 2s - loss: 4.1579e-04 - accuracy: 0.10 - ETA: 2s - loss: 4.1674e-04 - accuracy: 0.10 - ETA: 2s - loss: 4.1503e-04 - accuracy: 0.10 - ETA: 2s - loss: 4.1317e-04 - accuracy: 0.09 - ETA: 2s - loss: 4.1326e-04 - accuracy: 0.09 - ETA: 2s - loss: 4.1089e-04 - accuracy: 0.09 - ETA: 1s - loss: 4.0872e-04 - accuracy: 0.09 - ETA: 1s - loss: 4.0574e-04 - accuracy: 0.10 - ETA: 1s - loss: 4.1254e-04 - accuracy: 0.09 - ETA: 1s - loss: 4.1150e-04 - accuracy: 0.09 - ETA: 1s - loss: 4.1071e-04 - accuracy: 0.09 - ETA: 1s - loss: 4.1425e-04 - accuracy: 0.09 - ETA: 1s - loss: 4.1199e-04 - accuracy: 0.09 - ETA: 1s - loss: 4.3889e-04 - accuracy: 0.09 - ETA: 1s - loss: 4.3575e-04 - accuracy: 0.09 - ETA: 1s - loss: 4.3484e-04 - accuracy: 0.09 - ETA: 1s - loss: 4.3254e-04 - accuracy: 0.09 - ETA: 1s - loss: 4.4969e-04 - accuracy: 0.09 - ETA: 1s - loss: 4.4755e-04 - accuracy: 0.10 - ETA: 1s - loss: 4.4464e-04 - accuracy: 0.09 - ETA: 1s - loss: 4.4174e-04 - accuracy: 0.10 - ETA: 1s - loss: 4.3815e-04 - accuracy: 0.09 - ETA: 1s - loss: 4.3729e-04 - accuracy: 0.09 - ETA: 1s - loss: 4.3904e-04 - accuracy: 0.09 - ETA: 1s - loss: 4.3597e-04 - accuracy: 0.09 - ETA: 1s - loss: 4.3402e-04 - accuracy: 0.09 - ETA: 0s - loss: 4.3520e-04 - accuracy: 0.10 - ETA: 0s - loss: 4.3244e-04 - accuracy: 0.10 - ETA: 0s - loss: 4.2954e-04 - accuracy: 0.10 - ETA: 0s - loss: 4.2782e-04 - accuracy: 0.10 - ETA: 0s - loss: 4.2542e-04 - accuracy: 0.10 - ETA: 0s - loss: 4.2265e-04 - accuracy: 0.09 - ETA: 0s - loss: 4.1924e-04 - accuracy: 0.09 - ETA: 0s - loss: 4.1907e-04 - accuracy: 0.09 - ETA: 0s - loss: 4.1753e-04 - accuracy: 0.09 - ETA: 0s - loss: 4.1489e-04 - accuracy: 0.09 - ETA: 0s - loss: 4.1349e-04 - accuracy: 0.09 - ETA: 0s - loss: 4.1205e-04 - accuracy: 0.09 - ETA: 0s - loss: 4.1041e-04 - accuracy: 0.09 - ETA: 0s - loss: 4.0874e-04 - accuracy: 0.09 - ETA: 0s - loss: 4.2047e-04 - accuracy: 0.09 - ETA: 0s - loss: 4.2129e-04 - accuracy: 0.09 - ETA: 0s - loss: 4.2026e-04 - accuracy: 0.09 - ETA: 0s - loss: 4.1830e-04 - accuracy: 0.09 - ETA: 0s - loss: 4.1616e-04 - accuracy: 0.10 - ETA: 0s - loss: 4.1457e-04 - accuracy: 0.10 - 5s 2ms/step - loss: 4.1253e-04 - accuracy: 0.1001 - val_loss: 3.9480e-04 - val_accuracy: 0.0997\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 115a86ca877789b19b7d193bf682b8f6</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.09969999641180038</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-learning_rate: 0.001</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units: 480</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 2.4183 - accuracy: 0.0000e+ - ETA: 2s - loss: 2.3662 - accuracy: 0.0000e+ - ETA: 2s - loss: 2.3410 - accuracy: 0.0000e+ - ETA: 2s - loss: 2.3194 - accuracy: 0.0000e+ - ETA: 2s - loss: 2.2999 - accuracy: 1.8275e- - ETA: 2s - loss: 2.2815 - accuracy: 4.3808e- - ETA: 2s - loss: 2.2632 - accuracy: 6.0798e- - ETA: 2s - loss: 2.2448 - accuracy: 7.2674e- - ETA: 2s - loss: 2.2267 - accuracy: 0.0014   - ETA: 2s - loss: 2.2103 - accuracy: 0.00 - ETA: 2s - loss: 2.1940 - accuracy: 0.00 - ETA: 2s - loss: 2.1783 - accuracy: 0.00 - ETA: 1s - loss: 2.1597 - accuracy: 0.00 - ETA: 1s - loss: 2.1425 - accuracy: 0.00 - ETA: 1s - loss: 2.1251 - accuracy: 0.00 - ETA: 1s - loss: 2.1073 - accuracy: 0.00 - ETA: 1s - loss: 2.0902 - accuracy: 0.01 - ETA: 1s - loss: 2.0742 - accuracy: 0.01 - ETA: 1s - loss: 2.0569 - accuracy: 0.01 - ETA: 1s - loss: 2.0385 - accuracy: 0.01 - ETA: 1s - loss: 2.0198 - accuracy: 0.01 - ETA: 1s - loss: 2.0019 - accuracy: 0.02 - ETA: 1s - loss: 1.9835 - accuracy: 0.02 - ETA: 1s - loss: 1.9652 - accuracy: 0.02 - ETA: 1s - loss: 1.9455 - accuracy: 0.02 - ETA: 1s - loss: 1.9271 - accuracy: 0.03 - ETA: 1s - loss: 1.9080 - accuracy: 0.03 - ETA: 1s - loss: 1.8883 - accuracy: 0.03 - ETA: 1s - loss: 1.8686 - accuracy: 0.03 - ETA: 1s - loss: 1.8494 - accuracy: 0.04 - ETA: 1s - loss: 1.8305 - accuracy: 0.04 - ETA: 1s - loss: 1.8107 - accuracy: 0.04 - ETA: 0s - loss: 1.7918 - accuracy: 0.04 - ETA: 0s - loss: 1.7716 - accuracy: 0.04 - ETA: 0s - loss: 1.7523 - accuracy: 0.04 - ETA: 0s - loss: 1.7317 - accuracy: 0.05 - ETA: 0s - loss: 1.7117 - accuracy: 0.05 - ETA: 0s - loss: 1.6929 - accuracy: 0.05 - ETA: 0s - loss: 1.6732 - accuracy: 0.05 - ETA: 0s - loss: 1.6539 - accuracy: 0.05 - ETA: 0s - loss: 1.6336 - accuracy: 0.05 - ETA: 0s - loss: 1.6146 - accuracy: 0.05 - ETA: 0s - loss: 1.5949 - accuracy: 0.05 - ETA: 0s - loss: 1.5754 - accuracy: 0.05 - ETA: 0s - loss: 1.5564 - accuracy: 0.06 - ETA: 0s - loss: 1.5366 - accuracy: 0.06 - ETA: 0s - loss: 1.5178 - accuracy: 0.06 - ETA: 0s - loss: 1.4991 - accuracy: 0.06 - ETA: 0s - loss: 1.4805 - accuracy: 0.06 - ETA: 0s - loss: 1.4610 - accuracy: 0.06 - ETA: 0s - loss: 1.4429 - accuracy: 0.06 - ETA: 0s - loss: 1.4239 - accuracy: 0.06 - 3s 2ms/step - loss: 1.4216 - accuracy: 0.0665 - val_loss: 0.5148 - val_accuracy: 0.0997\n",
      "Epoch 2/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 0.4927 - accuracy: 0.06 - ETA: 2s - loss: 0.5015 - accuracy: 0.10 - ETA: 2s - loss: 0.4943 - accuracy: 0.09 - ETA: 2s - loss: 0.4816 - accuracy: 0.09 - ETA: 2s - loss: 0.4737 - accuracy: 0.09 - ETA: 2s - loss: 0.4665 - accuracy: 0.10 - ETA: 2s - loss: 0.4557 - accuracy: 0.09 - ETA: 2s - loss: 0.4466 - accuracy: 0.10 - ETA: 2s - loss: 0.4384 - accuracy: 0.10 - ETA: 2s - loss: 0.4307 - accuracy: 0.10 - ETA: 2s - loss: 0.4231 - accuracy: 0.10 - ETA: 1s - loss: 0.4144 - accuracy: 0.10 - ETA: 1s - loss: 0.4078 - accuracy: 0.09 - ETA: 1s - loss: 0.4006 - accuracy: 0.10 - ETA: 1s - loss: 0.3935 - accuracy: 0.10 - ETA: 1s - loss: 0.3864 - accuracy: 0.09 - ETA: 1s - loss: 0.3796 - accuracy: 0.09 - ETA: 1s - loss: 0.3728 - accuracy: 0.09 - ETA: 1s - loss: 0.3667 - accuracy: 0.09 - ETA: 1s - loss: 0.3610 - accuracy: 0.09 - ETA: 1s - loss: 0.3559 - accuracy: 0.09 - ETA: 1s - loss: 0.3505 - accuracy: 0.09 - ETA: 1s - loss: 0.3453 - accuracy: 0.09 - ETA: 1s - loss: 0.3403 - accuracy: 0.09 - ETA: 1s - loss: 0.3352 - accuracy: 0.09 - ETA: 1s - loss: 0.3306 - accuracy: 0.09 - ETA: 1s - loss: 0.3259 - accuracy: 0.10 - ETA: 1s - loss: 0.3212 - accuracy: 0.10 - ETA: 1s - loss: 0.3169 - accuracy: 0.10 - ETA: 1s - loss: 0.3128 - accuracy: 0.10 - ETA: 1s - loss: 0.3088 - accuracy: 0.10 - ETA: 0s - loss: 0.3046 - accuracy: 0.10 - ETA: 0s - loss: 0.3007 - accuracy: 0.10 - ETA: 0s - loss: 0.2969 - accuracy: 0.10 - ETA: 0s - loss: 0.2937 - accuracy: 0.10 - ETA: 0s - loss: 0.2897 - accuracy: 0.10 - ETA: 0s - loss: 0.2863 - accuracy: 0.10 - ETA: 0s - loss: 0.2828 - accuracy: 0.10 - ETA: 0s - loss: 0.2794 - accuracy: 0.10 - ETA: 0s - loss: 0.2761 - accuracy: 0.10 - ETA: 0s - loss: 0.2729 - accuracy: 0.10 - ETA: 0s - loss: 0.2697 - accuracy: 0.10 - ETA: 0s - loss: 0.2670 - accuracy: 0.10 - ETA: 0s - loss: 0.2640 - accuracy: 0.10 - ETA: 0s - loss: 0.2610 - accuracy: 0.10 - ETA: 0s - loss: 0.2580 - accuracy: 0.10 - ETA: 0s - loss: 0.2550 - accuracy: 0.10 - ETA: 0s - loss: 0.2524 - accuracy: 0.10 - ETA: 0s - loss: 0.2496 - accuracy: 0.10 - ETA: 0s - loss: 0.2469 - accuracy: 0.10 - ETA: 0s - loss: 0.2444 - accuracy: 0.10 - ETA: 0s - loss: 0.2419 - accuracy: 0.10 - 3s 1ms/step - loss: 0.2415 - accuracy: 0.1001 - val_loss: 0.1154 - val_accuracy: 0.0997\n",
      "Epoch 3/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 0.1271 - accuracy: 0.12 - ETA: 2s - loss: 0.1128 - accuracy: 0.09 - ETA: 2s - loss: 0.1132 - accuracy: 0.09 - ETA: 2s - loss: 0.1133 - accuracy: 0.09 - ETA: 2s - loss: 0.1111 - accuracy: 0.10 - ETA: 2s - loss: 0.1105 - accuracy: 0.10 - ETA: 2s - loss: 0.1099 - accuracy: 0.10 - ETA: 2s - loss: 0.1087 - accuracy: 0.10 - ETA: 2s - loss: 0.1077 - accuracy: 0.10 - ETA: 2s - loss: 0.1066 - accuracy: 0.10 - ETA: 2s - loss: 0.1055 - accuracy: 0.10 - ETA: 2s - loss: 0.1046 - accuracy: 0.10 - ETA: 2s - loss: 0.1036 - accuracy: 0.10 - ETA: 1s - loss: 0.1031 - accuracy: 0.10 - ETA: 1s - loss: 0.1027 - accuracy: 0.10 - ETA: 1s - loss: 0.1020 - accuracy: 0.10 - ETA: 1s - loss: 0.1013 - accuracy: 0.10 - ETA: 1s - loss: 0.1005 - accuracy: 0.10 - ETA: 1s - loss: 0.0996 - accuracy: 0.10 - ETA: 1s - loss: 0.0992 - accuracy: 0.10 - ETA: 1s - loss: 0.0984 - accuracy: 0.10 - ETA: 1s - loss: 0.0975 - accuracy: 0.10 - ETA: 1s - loss: 0.0965 - accuracy: 0.10 - ETA: 1s - loss: 0.0959 - accuracy: 0.10 - ETA: 1s - loss: 0.0951 - accuracy: 0.10 - ETA: 1s - loss: 0.0944 - accuracy: 0.10 - ETA: 1s - loss: 0.0938 - accuracy: 0.10 - ETA: 1s - loss: 0.0933 - accuracy: 0.10 - ETA: 1s - loss: 0.0925 - accuracy: 0.10 - ETA: 1s - loss: 0.0919 - accuracy: 0.10 - ETA: 1s - loss: 0.0911 - accuracy: 0.10 - ETA: 1s - loss: 0.0905 - accuracy: 0.10 - ETA: 1s - loss: 0.0898 - accuracy: 0.10 - ETA: 0s - loss: 0.0893 - accuracy: 0.10 - ETA: 0s - loss: 0.0887 - accuracy: 0.10 - ETA: 0s - loss: 0.0881 - accuracy: 0.10 - ETA: 0s - loss: 0.0875 - accuracy: 0.10 - ETA: 0s - loss: 0.0869 - accuracy: 0.10 - ETA: 0s - loss: 0.0864 - accuracy: 0.10 - ETA: 0s - loss: 0.0859 - accuracy: 0.10 - ETA: 0s - loss: 0.0855 - accuracy: 0.10 - ETA: 0s - loss: 0.0850 - accuracy: 0.10 - ETA: 0s - loss: 0.0844 - accuracy: 0.10 - ETA: 0s - loss: 0.0839 - accuracy: 0.10 - ETA: 0s - loss: 0.0833 - accuracy: 0.10 - ETA: 0s - loss: 0.0828 - accuracy: 0.10 - ETA: 0s - loss: 0.0823 - accuracy: 0.10 - ETA: 0s - loss: 0.0818 - accuracy: 0.10 - ETA: 0s - loss: 0.0812 - accuracy: 0.10 - ETA: 0s - loss: 0.0807 - accuracy: 0.10 - ETA: 0s - loss: 0.0802 - accuracy: 0.10 - ETA: 0s - loss: 0.0797 - accuracy: 0.10 - ETA: 0s - loss: 0.0792 - accuracy: 0.10 - 3s 1ms/step - loss: 0.0789 - accuracy: 0.1001 - val_loss: 0.0550 - val_accuracy: 0.0997\n",
      "Epoch 4/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 0.0588 - accuracy: 0.18 - ETA: 2s - loss: 0.0529 - accuracy: 0.11 - ETA: 2s - loss: 0.0550 - accuracy: 0.11 - ETA: 2s - loss: 0.0549 - accuracy: 0.10 - ETA: 2s - loss: 0.0544 - accuracy: 0.10 - ETA: 2s - loss: 0.0542 - accuracy: 0.10 - ETA: 2s - loss: 0.0535 - accuracy: 0.10 - ETA: 2s - loss: 0.0534 - accuracy: 0.10 - ETA: 2s - loss: 0.0526 - accuracy: 0.10 - ETA: 2s - loss: 0.0523 - accuracy: 0.10 - ETA: 2s - loss: 0.0518 - accuracy: 0.10 - ETA: 2s - loss: 0.0511 - accuracy: 0.10 - ETA: 1s - loss: 0.0509 - accuracy: 0.10 - ETA: 1s - loss: 0.0511 - accuracy: 0.10 - ETA: 1s - loss: 0.0511 - accuracy: 0.10 - ETA: 1s - loss: 0.0509 - accuracy: 0.09 - ETA: 1s - loss: 0.0505 - accuracy: 0.09 - ETA: 1s - loss: 0.0504 - accuracy: 0.09 - ETA: 1s - loss: 0.0500 - accuracy: 0.09 - ETA: 1s - loss: 0.0498 - accuracy: 0.10 - ETA: 1s - loss: 0.0497 - accuracy: 0.10 - ETA: 1s - loss: 0.0495 - accuracy: 0.10 - ETA: 1s - loss: 0.0492 - accuracy: 0.10 - ETA: 1s - loss: 0.0489 - accuracy: 0.10 - ETA: 1s - loss: 0.0485 - accuracy: 0.10 - ETA: 1s - loss: 0.0482 - accuracy: 0.10 - ETA: 1s - loss: 0.0480 - accuracy: 0.10 - ETA: 1s - loss: 0.0478 - accuracy: 0.10 - ETA: 1s - loss: 0.0476 - accuracy: 0.10 - ETA: 1s - loss: 0.0474 - accuracy: 0.10 - ETA: 1s - loss: 0.0472 - accuracy: 0.10 - ETA: 1s - loss: 0.0469 - accuracy: 0.09 - ETA: 0s - loss: 0.0468 - accuracy: 0.10 - ETA: 0s - loss: 0.0467 - accuracy: 0.10 - ETA: 0s - loss: 0.0464 - accuracy: 0.10 - ETA: 0s - loss: 0.0462 - accuracy: 0.10 - ETA: 0s - loss: 0.0460 - accuracy: 0.09 - ETA: 0s - loss: 0.0458 - accuracy: 0.10 - ETA: 0s - loss: 0.0456 - accuracy: 0.10 - ETA: 0s - loss: 0.0454 - accuracy: 0.10 - ETA: 0s - loss: 0.0452 - accuracy: 0.10 - ETA: 0s - loss: 0.0451 - accuracy: 0.10 - ETA: 0s - loss: 0.0450 - accuracy: 0.10 - ETA: 0s - loss: 0.0448 - accuracy: 0.10 - ETA: 0s - loss: 0.0446 - accuracy: 0.10 - ETA: 0s - loss: 0.0444 - accuracy: 0.10 - ETA: 0s - loss: 0.0442 - accuracy: 0.10 - ETA: 0s - loss: 0.0440 - accuracy: 0.10 - ETA: 0s - loss: 0.0439 - accuracy: 0.10 - ETA: 0s - loss: 0.0437 - accuracy: 0.10 - ETA: 0s - loss: 0.0436 - accuracy: 0.10 - ETA: 0s - loss: 0.0434 - accuracy: 0.10 - ETA: 0s - loss: 0.0432 - accuracy: 0.10 - 3s 1ms/step - loss: 0.0432 - accuracy: 0.1001 - val_loss: 0.0344 - val_accuracy: 0.0997\n",
      "Epoch 5/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 0.0389 - accuracy: 0.12 - ETA: 2s - loss: 0.0341 - accuracy: 0.10 - ETA: 2s - loss: 0.0353 - accuracy: 0.10 - ETA: 2s - loss: 0.0347 - accuracy: 0.09 - ETA: 2s - loss: 0.0343 - accuracy: 0.10 - ETA: 2s - loss: 0.0338 - accuracy: 0.10 - ETA: 2s - loss: 0.0340 - accuracy: 0.10 - ETA: 2s - loss: 0.0338 - accuracy: 0.10 - ETA: 2s - loss: 0.0337 - accuracy: 0.10 - ETA: 2s - loss: 0.0334 - accuracy: 0.10 - ETA: 2s - loss: 0.0330 - accuracy: 0.10 - ETA: 2s - loss: 0.0329 - accuracy: 0.10 - ETA: 2s - loss: 0.0328 - accuracy: 0.10 - ETA: 1s - loss: 0.0327 - accuracy: 0.10 - ETA: 1s - loss: 0.0325 - accuracy: 0.10 - ETA: 1s - loss: 0.0324 - accuracy: 0.10 - ETA: 1s - loss: 0.0325 - accuracy: 0.10 - ETA: 1s - loss: 0.0324 - accuracy: 0.10 - ETA: 1s - loss: 0.0322 - accuracy: 0.10 - ETA: 1s - loss: 0.0319 - accuracy: 0.10 - ETA: 1s - loss: 0.0318 - accuracy: 0.10 - ETA: 1s - loss: 0.0317 - accuracy: 0.10 - ETA: 1s - loss: 0.0316 - accuracy: 0.10 - ETA: 1s - loss: 0.0315 - accuracy: 0.10 - ETA: 1s - loss: 0.0314 - accuracy: 0.10 - ETA: 1s - loss: 0.0313 - accuracy: 0.10 - ETA: 1s - loss: 0.0313 - accuracy: 0.10 - ETA: 1s - loss: 0.0313 - accuracy: 0.10 - ETA: 1s - loss: 0.0311 - accuracy: 0.10 - ETA: 1s - loss: 0.0310 - accuracy: 0.10 - ETA: 1s - loss: 0.0310 - accuracy: 0.10 - ETA: 1s - loss: 0.0309 - accuracy: 0.10 - ETA: 1s - loss: 0.0309 - accuracy: 0.10 - ETA: 0s - loss: 0.0307 - accuracy: 0.10 - ETA: 0s - loss: 0.0307 - accuracy: 0.10 - ETA: 0s - loss: 0.0305 - accuracy: 0.10 - ETA: 0s - loss: 0.0305 - accuracy: 0.10 - ETA: 0s - loss: 0.0303 - accuracy: 0.10 - ETA: 0s - loss: 0.0302 - accuracy: 0.10 - ETA: 0s - loss: 0.0301 - accuracy: 0.10 - ETA: 0s - loss: 0.0300 - accuracy: 0.10 - ETA: 0s - loss: 0.0299 - accuracy: 0.10 - ETA: 0s - loss: 0.0298 - accuracy: 0.10 - ETA: 0s - loss: 0.0297 - accuracy: 0.10 - ETA: 0s - loss: 0.0296 - accuracy: 0.10 - ETA: 0s - loss: 0.0294 - accuracy: 0.10 - ETA: 0s - loss: 0.0293 - accuracy: 0.10 - ETA: 0s - loss: 0.0293 - accuracy: 0.10 - ETA: 0s - loss: 0.0292 - accuracy: 0.10 - ETA: 0s - loss: 0.0291 - accuracy: 0.10 - ETA: 0s - loss: 0.0290 - accuracy: 0.10 - ETA: 0s - loss: 0.0289 - accuracy: 0.10 - ETA: 0s - loss: 0.0288 - accuracy: 0.10 - ETA: 0s - loss: 0.0288 - accuracy: 0.10 - 3s 1ms/step - loss: 0.0288 - accuracy: 0.1001 - val_loss: 0.0245 - val_accuracy: 0.0997\n",
      "Epoch 1/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 2.4042 - accuracy: 0.0000e+ - ETA: 3s - loss: 2.3997 - accuracy: 0.0000e+ - ETA: 3s - loss: 2.3829 - accuracy: 0.0000e+ - ETA: 2s - loss: 2.3681 - accuracy: 0.0000e+ - ETA: 2s - loss: 2.3562 - accuracy: 0.0000e+ - ETA: 2s - loss: 2.3437 - accuracy: 0.0000e+ - ETA: 2s - loss: 2.3315 - accuracy: 0.0000e+ - ETA: 2s - loss: 2.3199 - accuracy: 0.0000e+ - ETA: 2s - loss: 2.3090 - accuracy: 0.0000e+ - ETA: 2s - loss: 2.2987 - accuracy: 0.0000e+ - ETA: 2s - loss: 2.2878 - accuracy: 0.0000e+ - ETA: 2s - loss: 2.2783 - accuracy: 0.0000e+ - ETA: 2s - loss: 2.2689 - accuracy: 0.0000e+ - ETA: 2s - loss: 2.2598 - accuracy: 1.2207e- - ETA: 2s - loss: 2.2506 - accuracy: 1.6984e- - ETA: 2s - loss: 2.2407 - accuracy: 3.1513e- - ETA: 1s - loss: 2.2313 - accuracy: 2.9481e- - ETA: 1s - loss: 2.2215 - accuracy: 5.9831e- - ETA: 1s - loss: 2.2123 - accuracy: 7.3887e- - ETA: 1s - loss: 2.2022 - accuracy: 8.1913e- - ETA: 1s - loss: 2.1928 - accuracy: 0.0011   - ETA: 1s - loss: 2.1833 - accuracy: 0.00 - ETA: 1s - loss: 2.1737 - accuracy: 0.00 - ETA: 1s - loss: 2.1635 - accuracy: 0.00 - ETA: 1s - loss: 2.1527 - accuracy: 0.00 - ETA: 1s - loss: 2.1426 - accuracy: 0.00 - ETA: 1s - loss: 2.1319 - accuracy: 0.00 - ETA: 1s - loss: 2.1211 - accuracy: 0.00 - ETA: 1s - loss: 2.1090 - accuracy: 0.01 - ETA: 1s - loss: 2.0980 - accuracy: 0.01 - ETA: 1s - loss: 2.0860 - accuracy: 0.01 - ETA: 1s - loss: 2.0746 - accuracy: 0.01 - ETA: 1s - loss: 2.0625 - accuracy: 0.02 - ETA: 1s - loss: 2.0498 - accuracy: 0.02 - ETA: 0s - loss: 2.0379 - accuracy: 0.02 - ETA: 0s - loss: 2.0243 - accuracy: 0.02 - ETA: 0s - loss: 2.0123 - accuracy: 0.03 - ETA: 0s - loss: 2.0000 - accuracy: 0.03 - ETA: 0s - loss: 1.9861 - accuracy: 0.03 - ETA: 0s - loss: 1.9733 - accuracy: 0.03 - ETA: 0s - loss: 1.9603 - accuracy: 0.03 - ETA: 0s - loss: 1.9468 - accuracy: 0.03 - ETA: 0s - loss: 1.9333 - accuracy: 0.04 - ETA: 0s - loss: 1.9194 - accuracy: 0.04 - ETA: 0s - loss: 1.9059 - accuracy: 0.04 - ETA: 0s - loss: 1.8911 - accuracy: 0.04 - ETA: 0s - loss: 1.8766 - accuracy: 0.04 - ETA: 0s - loss: 1.8604 - accuracy: 0.04 - ETA: 0s - loss: 1.8453 - accuracy: 0.04 - ETA: 0s - loss: 1.8296 - accuracy: 0.04 - ETA: 0s - loss: 1.8150 - accuracy: 0.05 - ETA: 0s - loss: 1.8004 - accuracy: 0.05 - ETA: 0s - loss: 1.7862 - accuracy: 0.05 - 3s 2ms/step - loss: 1.7727 - accuracy: 0.0537 - val_loss: 0.9856 - val_accuracy: 0.0997\n",
      "Epoch 2/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 0.9689 - accuracy: 0.03 - ETA: 2s - loss: 0.9777 - accuracy: 0.10 - ETA: 2s - loss: 0.9626 - accuracy: 0.09 - ETA: 2s - loss: 0.9431 - accuracy: 0.09 - ETA: 2s - loss: 0.9279 - accuracy: 0.09 - ETA: 2s - loss: 0.9132 - accuracy: 0.09 - ETA: 2s - loss: 0.8986 - accuracy: 0.09 - ETA: 2s - loss: 0.8839 - accuracy: 0.09 - ETA: 2s - loss: 0.8700 - accuracy: 0.09 - ETA: 2s - loss: 0.8552 - accuracy: 0.09 - ETA: 2s - loss: 0.8412 - accuracy: 0.09 - ETA: 2s - loss: 0.8290 - accuracy: 0.09 - ETA: 2s - loss: 0.8154 - accuracy: 0.09 - ETA: 2s - loss: 0.8023 - accuracy: 0.09 - ETA: 1s - loss: 0.7898 - accuracy: 0.09 - ETA: 1s - loss: 0.7783 - accuracy: 0.09 - ETA: 1s - loss: 0.7664 - accuracy: 0.09 - ETA: 1s - loss: 0.7551 - accuracy: 0.09 - ETA: 1s - loss: 0.7441 - accuracy: 0.09 - ETA: 1s - loss: 0.7334 - accuracy: 0.09 - ETA: 1s - loss: 0.7227 - accuracy: 0.09 - ETA: 1s - loss: 0.7122 - accuracy: 0.09 - ETA: 1s - loss: 0.7028 - accuracy: 0.09 - ETA: 1s - loss: 0.6927 - accuracy: 0.09 - ETA: 1s - loss: 0.6824 - accuracy: 0.09 - ETA: 1s - loss: 0.6725 - accuracy: 0.09 - ETA: 1s - loss: 0.6634 - accuracy: 0.09 - ETA: 1s - loss: 0.6549 - accuracy: 0.09 - ETA: 1s - loss: 0.6455 - accuracy: 0.09 - ETA: 1s - loss: 0.6361 - accuracy: 0.09 - ETA: 1s - loss: 0.6270 - accuracy: 0.09 - ETA: 1s - loss: 0.6188 - accuracy: 0.09 - ETA: 1s - loss: 0.6100 - accuracy: 0.09 - ETA: 1s - loss: 0.6020 - accuracy: 0.09 - ETA: 0s - loss: 0.5943 - accuracy: 0.09 - ETA: 0s - loss: 0.5864 - accuracy: 0.09 - ETA: 0s - loss: 0.5785 - accuracy: 0.09 - ETA: 0s - loss: 0.5714 - accuracy: 0.09 - ETA: 0s - loss: 0.5641 - accuracy: 0.09 - ETA: 0s - loss: 0.5567 - accuracy: 0.09 - ETA: 0s - loss: 0.5498 - accuracy: 0.09 - ETA: 0s - loss: 0.5427 - accuracy: 0.09 - ETA: 0s - loss: 0.5360 - accuracy: 0.09 - ETA: 0s - loss: 0.5292 - accuracy: 0.09 - ETA: 0s - loss: 0.5230 - accuracy: 0.10 - ETA: 0s - loss: 0.5166 - accuracy: 0.10 - ETA: 0s - loss: 0.5103 - accuracy: 0.10 - ETA: 0s - loss: 0.5043 - accuracy: 0.10 - ETA: 0s - loss: 0.4988 - accuracy: 0.10 - ETA: 0s - loss: 0.4937 - accuracy: 0.10 - ETA: 0s - loss: 0.4887 - accuracy: 0.10 - ETA: 0s - loss: 0.4836 - accuracy: 0.10 - ETA: 0s - loss: 0.4785 - accuracy: 0.10 - ETA: 0s - loss: 0.4733 - accuracy: 0.10 - 3s 1ms/step - loss: 0.4725 - accuracy: 0.1001 - val_loss: 0.1989 - val_accuracy: 0.0997\n",
      "Epoch 3/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 0.1486 - accuracy: 0.09 - ETA: 2s - loss: 0.1950 - accuracy: 0.09 - ETA: 2s - loss: 0.1913 - accuracy: 0.10 - ETA: 2s - loss: 0.1889 - accuracy: 0.09 - ETA: 2s - loss: 0.1882 - accuracy: 0.09 - ETA: 2s - loss: 0.1878 - accuracy: 0.09 - ETA: 2s - loss: 0.1868 - accuracy: 0.09 - ETA: 2s - loss: 0.1846 - accuracy: 0.09 - ETA: 2s - loss: 0.1829 - accuracy: 0.09 - ETA: 2s - loss: 0.1803 - accuracy: 0.09 - ETA: 2s - loss: 0.1786 - accuracy: 0.09 - ETA: 2s - loss: 0.1768 - accuracy: 0.09 - ETA: 2s - loss: 0.1755 - accuracy: 0.09 - ETA: 2s - loss: 0.1737 - accuracy: 0.09 - ETA: 2s - loss: 0.1724 - accuracy: 0.09 - ETA: 2s - loss: 0.1707 - accuracy: 0.09 - ETA: 1s - loss: 0.1688 - accuracy: 0.10 - ETA: 1s - loss: 0.1670 - accuracy: 0.10 - ETA: 1s - loss: 0.1655 - accuracy: 0.10 - ETA: 1s - loss: 0.1638 - accuracy: 0.10 - ETA: 1s - loss: 0.1624 - accuracy: 0.10 - ETA: 1s - loss: 0.1610 - accuracy: 0.10 - ETA: 1s - loss: 0.1594 - accuracy: 0.10 - ETA: 1s - loss: 0.1579 - accuracy: 0.10 - ETA: 1s - loss: 0.1566 - accuracy: 0.10 - ETA: 1s - loss: 0.1553 - accuracy: 0.10 - ETA: 1s - loss: 0.1541 - accuracy: 0.10 - ETA: 1s - loss: 0.1526 - accuracy: 0.10 - ETA: 1s - loss: 0.1514 - accuracy: 0.10 - ETA: 1s - loss: 0.1500 - accuracy: 0.09 - ETA: 1s - loss: 0.1485 - accuracy: 0.10 - ETA: 1s - loss: 0.1472 - accuracy: 0.09 - ETA: 1s - loss: 0.1460 - accuracy: 0.09 - ETA: 1s - loss: 0.1447 - accuracy: 0.09 - ETA: 0s - loss: 0.1437 - accuracy: 0.09 - ETA: 0s - loss: 0.1427 - accuracy: 0.09 - ETA: 0s - loss: 0.1415 - accuracy: 0.09 - ETA: 0s - loss: 0.1403 - accuracy: 0.09 - ETA: 0s - loss: 0.1392 - accuracy: 0.10 - ETA: 0s - loss: 0.1381 - accuracy: 0.10 - ETA: 0s - loss: 0.1370 - accuracy: 0.10 - ETA: 0s - loss: 0.1359 - accuracy: 0.10 - ETA: 0s - loss: 0.1349 - accuracy: 0.10 - ETA: 0s - loss: 0.1338 - accuracy: 0.10 - ETA: 0s - loss: 0.1328 - accuracy: 0.10 - ETA: 0s - loss: 0.1320 - accuracy: 0.10 - ETA: 0s - loss: 0.1310 - accuracy: 0.10 - ETA: 0s - loss: 0.1299 - accuracy: 0.10 - ETA: 0s - loss: 0.1290 - accuracy: 0.10 - ETA: 0s - loss: 0.1280 - accuracy: 0.09 - ETA: 0s - loss: 0.1270 - accuracy: 0.09 - ETA: 0s - loss: 0.1260 - accuracy: 0.10 - ETA: 0s - loss: 0.1251 - accuracy: 0.10 - ETA: 0s - loss: 0.1241 - accuracy: 0.10 - 3s 1ms/step - loss: 0.1240 - accuracy: 0.1001 - val_loss: 0.0773 - val_accuracy: 0.0997\n",
      "Epoch 4/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 0.0497 - accuracy: 0.09 - ETA: 2s - loss: 0.0728 - accuracy: 0.09 - ETA: 2s - loss: 0.0781 - accuracy: 0.10 - ETA: 2s - loss: 0.0779 - accuracy: 0.10 - ETA: 2s - loss: 0.0771 - accuracy: 0.10 - ETA: 2s - loss: 0.0765 - accuracy: 0.10 - ETA: 2s - loss: 0.0752 - accuracy: 0.10 - ETA: 2s - loss: 0.0743 - accuracy: 0.10 - ETA: 2s - loss: 0.0740 - accuracy: 0.10 - ETA: 2s - loss: 0.0737 - accuracy: 0.10 - ETA: 2s - loss: 0.0730 - accuracy: 0.09 - ETA: 2s - loss: 0.0724 - accuracy: 0.10 - ETA: 2s - loss: 0.0717 - accuracy: 0.10 - ETA: 1s - loss: 0.0714 - accuracy: 0.10 - ETA: 1s - loss: 0.0711 - accuracy: 0.10 - ETA: 1s - loss: 0.0703 - accuracy: 0.10 - ETA: 1s - loss: 0.0698 - accuracy: 0.09 - ETA: 1s - loss: 0.0694 - accuracy: 0.09 - ETA: 1s - loss: 0.0690 - accuracy: 0.10 - ETA: 1s - loss: 0.0684 - accuracy: 0.10 - ETA: 1s - loss: 0.0682 - accuracy: 0.10 - ETA: 1s - loss: 0.0678 - accuracy: 0.10 - ETA: 1s - loss: 0.0676 - accuracy: 0.10 - ETA: 1s - loss: 0.0673 - accuracy: 0.10 - ETA: 1s - loss: 0.0668 - accuracy: 0.10 - ETA: 1s - loss: 0.0665 - accuracy: 0.10 - ETA: 1s - loss: 0.0660 - accuracy: 0.10 - ETA: 1s - loss: 0.0656 - accuracy: 0.10 - ETA: 1s - loss: 0.0653 - accuracy: 0.10 - ETA: 1s - loss: 0.0649 - accuracy: 0.10 - ETA: 1s - loss: 0.0645 - accuracy: 0.10 - ETA: 1s - loss: 0.0641 - accuracy: 0.10 - ETA: 0s - loss: 0.0638 - accuracy: 0.10 - ETA: 0s - loss: 0.0633 - accuracy: 0.10 - ETA: 0s - loss: 0.0629 - accuracy: 0.10 - ETA: 0s - loss: 0.0626 - accuracy: 0.10 - ETA: 0s - loss: 0.0622 - accuracy: 0.10 - ETA: 0s - loss: 0.0619 - accuracy: 0.10 - ETA: 0s - loss: 0.0615 - accuracy: 0.10 - ETA: 0s - loss: 0.0612 - accuracy: 0.10 - ETA: 0s - loss: 0.0609 - accuracy: 0.10 - ETA: 0s - loss: 0.0607 - accuracy: 0.10 - ETA: 0s - loss: 0.0604 - accuracy: 0.10 - ETA: 0s - loss: 0.0602 - accuracy: 0.10 - ETA: 0s - loss: 0.0598 - accuracy: 0.10 - ETA: 0s - loss: 0.0596 - accuracy: 0.10 - ETA: 0s - loss: 0.0593 - accuracy: 0.10 - ETA: 0s - loss: 0.0590 - accuracy: 0.09 - ETA: 0s - loss: 0.0587 - accuracy: 0.10 - ETA: 0s - loss: 0.0585 - accuracy: 0.10 - ETA: 0s - loss: 0.0582 - accuracy: 0.10 - ETA: 0s - loss: 0.0579 - accuracy: 0.10 - 3s 1ms/step - loss: 0.0577 - accuracy: 0.1001 - val_loss: 0.0430 - val_accuracy: 0.0997\n",
      "Epoch 5/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 0.0282 - accuracy: 0.03 - ETA: 2s - loss: 0.0446 - accuracy: 0.09 - ETA: 2s - loss: 0.0416 - accuracy: 0.09 - ETA: 2s - loss: 0.0418 - accuracy: 0.09 - ETA: 2s - loss: 0.0413 - accuracy: 0.10 - ETA: 2s - loss: 0.0408 - accuracy: 0.10 - ETA: 2s - loss: 0.0404 - accuracy: 0.10 - ETA: 2s - loss: 0.0404 - accuracy: 0.10 - ETA: 2s - loss: 0.0403 - accuracy: 0.10 - ETA: 2s - loss: 0.0403 - accuracy: 0.10 - ETA: 2s - loss: 0.0403 - accuracy: 0.10 - ETA: 2s - loss: 0.0403 - accuracy: 0.10 - ETA: 2s - loss: 0.0403 - accuracy: 0.10 - ETA: 2s - loss: 0.0402 - accuracy: 0.10 - ETA: 2s - loss: 0.0401 - accuracy: 0.10 - ETA: 1s - loss: 0.0402 - accuracy: 0.10 - ETA: 1s - loss: 0.0401 - accuracy: 0.10 - ETA: 1s - loss: 0.0399 - accuracy: 0.10 - ETA: 1s - loss: 0.0399 - accuracy: 0.10 - ETA: 1s - loss: 0.0398 - accuracy: 0.10 - ETA: 1s - loss: 0.0398 - accuracy: 0.10 - ETA: 1s - loss: 0.0399 - accuracy: 0.10 - ETA: 1s - loss: 0.0397 - accuracy: 0.10 - ETA: 1s - loss: 0.0395 - accuracy: 0.10 - ETA: 1s - loss: 0.0393 - accuracy: 0.10 - ETA: 1s - loss: 0.0392 - accuracy: 0.10 - ETA: 1s - loss: 0.0391 - accuracy: 0.10 - ETA: 1s - loss: 0.0388 - accuracy: 0.10 - ETA: 1s - loss: 0.0387 - accuracy: 0.10 - ETA: 1s - loss: 0.0385 - accuracy: 0.10 - ETA: 1s - loss: 0.0383 - accuracy: 0.10 - ETA: 1s - loss: 0.0380 - accuracy: 0.10 - ETA: 1s - loss: 0.0378 - accuracy: 0.10 - ETA: 0s - loss: 0.0376 - accuracy: 0.10 - ETA: 0s - loss: 0.0376 - accuracy: 0.10 - ETA: 0s - loss: 0.0375 - accuracy: 0.10 - ETA: 0s - loss: 0.0374 - accuracy: 0.10 - ETA: 0s - loss: 0.0373 - accuracy: 0.10 - ETA: 0s - loss: 0.0371 - accuracy: 0.10 - ETA: 0s - loss: 0.0370 - accuracy: 0.10 - ETA: 0s - loss: 0.0368 - accuracy: 0.10 - ETA: 0s - loss: 0.0367 - accuracy: 0.10 - ETA: 0s - loss: 0.0366 - accuracy: 0.10 - ETA: 0s - loss: 0.0364 - accuracy: 0.09 - ETA: 0s - loss: 0.0364 - accuracy: 0.09 - ETA: 0s - loss: 0.0363 - accuracy: 0.10 - ETA: 0s - loss: 0.0361 - accuracy: 0.10 - ETA: 0s - loss: 0.0360 - accuracy: 0.10 - ETA: 0s - loss: 0.0359 - accuracy: 0.10 - ETA: 0s - loss: 0.0357 - accuracy: 0.10 - ETA: 0s - loss: 0.0355 - accuracy: 0.10 - ETA: 0s - loss: 0.0353 - accuracy: 0.10 - ETA: 0s - loss: 0.0352 - accuracy: 0.09 - 3s 1ms/step - loss: 0.0352 - accuracy: 0.1001 - val_loss: 0.0287 - val_accuracy: 0.0997\n",
      "Epoch 1/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 2.4449 - accuracy: 0.0000e+ - ETA: 2s - loss: 2.4435 - accuracy: 0.0000e+ - ETA: 2s - loss: 2.4295 - accuracy: 0.0000e+ - ETA: 2s - loss: 2.4170 - accuracy: 2.4414e- - ETA: 2s - loss: 2.4065 - accuracy: 3.6765e- - ETA: 2s - loss: 2.3970 - accuracy: 2.9481e- - ETA: 2s - loss: 2.3879 - accuracy: 2.4606e- - ETA: 2s - loss: 2.3800 - accuracy: 3.1672e- - ETA: 2s - loss: 2.3722 - accuracy: 3.6982e- - ETA: 2s - loss: 2.3654 - accuracy: 4.1118e- - ETA: 2s - loss: 2.3584 - accuracy: 3.6851e- - ETA: 2s - loss: 2.3521 - accuracy: 4.7043e- - ETA: 2s - loss: 2.3465 - accuracy: 7.4405e- - ETA: 1s - loss: 2.3411 - accuracy: 8.6645e- - ETA: 1s - loss: 2.3354 - accuracy: 9.1123e- - ETA: 1s - loss: 2.3295 - accuracy: 9.4848e- - ETA: 1s - loss: 2.3238 - accuracy: 9.8094e- - ETA: 1s - loss: 2.3180 - accuracy: 0.0012   - ETA: 1s - loss: 2.3119 - accuracy: 0.00 - ETA: 1s - loss: 2.3062 - accuracy: 0.00 - ETA: 1s - loss: 2.3005 - accuracy: 0.00 - ETA: 1s - loss: 2.2949 - accuracy: 0.00 - ETA: 1s - loss: 2.2890 - accuracy: 0.00 - ETA: 1s - loss: 2.2828 - accuracy: 0.00 - ETA: 1s - loss: 2.2766 - accuracy: 0.00 - ETA: 1s - loss: 2.2704 - accuracy: 0.00 - ETA: 1s - loss: 2.2639 - accuracy: 0.00 - ETA: 1s - loss: 2.2568 - accuracy: 0.00 - ETA: 1s - loss: 2.2496 - accuracy: 0.00 - ETA: 1s - loss: 2.2430 - accuracy: 0.00 - ETA: 1s - loss: 2.2362 - accuracy: 0.00 - ETA: 1s - loss: 2.2289 - accuracy: 0.00 - ETA: 0s - loss: 2.2214 - accuracy: 0.00 - ETA: 0s - loss: 2.2137 - accuracy: 0.00 - ETA: 0s - loss: 2.2059 - accuracy: 0.01 - ETA: 0s - loss: 2.1978 - accuracy: 0.01 - ETA: 0s - loss: 2.1896 - accuracy: 0.01 - ETA: 0s - loss: 2.1811 - accuracy: 0.01 - ETA: 0s - loss: 2.1725 - accuracy: 0.01 - ETA: 0s - loss: 2.1638 - accuracy: 0.01 - ETA: 0s - loss: 2.1545 - accuracy: 0.01 - ETA: 0s - loss: 2.1452 - accuracy: 0.01 - ETA: 0s - loss: 2.1358 - accuracy: 0.01 - ETA: 0s - loss: 2.1261 - accuracy: 0.02 - ETA: 0s - loss: 2.1161 - accuracy: 0.02 - ETA: 0s - loss: 2.1058 - accuracy: 0.02 - ETA: 0s - loss: 2.0950 - accuracy: 0.02 - ETA: 0s - loss: 2.0845 - accuracy: 0.02 - ETA: 0s - loss: 2.0730 - accuracy: 0.02 - ETA: 0s - loss: 2.0615 - accuracy: 0.03 - ETA: 0s - loss: 2.0497 - accuracy: 0.03 - ETA: 0s - loss: 2.0374 - accuracy: 0.03 - 3s 1ms/step - loss: 2.0315 - accuracy: 0.0339 - val_loss: 1.4078 - val_accuracy: 0.0989\n",
      "Epoch 2/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 1.4358 - accuracy: 0.09 - ETA: 2s - loss: 1.3865 - accuracy: 0.09 - ETA: 2s - loss: 1.3706 - accuracy: 0.09 - ETA: 2s - loss: 1.3547 - accuracy: 0.10 - ETA: 2s - loss: 1.3408 - accuracy: 0.10 - ETA: 2s - loss: 1.3215 - accuracy: 0.10 - ETA: 2s - loss: 1.3035 - accuracy: 0.10 - ETA: 2s - loss: 1.2865 - accuracy: 0.10 - ETA: 2s - loss: 1.2691 - accuracy: 0.10 - ETA: 2s - loss: 1.2511 - accuracy: 0.10 - ETA: 2s - loss: 1.2347 - accuracy: 0.10 - ETA: 2s - loss: 1.2192 - accuracy: 0.10 - ETA: 1s - loss: 1.2034 - accuracy: 0.10 - ETA: 1s - loss: 1.1872 - accuracy: 0.10 - ETA: 1s - loss: 1.1718 - accuracy: 0.10 - ETA: 1s - loss: 1.1568 - accuracy: 0.10 - ETA: 1s - loss: 1.1410 - accuracy: 0.10 - ETA: 1s - loss: 1.1262 - accuracy: 0.10 - ETA: 1s - loss: 1.1113 - accuracy: 0.10 - ETA: 1s - loss: 1.0965 - accuracy: 0.10 - ETA: 1s - loss: 1.0811 - accuracy: 0.09 - ETA: 1s - loss: 1.0677 - accuracy: 0.09 - ETA: 1s - loss: 1.0541 - accuracy: 0.09 - ETA: 1s - loss: 1.0448 - accuracy: 0.09 - ETA: 1s - loss: 1.0315 - accuracy: 0.09 - ETA: 1s - loss: 1.0202 - accuracy: 0.10 - ETA: 1s - loss: 1.0073 - accuracy: 0.10 - ETA: 1s - loss: 0.9944 - accuracy: 0.10 - ETA: 1s - loss: 0.9813 - accuracy: 0.10 - ETA: 1s - loss: 0.9692 - accuracy: 0.10 - ETA: 1s - loss: 0.9570 - accuracy: 0.10 - ETA: 1s - loss: 0.9449 - accuracy: 0.10 - ETA: 1s - loss: 0.9327 - accuracy: 0.10 - ETA: 0s - loss: 0.9208 - accuracy: 0.10 - ETA: 0s - loss: 0.9094 - accuracy: 0.10 - ETA: 0s - loss: 0.8983 - accuracy: 0.09 - ETA: 0s - loss: 0.8869 - accuracy: 0.10 - ETA: 0s - loss: 0.8763 - accuracy: 0.10 - ETA: 0s - loss: 0.8659 - accuracy: 0.10 - ETA: 0s - loss: 0.8560 - accuracy: 0.10 - ETA: 0s - loss: 0.8461 - accuracy: 0.09 - ETA: 0s - loss: 0.8361 - accuracy: 0.10 - ETA: 0s - loss: 0.8263 - accuracy: 0.10 - ETA: 0s - loss: 0.8170 - accuracy: 0.09 - ETA: 0s - loss: 0.8076 - accuracy: 0.09 - ETA: 0s - loss: 0.7987 - accuracy: 0.10 - ETA: 0s - loss: 0.7892 - accuracy: 0.10 - ETA: 0s - loss: 0.7806 - accuracy: 0.09 - ETA: 0s - loss: 0.7717 - accuracy: 0.09 - ETA: 0s - loss: 0.7631 - accuracy: 0.09 - ETA: 0s - loss: 0.7548 - accuracy: 0.10 - ETA: 0s - loss: 0.7462 - accuracy: 0.10 - 3s 1ms/step - loss: 0.7414 - accuracy: 0.1001 - val_loss: 0.3324 - val_accuracy: 0.0997\n",
      "Epoch 3/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 0.3763 - accuracy: 0.15 - ETA: 2s - loss: 0.3284 - accuracy: 0.09 - ETA: 2s - loss: 0.3272 - accuracy: 0.09 - ETA: 2s - loss: 0.3229 - accuracy: 0.09 - ETA: 2s - loss: 0.3163 - accuracy: 0.09 - ETA: 2s - loss: 0.3124 - accuracy: 0.09 - ETA: 2s - loss: 0.3088 - accuracy: 0.09 - ETA: 2s - loss: 0.3049 - accuracy: 0.09 - ETA: 2s - loss: 0.3014 - accuracy: 0.09 - ETA: 2s - loss: 0.2985 - accuracy: 0.09 - ETA: 2s - loss: 0.2950 - accuracy: 0.09 - ETA: 1s - loss: 0.2921 - accuracy: 0.09 - ETA: 1s - loss: 0.2893 - accuracy: 0.09 - ETA: 1s - loss: 0.2865 - accuracy: 0.09 - ETA: 1s - loss: 0.2831 - accuracy: 0.09 - ETA: 1s - loss: 0.2802 - accuracy: 0.09 - ETA: 1s - loss: 0.2779 - accuracy: 0.10 - ETA: 1s - loss: 0.2749 - accuracy: 0.10 - ETA: 1s - loss: 0.2721 - accuracy: 0.10 - ETA: 1s - loss: 0.2695 - accuracy: 0.10 - ETA: 1s - loss: 0.2671 - accuracy: 0.10 - ETA: 1s - loss: 0.2646 - accuracy: 0.10 - ETA: 1s - loss: 0.2625 - accuracy: 0.10 - ETA: 1s - loss: 0.2600 - accuracy: 0.10 - ETA: 1s - loss: 0.2572 - accuracy: 0.10 - ETA: 1s - loss: 0.2547 - accuracy: 0.10 - ETA: 1s - loss: 0.2524 - accuracy: 0.10 - ETA: 1s - loss: 0.2501 - accuracy: 0.10 - ETA: 1s - loss: 0.2479 - accuracy: 0.10 - ETA: 1s - loss: 0.2453 - accuracy: 0.10 - ETA: 1s - loss: 0.2431 - accuracy: 0.10 - ETA: 0s - loss: 0.2413 - accuracy: 0.10 - ETA: 0s - loss: 0.2394 - accuracy: 0.10 - ETA: 0s - loss: 0.2371 - accuracy: 0.10 - ETA: 0s - loss: 0.2353 - accuracy: 0.10 - ETA: 0s - loss: 0.2336 - accuracy: 0.10 - ETA: 0s - loss: 0.2316 - accuracy: 0.10 - ETA: 0s - loss: 0.2298 - accuracy: 0.10 - ETA: 0s - loss: 0.2280 - accuracy: 0.09 - ETA: 0s - loss: 0.2261 - accuracy: 0.10 - ETA: 0s - loss: 0.2243 - accuracy: 0.09 - ETA: 0s - loss: 0.2224 - accuracy: 0.09 - ETA: 0s - loss: 0.2206 - accuracy: 0.09 - ETA: 0s - loss: 0.2189 - accuracy: 0.10 - ETA: 0s - loss: 0.2173 - accuracy: 0.09 - ETA: 0s - loss: 0.2157 - accuracy: 0.10 - ETA: 0s - loss: 0.2142 - accuracy: 0.10 - ETA: 0s - loss: 0.2126 - accuracy: 0.10 - ETA: 0s - loss: 0.2110 - accuracy: 0.10 - ETA: 0s - loss: 0.2095 - accuracy: 0.10 - ETA: 0s - loss: 0.2078 - accuracy: 0.10 - ETA: 0s - loss: 0.2063 - accuracy: 0.10 - 3s 1ms/step - loss: 0.2054 - accuracy: 0.1001 - val_loss: 0.1273 - val_accuracy: 0.0997\n",
      "Epoch 4/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 0.0911 - accuracy: 0.06 - ETA: 2s - loss: 0.1206 - accuracy: 0.08 - ETA: 2s - loss: 0.1219 - accuracy: 0.09 - ETA: 2s - loss: 0.1217 - accuracy: 0.09 - ETA: 2s - loss: 0.1217 - accuracy: 0.10 - ETA: 2s - loss: 0.1219 - accuracy: 0.10 - ETA: 2s - loss: 0.1200 - accuracy: 0.10 - ETA: 2s - loss: 0.1198 - accuracy: 0.10 - ETA: 2s - loss: 0.1187 - accuracy: 0.10 - ETA: 1s - loss: 0.1180 - accuracy: 0.10 - ETA: 1s - loss: 0.1168 - accuracy: 0.10 - ETA: 1s - loss: 0.1162 - accuracy: 0.10 - ETA: 1s - loss: 0.1158 - accuracy: 0.10 - ETA: 1s - loss: 0.1150 - accuracy: 0.10 - ETA: 1s - loss: 0.1145 - accuracy: 0.10 - ETA: 1s - loss: 0.1136 - accuracy: 0.10 - ETA: 1s - loss: 0.1129 - accuracy: 0.10 - ETA: 1s - loss: 0.1120 - accuracy: 0.10 - ETA: 1s - loss: 0.1112 - accuracy: 0.10 - ETA: 1s - loss: 0.1107 - accuracy: 0.10 - ETA: 1s - loss: 0.1100 - accuracy: 0.10 - ETA: 1s - loss: 0.1096 - accuracy: 0.10 - ETA: 1s - loss: 0.1090 - accuracy: 0.10 - ETA: 1s - loss: 0.1084 - accuracy: 0.10 - ETA: 1s - loss: 0.1076 - accuracy: 0.10 - ETA: 1s - loss: 0.1069 - accuracy: 0.10 - ETA: 1s - loss: 0.1064 - accuracy: 0.10 - ETA: 1s - loss: 0.1057 - accuracy: 0.10 - ETA: 0s - loss: 0.1050 - accuracy: 0.10 - ETA: 0s - loss: 0.1043 - accuracy: 0.10 - ETA: 0s - loss: 0.1039 - accuracy: 0.10 - ETA: 0s - loss: 0.1033 - accuracy: 0.10 - ETA: 0s - loss: 0.1028 - accuracy: 0.10 - ETA: 0s - loss: 0.1024 - accuracy: 0.10 - ETA: 0s - loss: 0.1019 - accuracy: 0.10 - ETA: 0s - loss: 0.1015 - accuracy: 0.10 - ETA: 0s - loss: 0.1008 - accuracy: 0.10 - ETA: 0s - loss: 0.1003 - accuracy: 0.10 - ETA: 0s - loss: 0.0997 - accuracy: 0.10 - ETA: 0s - loss: 0.0991 - accuracy: 0.10 - ETA: 0s - loss: 0.0985 - accuracy: 0.10 - ETA: 0s - loss: 0.0980 - accuracy: 0.10 - ETA: 0s - loss: 0.0974 - accuracy: 0.10 - ETA: 0s - loss: 0.0969 - accuracy: 0.10 - ETA: 0s - loss: 0.0964 - accuracy: 0.10 - ETA: 0s - loss: 0.0959 - accuracy: 0.10 - ETA: 0s - loss: 0.0954 - accuracy: 0.10 - ETA: 0s - loss: 0.0950 - accuracy: 0.10 - ETA: 0s - loss: 0.0944 - accuracy: 0.10 - 3s 1ms/step - loss: 0.0942 - accuracy: 0.1001 - val_loss: 0.0703 - val_accuracy: 0.0997\n",
      "Epoch 5/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 0.0827 - accuracy: 0.15 - ETA: 2s - loss: 0.0636 - accuracy: 0.09 - ETA: 2s - loss: 0.0672 - accuracy: 0.10 - ETA: 2s - loss: 0.0666 - accuracy: 0.10 - ETA: 2s - loss: 0.0673 - accuracy: 0.10 - ETA: 2s - loss: 0.0676 - accuracy: 0.10 - ETA: 2s - loss: 0.0672 - accuracy: 0.10 - ETA: 2s - loss: 0.0670 - accuracy: 0.10 - ETA: 2s - loss: 0.0669 - accuracy: 0.10 - ETA: 1s - loss: 0.0668 - accuracy: 0.10 - ETA: 1s - loss: 0.0665 - accuracy: 0.10 - ETA: 1s - loss: 0.0660 - accuracy: 0.09 - ETA: 1s - loss: 0.0657 - accuracy: 0.09 - ETA: 1s - loss: 0.0659 - accuracy: 0.09 - ETA: 1s - loss: 0.0653 - accuracy: 0.09 - ETA: 1s - loss: 0.0650 - accuracy: 0.09 - ETA: 1s - loss: 0.0647 - accuracy: 0.09 - ETA: 1s - loss: 0.0644 - accuracy: 0.09 - ETA: 1s - loss: 0.0641 - accuracy: 0.09 - ETA: 1s - loss: 0.0640 - accuracy: 0.09 - ETA: 1s - loss: 0.0637 - accuracy: 0.09 - ETA: 1s - loss: 0.0635 - accuracy: 0.09 - ETA: 1s - loss: 0.0632 - accuracy: 0.09 - ETA: 1s - loss: 0.0630 - accuracy: 0.09 - ETA: 1s - loss: 0.0627 - accuracy: 0.09 - ETA: 1s - loss: 0.0625 - accuracy: 0.09 - ETA: 1s - loss: 0.0624 - accuracy: 0.09 - ETA: 1s - loss: 0.0622 - accuracy: 0.09 - ETA: 1s - loss: 0.0620 - accuracy: 0.09 - ETA: 1s - loss: 0.0618 - accuracy: 0.09 - ETA: 0s - loss: 0.0614 - accuracy: 0.09 - ETA: 0s - loss: 0.0611 - accuracy: 0.09 - ETA: 0s - loss: 0.0608 - accuracy: 0.09 - ETA: 0s - loss: 0.0605 - accuracy: 0.09 - ETA: 0s - loss: 0.0602 - accuracy: 0.09 - ETA: 0s - loss: 0.0600 - accuracy: 0.09 - ETA: 0s - loss: 0.0598 - accuracy: 0.09 - ETA: 0s - loss: 0.0596 - accuracy: 0.09 - ETA: 0s - loss: 0.0595 - accuracy: 0.09 - ETA: 0s - loss: 0.0594 - accuracy: 0.09 - ETA: 0s - loss: 0.0592 - accuracy: 0.09 - ETA: 0s - loss: 0.0590 - accuracy: 0.09 - ETA: 0s - loss: 0.0587 - accuracy: 0.09 - ETA: 0s - loss: 0.0585 - accuracy: 0.09 - ETA: 0s - loss: 0.0583 - accuracy: 0.10 - ETA: 0s - loss: 0.0581 - accuracy: 0.09 - ETA: 0s - loss: 0.0578 - accuracy: 0.10 - ETA: 0s - loss: 0.0576 - accuracy: 0.10 - ETA: 0s - loss: 0.0574 - accuracy: 0.10 - ETA: 0s - loss: 0.0571 - accuracy: 0.10 - 3s 1ms/step - loss: 0.0569 - accuracy: 0.1001 - val_loss: 0.0465 - val_accuracy: 0.0997\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 784ef1a70d0fa1d588a475458fbe0d39</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.09969999641180038</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-learning_rate: 0.0001</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units: 192</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 2.2607 - accuracy: 0.0000e+ - ETA: 3s - loss: 2.2355 - accuracy: 0.0000e+ - ETA: 3s - loss: 2.2213 - accuracy: 0.0000e+ - ETA: 3s - loss: 2.2092 - accuracy: 0.0000e+ - ETA: 3s - loss: 2.1955 - accuracy: 0.0000e+ - ETA: 3s - loss: 2.1839 - accuracy: 0.0000e+ - ETA: 3s - loss: 2.1709 - accuracy: 1.9172e- - ETA: 3s - loss: 2.1596 - accuracy: 3.2895e- - ETA: 3s - loss: 2.1486 - accuracy: 7.2005e- - ETA: 3s - loss: 2.1376 - accuracy: 0.0010   - ETA: 3s - loss: 2.1270 - accuracy: 0.00 - ETA: 3s - loss: 2.1166 - accuracy: 0.00 - ETA: 3s - loss: 2.1056 - accuracy: 0.00 - ETA: 3s - loss: 2.0947 - accuracy: 0.00 - ETA: 3s - loss: 2.0841 - accuracy: 0.00 - ETA: 3s - loss: 2.0737 - accuracy: 0.00 - ETA: 3s - loss: 2.0633 - accuracy: 0.00 - ETA: 3s - loss: 2.0533 - accuracy: 0.00 - ETA: 3s - loss: 2.0427 - accuracy: 0.01 - ETA: 3s - loss: 2.0319 - accuracy: 0.01 - ETA: 3s - loss: 2.0214 - accuracy: 0.01 - ETA: 3s - loss: 2.0109 - accuracy: 0.01 - ETA: 2s - loss: 2.0002 - accuracy: 0.02 - ETA: 2s - loss: 1.9897 - accuracy: 0.02 - ETA: 2s - loss: 1.9789 - accuracy: 0.02 - ETA: 2s - loss: 1.9677 - accuracy: 0.02 - ETA: 2s - loss: 1.9571 - accuracy: 0.03 - ETA: 2s - loss: 1.9462 - accuracy: 0.03 - ETA: 2s - loss: 1.9365 - accuracy: 0.03 - ETA: 2s - loss: 1.9294 - accuracy: 0.03 - ETA: 2s - loss: 1.9212 - accuracy: 0.03 - ETA: 2s - loss: 1.9127 - accuracy: 0.03 - ETA: 2s - loss: 1.9039 - accuracy: 0.04 - ETA: 2s - loss: 1.8954 - accuracy: 0.04 - ETA: 2s - loss: 1.8860 - accuracy: 0.04 - ETA: 2s - loss: 1.8768 - accuracy: 0.04 - ETA: 2s - loss: 1.8672 - accuracy: 0.04 - ETA: 2s - loss: 1.8574 - accuracy: 0.04 - ETA: 2s - loss: 1.8475 - accuracy: 0.05 - ETA: 2s - loss: 1.8385 - accuracy: 0.05 - ETA: 2s - loss: 1.8298 - accuracy: 0.05 - ETA: 2s - loss: 1.8200 - accuracy: 0.05 - ETA: 2s - loss: 1.8118 - accuracy: 0.05 - ETA: 2s - loss: 1.8034 - accuracy: 0.05 - ETA: 2s - loss: 1.7930 - accuracy: 0.05 - ETA: 2s - loss: 1.7827 - accuracy: 0.05 - ETA: 2s - loss: 1.7724 - accuracy: 0.05 - ETA: 1s - loss: 1.7617 - accuracy: 0.05 - ETA: 1s - loss: 1.7509 - accuracy: 0.06 - ETA: 1s - loss: 1.7401 - accuracy: 0.06 - ETA: 1s - loss: 1.7283 - accuracy: 0.06 - ETA: 1s - loss: 1.7176 - accuracy: 0.06 - ETA: 1s - loss: 1.7061 - accuracy: 0.06 - ETA: 1s - loss: 1.6948 - accuracy: 0.06 - ETA: 1s - loss: 1.6841 - accuracy: 0.06 - ETA: 1s - loss: 1.6731 - accuracy: 0.06 - ETA: 1s - loss: 1.6619 - accuracy: 0.06 - ETA: 1s - loss: 1.6508 - accuracy: 0.06 - ETA: 1s - loss: 1.6393 - accuracy: 0.06 - ETA: 1s - loss: 1.6279 - accuracy: 0.06 - ETA: 1s - loss: 1.6166 - accuracy: 0.06 - ETA: 1s - loss: 1.6051 - accuracy: 0.06 - ETA: 1s - loss: 1.5941 - accuracy: 0.06 - ETA: 1s - loss: 1.5851 - accuracy: 0.06 - ETA: 1s - loss: 1.5736 - accuracy: 0.07 - ETA: 1s - loss: 1.5628 - accuracy: 0.07 - ETA: 0s - loss: 1.5516 - accuracy: 0.07 - ETA: 0s - loss: 1.5400 - accuracy: 0.07 - ETA: 0s - loss: 1.5286 - accuracy: 0.07 - ETA: 0s - loss: 1.5173 - accuracy: 0.07 - ETA: 0s - loss: 1.5067 - accuracy: 0.07 - ETA: 0s - loss: 1.4956 - accuracy: 0.07 - ETA: 0s - loss: 1.4846 - accuracy: 0.07 - ETA: 0s - loss: 1.4734 - accuracy: 0.07 - ETA: 0s - loss: 1.4629 - accuracy: 0.07 - ETA: 0s - loss: 1.4523 - accuracy: 0.07 - ETA: 0s - loss: 1.4423 - accuracy: 0.07 - ETA: 0s - loss: 1.4314 - accuracy: 0.07 - ETA: 0s - loss: 1.4211 - accuracy: 0.07 - ETA: 0s - loss: 1.4105 - accuracy: 0.07 - ETA: 0s - loss: 1.4001 - accuracy: 0.07 - ETA: 0s - loss: 1.3902 - accuracy: 0.07 - ETA: 0s - loss: 1.3806 - accuracy: 0.07 - ETA: 0s - loss: 1.3704 - accuracy: 0.07 - ETA: 0s - loss: 1.3609 - accuracy: 0.07 - 5s 2ms/step - loss: 1.3609 - accuracy: 0.0772 - val_loss: 0.5670 - val_accuracy: 0.0997\n",
      "Epoch 2/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 0.5086 - accuracy: 0.03 - ETA: 3s - loss: 0.5557 - accuracy: 0.10 - ETA: 3s - loss: 0.5534 - accuracy: 0.10 - ETA: 3s - loss: 0.5443 - accuracy: 0.09 - ETA: 3s - loss: 0.5398 - accuracy: 0.09 - ETA: 3s - loss: 0.5342 - accuracy: 0.10 - ETA: 3s - loss: 0.5285 - accuracy: 0.10 - ETA: 3s - loss: 0.5226 - accuracy: 0.10 - ETA: 3s - loss: 0.5177 - accuracy: 0.10 - ETA: 3s - loss: 0.5118 - accuracy: 0.10 - ETA: 3s - loss: 0.5065 - accuracy: 0.09 - ETA: 3s - loss: 0.5013 - accuracy: 0.09 - ETA: 3s - loss: 0.4966 - accuracy: 0.09 - ETA: 3s - loss: 0.4920 - accuracy: 0.09 - ETA: 3s - loss: 0.4875 - accuracy: 0.09 - ETA: 3s - loss: 0.4836 - accuracy: 0.09 - ETA: 3s - loss: 0.4807 - accuracy: 0.09 - ETA: 3s - loss: 0.4767 - accuracy: 0.09 - ETA: 3s - loss: 0.4719 - accuracy: 0.09 - ETA: 3s - loss: 0.4675 - accuracy: 0.09 - ETA: 3s - loss: 0.4631 - accuracy: 0.09 - ETA: 3s - loss: 0.4584 - accuracy: 0.09 - ETA: 3s - loss: 0.4542 - accuracy: 0.09 - ETA: 3s - loss: 0.4505 - accuracy: 0.09 - ETA: 3s - loss: 0.4468 - accuracy: 0.09 - ETA: 3s - loss: 0.4426 - accuracy: 0.09 - ETA: 3s - loss: 0.4389 - accuracy: 0.09 - ETA: 3s - loss: 0.4357 - accuracy: 0.09 - ETA: 3s - loss: 0.4324 - accuracy: 0.09 - ETA: 3s - loss: 0.4284 - accuracy: 0.09 - ETA: 2s - loss: 0.4252 - accuracy: 0.09 - ETA: 2s - loss: 0.4224 - accuracy: 0.09 - ETA: 2s - loss: 0.4190 - accuracy: 0.09 - ETA: 2s - loss: 0.4156 - accuracy: 0.09 - ETA: 2s - loss: 0.4123 - accuracy: 0.09 - ETA: 2s - loss: 0.4097 - accuracy: 0.09 - ETA: 2s - loss: 0.4065 - accuracy: 0.09 - ETA: 2s - loss: 0.4034 - accuracy: 0.09 - ETA: 2s - loss: 0.4003 - accuracy: 0.09 - ETA: 2s - loss: 0.3970 - accuracy: 0.09 - ETA: 2s - loss: 0.3935 - accuracy: 0.09 - ETA: 2s - loss: 0.3899 - accuracy: 0.09 - ETA: 2s - loss: 0.3864 - accuracy: 0.09 - ETA: 2s - loss: 0.3832 - accuracy: 0.09 - ETA: 2s - loss: 0.3798 - accuracy: 0.09 - ETA: 2s - loss: 0.3767 - accuracy: 0.09 - ETA: 2s - loss: 0.3737 - accuracy: 0.09 - ETA: 2s - loss: 0.3707 - accuracy: 0.09 - ETA: 2s - loss: 0.3675 - accuracy: 0.09 - ETA: 1s - loss: 0.3647 - accuracy: 0.09 - ETA: 1s - loss: 0.3617 - accuracy: 0.09 - ETA: 1s - loss: 0.3586 - accuracy: 0.09 - ETA: 1s - loss: 0.3559 - accuracy: 0.09 - ETA: 1s - loss: 0.3532 - accuracy: 0.09 - ETA: 1s - loss: 0.3504 - accuracy: 0.09 - ETA: 1s - loss: 0.3477 - accuracy: 0.09 - ETA: 1s - loss: 0.3449 - accuracy: 0.09 - ETA: 1s - loss: 0.3421 - accuracy: 0.09 - ETA: 1s - loss: 0.3395 - accuracy: 0.09 - ETA: 1s - loss: 0.3370 - accuracy: 0.09 - ETA: 1s - loss: 0.3344 - accuracy: 0.09 - ETA: 1s - loss: 0.3319 - accuracy: 0.09 - ETA: 1s - loss: 0.3294 - accuracy: 0.09 - ETA: 1s - loss: 0.3271 - accuracy: 0.09 - ETA: 1s - loss: 0.3247 - accuracy: 0.10 - ETA: 1s - loss: 0.3224 - accuracy: 0.10 - ETA: 1s - loss: 0.3199 - accuracy: 0.10 - ETA: 0s - loss: 0.3177 - accuracy: 0.10 - ETA: 0s - loss: 0.3153 - accuracy: 0.10 - ETA: 0s - loss: 0.3132 - accuracy: 0.10 - ETA: 0s - loss: 0.3110 - accuracy: 0.10 - ETA: 0s - loss: 0.3088 - accuracy: 0.10 - ETA: 0s - loss: 0.3065 - accuracy: 0.10 - ETA: 0s - loss: 0.3045 - accuracy: 0.10 - ETA: 0s - loss: 0.3026 - accuracy: 0.10 - ETA: 0s - loss: 0.3006 - accuracy: 0.10 - ETA: 0s - loss: 0.2985 - accuracy: 0.10 - ETA: 0s - loss: 0.2968 - accuracy: 0.10 - ETA: 0s - loss: 0.2952 - accuracy: 0.10 - ETA: 0s - loss: 0.2934 - accuracy: 0.10 - ETA: 0s - loss: 0.2914 - accuracy: 0.10 - ETA: 0s - loss: 0.2895 - accuracy: 0.10 - ETA: 0s - loss: 0.2877 - accuracy: 0.10 - ETA: 0s - loss: 0.2858 - accuracy: 0.10 - ETA: 0s - loss: 0.2840 - accuracy: 0.10 - ETA: 0s - loss: 0.2822 - accuracy: 0.10 - ETA: 0s - loss: 0.2805 - accuracy: 0.10 - 5s 2ms/step - loss: 0.2805 - accuracy: 0.1001 - val_loss: 0.1339 - val_accuracy: 0.0997\n",
      "Epoch 3/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 0.1540 - accuracy: 0.09 - ETA: 4s - loss: 0.1336 - accuracy: 0.09 - ETA: 4s - loss: 0.1317 - accuracy: 0.09 - ETA: 4s - loss: 0.1302 - accuracy: 0.09 - ETA: 4s - loss: 0.1300 - accuracy: 0.09 - ETA: 4s - loss: 0.1301 - accuracy: 0.09 - ETA: 4s - loss: 0.1303 - accuracy: 0.09 - ETA: 3s - loss: 0.1292 - accuracy: 0.09 - ETA: 3s - loss: 0.1287 - accuracy: 0.09 - ETA: 3s - loss: 0.1277 - accuracy: 0.09 - ETA: 3s - loss: 0.1273 - accuracy: 0.09 - ETA: 3s - loss: 0.1266 - accuracy: 0.09 - ETA: 3s - loss: 0.1256 - accuracy: 0.09 - ETA: 3s - loss: 0.1250 - accuracy: 0.09 - ETA: 3s - loss: 0.1245 - accuracy: 0.09 - ETA: 3s - loss: 0.1235 - accuracy: 0.09 - ETA: 3s - loss: 0.1225 - accuracy: 0.09 - ETA: 3s - loss: 0.1217 - accuracy: 0.09 - ETA: 3s - loss: 0.1211 - accuracy: 0.09 - ETA: 3s - loss: 0.1208 - accuracy: 0.09 - ETA: 3s - loss: 0.1202 - accuracy: 0.09 - ETA: 3s - loss: 0.1195 - accuracy: 0.09 - ETA: 3s - loss: 0.1190 - accuracy: 0.09 - ETA: 3s - loss: 0.1183 - accuracy: 0.09 - ETA: 3s - loss: 0.1177 - accuracy: 0.09 - ETA: 3s - loss: 0.1172 - accuracy: 0.09 - ETA: 3s - loss: 0.1162 - accuracy: 0.09 - ETA: 2s - loss: 0.1155 - accuracy: 0.09 - ETA: 2s - loss: 0.1150 - accuracy: 0.09 - ETA: 2s - loss: 0.1143 - accuracy: 0.09 - ETA: 2s - loss: 0.1137 - accuracy: 0.09 - ETA: 2s - loss: 0.1132 - accuracy: 0.09 - ETA: 2s - loss: 0.1126 - accuracy: 0.09 - ETA: 2s - loss: 0.1119 - accuracy: 0.09 - ETA: 2s - loss: 0.1115 - accuracy: 0.10 - ETA: 2s - loss: 0.1109 - accuracy: 0.09 - ETA: 2s - loss: 0.1103 - accuracy: 0.09 - ETA: 2s - loss: 0.1097 - accuracy: 0.09 - ETA: 2s - loss: 0.1092 - accuracy: 0.09 - ETA: 2s - loss: 0.1087 - accuracy: 0.09 - ETA: 2s - loss: 0.1081 - accuracy: 0.09 - ETA: 2s - loss: 0.1076 - accuracy: 0.09 - ETA: 2s - loss: 0.1071 - accuracy: 0.09 - ETA: 2s - loss: 0.1066 - accuracy: 0.09 - ETA: 2s - loss: 0.1060 - accuracy: 0.09 - ETA: 2s - loss: 0.1055 - accuracy: 0.09 - ETA: 2s - loss: 0.1051 - accuracy: 0.09 - ETA: 1s - loss: 0.1045 - accuracy: 0.09 - ETA: 1s - loss: 0.1041 - accuracy: 0.09 - ETA: 1s - loss: 0.1035 - accuracy: 0.09 - ETA: 1s - loss: 0.1030 - accuracy: 0.09 - ETA: 1s - loss: 0.1025 - accuracy: 0.09 - ETA: 1s - loss: 0.1021 - accuracy: 0.10 - ETA: 1s - loss: 0.1017 - accuracy: 0.10 - ETA: 1s - loss: 0.1013 - accuracy: 0.09 - ETA: 1s - loss: 0.1007 - accuracy: 0.09 - ETA: 1s - loss: 0.1004 - accuracy: 0.09 - ETA: 1s - loss: 0.0999 - accuracy: 0.09 - ETA: 1s - loss: 0.0994 - accuracy: 0.09 - ETA: 1s - loss: 0.0989 - accuracy: 0.09 - ETA: 1s - loss: 0.0984 - accuracy: 0.10 - ETA: 1s - loss: 0.0980 - accuracy: 0.09 - ETA: 1s - loss: 0.0976 - accuracy: 0.09 - ETA: 1s - loss: 0.0972 - accuracy: 0.10 - ETA: 1s - loss: 0.0968 - accuracy: 0.10 - ETA: 1s - loss: 0.0964 - accuracy: 0.10 - ETA: 0s - loss: 0.0960 - accuracy: 0.10 - ETA: 0s - loss: 0.0956 - accuracy: 0.09 - ETA: 0s - loss: 0.0952 - accuracy: 0.10 - ETA: 0s - loss: 0.0947 - accuracy: 0.09 - ETA: 0s - loss: 0.0944 - accuracy: 0.09 - ETA: 0s - loss: 0.0939 - accuracy: 0.09 - ETA: 0s - loss: 0.0935 - accuracy: 0.09 - ETA: 0s - loss: 0.0931 - accuracy: 0.09 - ETA: 0s - loss: 0.0928 - accuracy: 0.09 - ETA: 0s - loss: 0.0924 - accuracy: 0.09 - ETA: 0s - loss: 0.0920 - accuracy: 0.10 - ETA: 0s - loss: 0.0917 - accuracy: 0.10 - ETA: 0s - loss: 0.0913 - accuracy: 0.10 - ETA: 0s - loss: 0.0910 - accuracy: 0.10 - ETA: 0s - loss: 0.0906 - accuracy: 0.10 - ETA: 0s - loss: 0.0903 - accuracy: 0.10 - ETA: 0s - loss: 0.0900 - accuracy: 0.10 - ETA: 0s - loss: 0.0896 - accuracy: 0.10 - ETA: 0s - loss: 0.0892 - accuracy: 0.10 - 5s 2ms/step - loss: 0.0892 - accuracy: 0.1001 - val_loss: 0.0601 - val_accuracy: 0.0997\n",
      "Epoch 4/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 0.0626 - accuracy: 0.12 - ETA: 3s - loss: 0.0622 - accuracy: 0.11 - ETA: 3s - loss: 0.0624 - accuracy: 0.10 - ETA: 3s - loss: 0.0600 - accuracy: 0.10 - ETA: 3s - loss: 0.0581 - accuracy: 0.10 - ETA: 3s - loss: 0.0583 - accuracy: 0.10 - ETA: 3s - loss: 0.0584 - accuracy: 0.10 - ETA: 3s - loss: 0.0588 - accuracy: 0.10 - ETA: 3s - loss: 0.0585 - accuracy: 0.10 - ETA: 3s - loss: 0.0581 - accuracy: 0.10 - ETA: 3s - loss: 0.0579 - accuracy: 0.10 - ETA: 3s - loss: 0.0581 - accuracy: 0.10 - ETA: 3s - loss: 0.0579 - accuracy: 0.10 - ETA: 3s - loss: 0.0579 - accuracy: 0.10 - ETA: 3s - loss: 0.0575 - accuracy: 0.10 - ETA: 3s - loss: 0.0573 - accuracy: 0.10 - ETA: 3s - loss: 0.0573 - accuracy: 0.10 - ETA: 3s - loss: 0.0569 - accuracy: 0.10 - ETA: 3s - loss: 0.0567 - accuracy: 0.10 - ETA: 3s - loss: 0.0565 - accuracy: 0.10 - ETA: 3s - loss: 0.0565 - accuracy: 0.10 - ETA: 3s - loss: 0.0560 - accuracy: 0.10 - ETA: 3s - loss: 0.0557 - accuracy: 0.10 - ETA: 3s - loss: 0.0555 - accuracy: 0.10 - ETA: 3s - loss: 0.0553 - accuracy: 0.10 - ETA: 3s - loss: 0.0552 - accuracy: 0.10 - ETA: 2s - loss: 0.0549 - accuracy: 0.10 - ETA: 2s - loss: 0.0547 - accuracy: 0.10 - ETA: 2s - loss: 0.0547 - accuracy: 0.10 - ETA: 2s - loss: 0.0545 - accuracy: 0.10 - ETA: 2s - loss: 0.0544 - accuracy: 0.10 - ETA: 2s - loss: 0.0542 - accuracy: 0.10 - ETA: 2s - loss: 0.0541 - accuracy: 0.10 - ETA: 2s - loss: 0.0539 - accuracy: 0.10 - ETA: 2s - loss: 0.0537 - accuracy: 0.10 - ETA: 2s - loss: 0.0536 - accuracy: 0.10 - ETA: 2s - loss: 0.0535 - accuracy: 0.10 - ETA: 2s - loss: 0.0534 - accuracy: 0.10 - ETA: 2s - loss: 0.0532 - accuracy: 0.10 - ETA: 2s - loss: 0.0532 - accuracy: 0.10 - ETA: 2s - loss: 0.0530 - accuracy: 0.10 - ETA: 2s - loss: 0.0528 - accuracy: 0.10 - ETA: 2s - loss: 0.0527 - accuracy: 0.10 - ETA: 2s - loss: 0.0525 - accuracy: 0.10 - ETA: 2s - loss: 0.0524 - accuracy: 0.10 - ETA: 2s - loss: 0.0521 - accuracy: 0.10 - ETA: 2s - loss: 0.0519 - accuracy: 0.10 - ETA: 2s - loss: 0.0518 - accuracy: 0.10 - ETA: 2s - loss: 0.0517 - accuracy: 0.10 - ETA: 2s - loss: 0.0515 - accuracy: 0.10 - ETA: 1s - loss: 0.0514 - accuracy: 0.10 - ETA: 1s - loss: 0.0512 - accuracy: 0.10 - ETA: 1s - loss: 0.0511 - accuracy: 0.10 - ETA: 1s - loss: 0.0510 - accuracy: 0.10 - ETA: 1s - loss: 0.0508 - accuracy: 0.10 - ETA: 1s - loss: 0.0507 - accuracy: 0.10 - ETA: 1s - loss: 0.0505 - accuracy: 0.10 - ETA: 1s - loss: 0.0504 - accuracy: 0.10 - ETA: 1s - loss: 0.0502 - accuracy: 0.10 - ETA: 1s - loss: 0.0501 - accuracy: 0.10 - ETA: 1s - loss: 0.0499 - accuracy: 0.10 - ETA: 1s - loss: 0.0498 - accuracy: 0.10 - ETA: 1s - loss: 0.0496 - accuracy: 0.10 - ETA: 1s - loss: 0.0495 - accuracy: 0.10 - ETA: 1s - loss: 0.0493 - accuracy: 0.10 - ETA: 1s - loss: 0.0492 - accuracy: 0.10 - ETA: 1s - loss: 0.0491 - accuracy: 0.10 - ETA: 1s - loss: 0.0489 - accuracy: 0.10 - ETA: 0s - loss: 0.0487 - accuracy: 0.10 - ETA: 0s - loss: 0.0486 - accuracy: 0.10 - ETA: 0s - loss: 0.0484 - accuracy: 0.10 - ETA: 0s - loss: 0.0483 - accuracy: 0.10 - ETA: 0s - loss: 0.0482 - accuracy: 0.10 - ETA: 0s - loss: 0.0481 - accuracy: 0.10 - ETA: 0s - loss: 0.0480 - accuracy: 0.10 - ETA: 0s - loss: 0.0479 - accuracy: 0.10 - ETA: 0s - loss: 0.0477 - accuracy: 0.10 - ETA: 0s - loss: 0.0476 - accuracy: 0.10 - ETA: 0s - loss: 0.0475 - accuracy: 0.10 - ETA: 0s - loss: 0.0474 - accuracy: 0.10 - ETA: 0s - loss: 0.0473 - accuracy: 0.10 - ETA: 0s - loss: 0.0471 - accuracy: 0.10 - ETA: 0s - loss: 0.0470 - accuracy: 0.10 - ETA: 0s - loss: 0.0469 - accuracy: 0.10 - ETA: 0s - loss: 0.0468 - accuracy: 0.10 - ETA: 0s - loss: 0.0467 - accuracy: 0.10 - ETA: 0s - loss: 0.0466 - accuracy: 0.09 - ETA: 0s - loss: 0.0464 - accuracy: 0.09 - ETA: 0s - loss: 0.0463 - accuracy: 0.10 - 5s 2ms/step - loss: 0.0462 - accuracy: 0.1001 - val_loss: 0.0358 - val_accuracy: 0.0997\n",
      "Epoch 5/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 0.0235 - accuracy: 0.06 - ETA: 4s - loss: 0.0405 - accuracy: 0.10 - ETA: 4s - loss: 0.0386 - accuracy: 0.10 - ETA: 4s - loss: 0.0380 - accuracy: 0.10 - ETA: 4s - loss: 0.0370 - accuracy: 0.10 - ETA: 3s - loss: 0.0366 - accuracy: 0.10 - ETA: 3s - loss: 0.0364 - accuracy: 0.10 - ETA: 3s - loss: 0.0362 - accuracy: 0.09 - ETA: 3s - loss: 0.0357 - accuracy: 0.09 - ETA: 3s - loss: 0.0353 - accuracy: 0.10 - ETA: 3s - loss: 0.0352 - accuracy: 0.10 - ETA: 3s - loss: 0.0353 - accuracy: 0.09 - ETA: 3s - loss: 0.0352 - accuracy: 0.09 - ETA: 3s - loss: 0.0353 - accuracy: 0.09 - ETA: 3s - loss: 0.0352 - accuracy: 0.10 - ETA: 3s - loss: 0.0352 - accuracy: 0.10 - ETA: 3s - loss: 0.0351 - accuracy: 0.09 - ETA: 3s - loss: 0.0349 - accuracy: 0.09 - ETA: 3s - loss: 0.0348 - accuracy: 0.09 - ETA: 3s - loss: 0.0348 - accuracy: 0.09 - ETA: 3s - loss: 0.0347 - accuracy: 0.09 - ETA: 3s - loss: 0.0345 - accuracy: 0.09 - ETA: 3s - loss: 0.0343 - accuracy: 0.09 - ETA: 3s - loss: 0.0344 - accuracy: 0.09 - ETA: 3s - loss: 0.0343 - accuracy: 0.09 - ETA: 3s - loss: 0.0342 - accuracy: 0.09 - ETA: 3s - loss: 0.0341 - accuracy: 0.09 - ETA: 2s - loss: 0.0340 - accuracy: 0.09 - ETA: 2s - loss: 0.0338 - accuracy: 0.09 - ETA: 2s - loss: 0.0336 - accuracy: 0.09 - ETA: 2s - loss: 0.0335 - accuracy: 0.09 - ETA: 2s - loss: 0.0334 - accuracy: 0.09 - ETA: 2s - loss: 0.0333 - accuracy: 0.09 - ETA: 2s - loss: 0.0331 - accuracy: 0.09 - ETA: 2s - loss: 0.0330 - accuracy: 0.09 - ETA: 2s - loss: 0.0329 - accuracy: 0.09 - ETA: 2s - loss: 0.0328 - accuracy: 0.09 - ETA: 2s - loss: 0.0328 - accuracy: 0.09 - ETA: 2s - loss: 0.0327 - accuracy: 0.09 - ETA: 2s - loss: 0.0327 - accuracy: 0.09 - ETA: 2s - loss: 0.0327 - accuracy: 0.09 - ETA: 2s - loss: 0.0326 - accuracy: 0.09 - ETA: 2s - loss: 0.0325 - accuracy: 0.09 - ETA: 2s - loss: 0.0325 - accuracy: 0.09 - ETA: 2s - loss: 0.0324 - accuracy: 0.09 - ETA: 2s - loss: 0.0323 - accuracy: 0.09 - ETA: 2s - loss: 0.0322 - accuracy: 0.09 - ETA: 1s - loss: 0.0321 - accuracy: 0.09 - ETA: 1s - loss: 0.0321 - accuracy: 0.09 - ETA: 1s - loss: 0.0320 - accuracy: 0.09 - ETA: 1s - loss: 0.0319 - accuracy: 0.09 - ETA: 1s - loss: 0.0318 - accuracy: 0.09 - ETA: 1s - loss: 0.0318 - accuracy: 0.09 - ETA: 1s - loss: 0.0317 - accuracy: 0.09 - ETA: 1s - loss: 0.0316 - accuracy: 0.09 - ETA: 1s - loss: 0.0316 - accuracy: 0.09 - ETA: 1s - loss: 0.0315 - accuracy: 0.09 - ETA: 1s - loss: 0.0315 - accuracy: 0.09 - ETA: 1s - loss: 0.0314 - accuracy: 0.09 - ETA: 1s - loss: 0.0313 - accuracy: 0.09 - ETA: 1s - loss: 0.0312 - accuracy: 0.09 - ETA: 1s - loss: 0.0311 - accuracy: 0.09 - ETA: 1s - loss: 0.0310 - accuracy: 0.09 - ETA: 1s - loss: 0.0310 - accuracy: 0.09 - ETA: 1s - loss: 0.0309 - accuracy: 0.09 - ETA: 1s - loss: 0.0308 - accuracy: 0.09 - ETA: 0s - loss: 0.0308 - accuracy: 0.09 - ETA: 0s - loss: 0.0307 - accuracy: 0.09 - ETA: 0s - loss: 0.0306 - accuracy: 0.09 - ETA: 0s - loss: 0.0305 - accuracy: 0.09 - ETA: 0s - loss: 0.0305 - accuracy: 0.09 - ETA: 0s - loss: 0.0304 - accuracy: 0.09 - ETA: 0s - loss: 0.0304 - accuracy: 0.09 - ETA: 0s - loss: 0.0303 - accuracy: 0.09 - ETA: 0s - loss: 0.0303 - accuracy: 0.09 - ETA: 0s - loss: 0.0302 - accuracy: 0.09 - ETA: 0s - loss: 0.0301 - accuracy: 0.09 - ETA: 0s - loss: 0.0300 - accuracy: 0.09 - ETA: 0s - loss: 0.0300 - accuracy: 0.09 - ETA: 0s - loss: 0.0299 - accuracy: 0.09 - ETA: 0s - loss: 0.0299 - accuracy: 0.10 - ETA: 0s - loss: 0.0298 - accuracy: 0.10 - ETA: 0s - loss: 0.0297 - accuracy: 0.10 - ETA: 0s - loss: 0.0297 - accuracy: 0.10 - ETA: 0s - loss: 0.0296 - accuracy: 0.10 - ETA: 0s - loss: 0.0296 - accuracy: 0.10 - 5s 2ms/step - loss: 0.0296 - accuracy: 0.1001 - val_loss: 0.0246 - val_accuracy: 0.0997\n",
      "Epoch 1/5\n",
      "2173/2188 [============================>.] - ETA: 0s - loss: 2.3218 - accuracy: 0.0000e+ - ETA: 3s - loss: 2.3125 - accuracy: 0.0000e+ - ETA: 3s - loss: 2.3073 - accuracy: 0.0000e+ - ETA: 3s - loss: 2.3025 - accuracy: 0.0000e+ - ETA: 3s - loss: 2.2980 - accuracy: 0.0000e+ - ETA: 3s - loss: 2.2936 - accuracy: 0.0000e+ - ETA: 3s - loss: 2.2897 - accuracy: 0.0000e+ - ETA: 3s - loss: 2.2860 - accuracy: 0.0000e+ - ETA: 3s - loss: 2.2824 - accuracy: 0.0000e+ - ETA: 3s - loss: 2.2789 - accuracy: 0.0000e+ - ETA: 3s - loss: 2.2755 - accuracy: 0.0000e+ - ETA: 3s - loss: 2.2724 - accuracy: 1.0452e- - ETA: 3s - loss: 2.2692 - accuracy: 9.5859e- - ETA: 3s - loss: 2.2659 - accuracy: 1.7655e- - ETA: 3s - loss: 2.2629 - accuracy: 2.4606e- - ETA: 3s - loss: 2.2598 - accuracy: 2.2978e- - ETA: 3s - loss: 2.2568 - accuracy: 2.1552e- - ETA: 3s - loss: 2.2536 - accuracy: 2.7056e- - ETA: 3s - loss: 2.2505 - accuracy: 3.1953e- - ETA: 3s - loss: 2.2474 - accuracy: 3.0281e- - ETA: 3s - loss: 2.2444 - accuracy: 3.4530e- - ETA: 3s - loss: 2.2413 - accuracy: 3.2895e- - ETA: 2s - loss: 2.2382 - accuracy: 3.6642e- - ETA: 2s - loss: 2.2351 - accuracy: 4.0064e- - ETA: 2s - loss: 2.2321 - accuracy: 3.8402e- - ETA: 2s - loss: 2.2289 - accuracy: 3.6819e- - ETA: 2s - loss: 2.2256 - accuracy: 3.9837e- - ETA: 2s - loss: 2.2224 - accuracy: 5.1160e- - ETA: 2s - loss: 2.2191 - accuracy: 5.3384e- - ETA: 2s - loss: 2.2160 - accuracy: 6.3452e- - ETA: 2s - loss: 2.2127 - accuracy: 6.1350e- - ETA: 2s - loss: 2.2099 - accuracy: 5.9666e- - ETA: 2s - loss: 2.2069 - accuracy: 6.5255e- - ETA: 2s - loss: 2.2037 - accuracy: 7.0383e- - ETA: 2s - loss: 2.2005 - accuracy: 7.1721e- - ETA: 2s - loss: 2.1969 - accuracy: 8.2847e- - ETA: 2s - loss: 2.1935 - accuracy: 9.0206e- - ETA: 2s - loss: 2.1901 - accuracy: 0.0010   - ETA: 2s - loss: 2.1866 - accuracy: 0.00 - ETA: 2s - loss: 2.1832 - accuracy: 0.00 - ETA: 2s - loss: 2.1797 - accuracy: 0.00 - ETA: 2s - loss: 2.1760 - accuracy: 0.00 - ETA: 1s - loss: 2.1725 - accuracy: 0.00 - ETA: 1s - loss: 2.1687 - accuracy: 0.00 - ETA: 1s - loss: 2.1648 - accuracy: 0.00 - ETA: 1s - loss: 2.1609 - accuracy: 0.00 - ETA: 1s - loss: 2.1570 - accuracy: 0.00 - ETA: 1s - loss: 2.1531 - accuracy: 0.00 - ETA: 1s - loss: 2.1492 - accuracy: 0.00 - ETA: 1s - loss: 2.1452 - accuracy: 0.00 - ETA: 1s - loss: 2.1413 - accuracy: 0.00 - ETA: 1s - loss: 2.1372 - accuracy: 0.00 - ETA: 1s - loss: 2.1332 - accuracy: 0.00 - ETA: 1s - loss: 2.1288 - accuracy: 0.00 - ETA: 1s - loss: 2.1247 - accuracy: 0.00 - ETA: 1s - loss: 2.1209 - accuracy: 0.00 - ETA: 1s - loss: 2.1169 - accuracy: 0.00 - ETA: 1s - loss: 2.1129 - accuracy: 0.00 - ETA: 1s - loss: 2.1085 - accuracy: 0.00 - ETA: 1s - loss: 2.1037 - accuracy: 0.00 - ETA: 1s - loss: 2.0992 - accuracy: 0.00 - ETA: 1s - loss: 2.0947 - accuracy: 0.00 - ETA: 1s - loss: 2.0906 - accuracy: 0.00 - ETA: 0s - loss: 2.0862 - accuracy: 0.00 - ETA: 0s - loss: 2.0817 - accuracy: 0.00 - ETA: 0s - loss: 2.0769 - accuracy: 0.00 - ETA: 0s - loss: 2.0721 - accuracy: 0.00 - ETA: 0s - loss: 2.0672 - accuracy: 0.00 - ETA: 0s - loss: 2.0625 - accuracy: 0.01 - ETA: 0s - loss: 2.0578 - accuracy: 0.01 - ETA: 0s - loss: 2.0530 - accuracy: 0.01 - ETA: 0s - loss: 2.0483 - accuracy: 0.01 - ETA: 0s - loss: 2.0428 - accuracy: 0.01 - ETA: 0s - loss: 2.0374 - accuracy: 0.01 - ETA: 0s - loss: 2.0324 - accuracy: 0.01 - ETA: 0s - loss: 2.0274 - accuracy: 0.01 - ETA: 0s - loss: 2.0223 - accuracy: 0.01 - ETA: 0s - loss: 2.0170 - accuracy: 0.01 - ETA: 0s - loss: 2.0118 - accuracy: 0.01 - ETA: 0s - loss: 2.0063 - accuracy: 0.01 - ETA: 0s - loss: 2.0005 - accuracy: 0.02 - ETA: 0s - loss: 1.9949 - accuracy: 0.02 - ETA: 0s - loss: 1.9896 - accuracy: 0.0230WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_end` time: 0.0010s). Check your callbacks.\n",
      "2188/2188 [==============================] - 5s 2ms/step - loss: 1.9865 - accuracy: 0.0235 - val_loss: 1.5186 - val_accuracy: 0.0976\n",
      "Epoch 2/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 1.5521 - accuracy: 0.09 - ETA: 3s - loss: 1.5057 - accuracy: 0.09 - ETA: 4s - loss: 1.4968 - accuracy: 0.09 - ETA: 4s - loss: 1.4904 - accuracy: 0.09 - ETA: 4s - loss: 1.4860 - accuracy: 0.09 - ETA: 4s - loss: 1.4787 - accuracy: 0.09 - ETA: 4s - loss: 1.4731 - accuracy: 0.09 - ETA: 3s - loss: 1.4654 - accuracy: 0.09 - ETA: 3s - loss: 1.4585 - accuracy: 0.09 - ETA: 3s - loss: 1.4523 - accuracy: 0.09 - ETA: 3s - loss: 1.4446 - accuracy: 0.09 - ETA: 3s - loss: 1.4366 - accuracy: 0.09 - ETA: 3s - loss: 1.4283 - accuracy: 0.09 - ETA: 3s - loss: 1.4196 - accuracy: 0.09 - ETA: 3s - loss: 1.4122 - accuracy: 0.09 - ETA: 3s - loss: 1.4049 - accuracy: 0.09 - ETA: 3s - loss: 1.3971 - accuracy: 0.09 - ETA: 3s - loss: 1.3889 - accuracy: 0.09 - ETA: 3s - loss: 1.3813 - accuracy: 0.09 - ETA: 3s - loss: 1.3732 - accuracy: 0.09 - ETA: 3s - loss: 1.3658 - accuracy: 0.09 - ETA: 3s - loss: 1.3578 - accuracy: 0.09 - ETA: 3s - loss: 1.3499 - accuracy: 0.09 - ETA: 3s - loss: 1.3420 - accuracy: 0.09 - ETA: 3s - loss: 1.3346 - accuracy: 0.09 - ETA: 2s - loss: 1.3264 - accuracy: 0.09 - ETA: 2s - loss: 1.3185 - accuracy: 0.09 - ETA: 2s - loss: 1.3106 - accuracy: 0.09 - ETA: 2s - loss: 1.3024 - accuracy: 0.09 - ETA: 2s - loss: 1.2950 - accuracy: 0.09 - ETA: 2s - loss: 1.2875 - accuracy: 0.09 - ETA: 2s - loss: 1.2794 - accuracy: 0.09 - ETA: 2s - loss: 1.2713 - accuracy: 0.09 - ETA: 2s - loss: 1.2638 - accuracy: 0.09 - ETA: 2s - loss: 1.2571 - accuracy: 0.09 - ETA: 2s - loss: 1.2503 - accuracy: 0.09 - ETA: 2s - loss: 1.2448 - accuracy: 0.09 - ETA: 2s - loss: 1.2370 - accuracy: 0.09 - ETA: 2s - loss: 1.2290 - accuracy: 0.09 - ETA: 2s - loss: 1.2226 - accuracy: 0.09 - ETA: 2s - loss: 1.2150 - accuracy: 0.09 - ETA: 2s - loss: 1.2074 - accuracy: 0.09 - ETA: 2s - loss: 1.1997 - accuracy: 0.09 - ETA: 2s - loss: 1.1921 - accuracy: 0.09 - ETA: 2s - loss: 1.1846 - accuracy: 0.09 - ETA: 2s - loss: 1.1766 - accuracy: 0.09 - ETA: 1s - loss: 1.1691 - accuracy: 0.09 - ETA: 1s - loss: 1.1619 - accuracy: 0.09 - ETA: 1s - loss: 1.1548 - accuracy: 0.09 - ETA: 1s - loss: 1.1476 - accuracy: 0.09 - ETA: 1s - loss: 1.1413 - accuracy: 0.09 - ETA: 1s - loss: 1.1344 - accuracy: 0.09 - ETA: 1s - loss: 1.1273 - accuracy: 0.09 - ETA: 1s - loss: 1.1195 - accuracy: 0.09 - ETA: 1s - loss: 1.1122 - accuracy: 0.09 - ETA: 1s - loss: 1.1055 - accuracy: 0.09 - ETA: 1s - loss: 1.0988 - accuracy: 0.09 - ETA: 1s - loss: 1.0917 - accuracy: 0.09 - ETA: 1s - loss: 1.0847 - accuracy: 0.09 - ETA: 1s - loss: 1.0777 - accuracy: 0.09 - ETA: 1s - loss: 1.0708 - accuracy: 0.09 - ETA: 1s - loss: 1.0644 - accuracy: 0.09 - ETA: 1s - loss: 1.0576 - accuracy: 0.10 - ETA: 1s - loss: 1.0511 - accuracy: 0.10 - ETA: 1s - loss: 1.0443 - accuracy: 0.10 - ETA: 1s - loss: 1.0371 - accuracy: 0.10 - ETA: 0s - loss: 1.0302 - accuracy: 0.09 - ETA: 0s - loss: 1.0234 - accuracy: 0.09 - ETA: 0s - loss: 1.0169 - accuracy: 0.09 - ETA: 0s - loss: 1.0111 - accuracy: 0.09 - ETA: 0s - loss: 1.0048 - accuracy: 0.09 - ETA: 0s - loss: 0.9985 - accuracy: 0.09 - ETA: 0s - loss: 0.9923 - accuracy: 0.09 - ETA: 0s - loss: 0.9857 - accuracy: 0.09 - ETA: 0s - loss: 0.9794 - accuracy: 0.09 - ETA: 0s - loss: 0.9732 - accuracy: 0.09 - ETA: 0s - loss: 0.9670 - accuracy: 0.09 - ETA: 0s - loss: 0.9605 - accuracy: 0.09 - ETA: 0s - loss: 0.9544 - accuracy: 0.09 - ETA: 0s - loss: 0.9485 - accuracy: 0.09 - ETA: 0s - loss: 0.9429 - accuracy: 0.09 - ETA: 0s - loss: 0.9373 - accuracy: 0.09 - ETA: 0s - loss: 0.9312 - accuracy: 0.09 - ETA: 0s - loss: 0.9256 - accuracy: 0.10 - ETA: 0s - loss: 0.9199 - accuracy: 0.10 - 5s 2ms/step - loss: 0.9163 - accuracy: 0.1001 - val_loss: 0.4440 - val_accuracy: 0.0997\n",
      "Epoch 3/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 0.4492 - accuracy: 0.03 - ETA: 4s - loss: 0.4409 - accuracy: 0.08 - ETA: 4s - loss: 0.4394 - accuracy: 0.09 - ETA: 4s - loss: 0.4368 - accuracy: 0.09 - ETA: 4s - loss: 0.4348 - accuracy: 0.09 - ETA: 4s - loss: 0.4306 - accuracy: 0.09 - ETA: 4s - loss: 0.4281 - accuracy: 0.09 - ETA: 4s - loss: 0.4250 - accuracy: 0.09 - ETA: 3s - loss: 0.4211 - accuracy: 0.09 - ETA: 3s - loss: 0.4169 - accuracy: 0.09 - ETA: 3s - loss: 0.4141 - accuracy: 0.09 - ETA: 3s - loss: 0.4116 - accuracy: 0.09 - ETA: 3s - loss: 0.4072 - accuracy: 0.09 - ETA: 3s - loss: 0.4040 - accuracy: 0.09 - ETA: 3s - loss: 0.4015 - accuracy: 0.09 - ETA: 3s - loss: 0.3985 - accuracy: 0.09 - ETA: 3s - loss: 0.3955 - accuracy: 0.09 - ETA: 3s - loss: 0.3923 - accuracy: 0.09 - ETA: 3s - loss: 0.3891 - accuracy: 0.09 - ETA: 3s - loss: 0.3868 - accuracy: 0.09 - ETA: 3s - loss: 0.3838 - accuracy: 0.09 - ETA: 3s - loss: 0.3807 - accuracy: 0.09 - ETA: 3s - loss: 0.3777 - accuracy: 0.09 - ETA: 3s - loss: 0.3754 - accuracy: 0.09 - ETA: 3s - loss: 0.3729 - accuracy: 0.09 - ETA: 3s - loss: 0.3706 - accuracy: 0.09 - ETA: 2s - loss: 0.3684 - accuracy: 0.09 - ETA: 2s - loss: 0.3660 - accuracy: 0.09 - ETA: 2s - loss: 0.3636 - accuracy: 0.09 - ETA: 2s - loss: 0.3614 - accuracy: 0.09 - ETA: 2s - loss: 0.3588 - accuracy: 0.09 - ETA: 2s - loss: 0.3565 - accuracy: 0.09 - ETA: 2s - loss: 0.3542 - accuracy: 0.09 - ETA: 2s - loss: 0.3516 - accuracy: 0.09 - ETA: 2s - loss: 0.3494 - accuracy: 0.09 - ETA: 2s - loss: 0.3474 - accuracy: 0.09 - ETA: 2s - loss: 0.3462 - accuracy: 0.09 - ETA: 2s - loss: 0.3447 - accuracy: 0.09 - ETA: 2s - loss: 0.3431 - accuracy: 0.09 - ETA: 2s - loss: 0.3414 - accuracy: 0.10 - ETA: 2s - loss: 0.3392 - accuracy: 0.10 - ETA: 2s - loss: 0.3372 - accuracy: 0.10 - ETA: 2s - loss: 0.3354 - accuracy: 0.10 - ETA: 2s - loss: 0.3334 - accuracy: 0.10 - ETA: 2s - loss: 0.3312 - accuracy: 0.10 - ETA: 2s - loss: 0.3287 - accuracy: 0.10 - ETA: 2s - loss: 0.3264 - accuracy: 0.10 - ETA: 2s - loss: 0.3239 - accuracy: 0.09 - ETA: 1s - loss: 0.3218 - accuracy: 0.10 - ETA: 1s - loss: 0.3197 - accuracy: 0.09 - ETA: 1s - loss: 0.3176 - accuracy: 0.09 - ETA: 1s - loss: 0.3155 - accuracy: 0.09 - ETA: 1s - loss: 0.3133 - accuracy: 0.10 - ETA: 1s - loss: 0.3110 - accuracy: 0.09 - ETA: 1s - loss: 0.3089 - accuracy: 0.09 - ETA: 1s - loss: 0.3068 - accuracy: 0.09 - ETA: 1s - loss: 0.3048 - accuracy: 0.09 - ETA: 1s - loss: 0.3028 - accuracy: 0.09 - ETA: 1s - loss: 0.3009 - accuracy: 0.10 - ETA: 1s - loss: 0.2990 - accuracy: 0.09 - ETA: 1s - loss: 0.2972 - accuracy: 0.09 - ETA: 1s - loss: 0.2952 - accuracy: 0.10 - ETA: 1s - loss: 0.2933 - accuracy: 0.10 - ETA: 1s - loss: 0.2914 - accuracy: 0.09 - ETA: 1s - loss: 0.2897 - accuracy: 0.10 - ETA: 0s - loss: 0.2882 - accuracy: 0.09 - ETA: 0s - loss: 0.2866 - accuracy: 0.09 - ETA: 0s - loss: 0.2850 - accuracy: 0.09 - ETA: 0s - loss: 0.2832 - accuracy: 0.09 - ETA: 0s - loss: 0.2813 - accuracy: 0.09 - ETA: 0s - loss: 0.2798 - accuracy: 0.09 - ETA: 0s - loss: 0.2784 - accuracy: 0.09 - ETA: 0s - loss: 0.2767 - accuracy: 0.09 - ETA: 0s - loss: 0.2751 - accuracy: 0.09 - ETA: 0s - loss: 0.2737 - accuracy: 0.09 - ETA: 0s - loss: 0.2722 - accuracy: 0.09 - ETA: 0s - loss: 0.2708 - accuracy: 0.10 - ETA: 0s - loss: 0.2694 - accuracy: 0.10 - ETA: 0s - loss: 0.2680 - accuracy: 0.10 - ETA: 0s - loss: 0.2665 - accuracy: 0.10 - ETA: 0s - loss: 0.2649 - accuracy: 0.10 - ETA: 0s - loss: 0.2635 - accuracy: 0.10 - ETA: 0s - loss: 0.2621 - accuracy: 0.10 - 5s 2ms/step - loss: 0.2610 - accuracy: 0.1001 - val_loss: 0.1504 - val_accuracy: 0.0997\n",
      "Epoch 4/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 0.1167 - accuracy: 0.15 - ETA: 3s - loss: 0.1485 - accuracy: 0.09 - ETA: 3s - loss: 0.1502 - accuracy: 0.09 - ETA: 3s - loss: 0.1509 - accuracy: 0.10 - ETA: 3s - loss: 0.1494 - accuracy: 0.09 - ETA: 3s - loss: 0.1472 - accuracy: 0.09 - ETA: 3s - loss: 0.1456 - accuracy: 0.10 - ETA: 3s - loss: 0.1448 - accuracy: 0.10 - ETA: 3s - loss: 0.1442 - accuracy: 0.10 - ETA: 3s - loss: 0.1439 - accuracy: 0.10 - ETA: 3s - loss: 0.1432 - accuracy: 0.09 - ETA: 3s - loss: 0.1432 - accuracy: 0.10 - ETA: 3s - loss: 0.1424 - accuracy: 0.10 - ETA: 3s - loss: 0.1414 - accuracy: 0.10 - ETA: 3s - loss: 0.1408 - accuracy: 0.09 - ETA: 3s - loss: 0.1397 - accuracy: 0.09 - ETA: 3s - loss: 0.1392 - accuracy: 0.09 - ETA: 3s - loss: 0.1381 - accuracy: 0.09 - ETA: 2s - loss: 0.1375 - accuracy: 0.09 - ETA: 2s - loss: 0.1368 - accuracy: 0.09 - ETA: 2s - loss: 0.1361 - accuracy: 0.09 - ETA: 2s - loss: 0.1353 - accuracy: 0.09 - ETA: 2s - loss: 0.1348 - accuracy: 0.09 - ETA: 2s - loss: 0.1344 - accuracy: 0.09 - ETA: 2s - loss: 0.1338 - accuracy: 0.09 - ETA: 2s - loss: 0.1331 - accuracy: 0.09 - ETA: 2s - loss: 0.1325 - accuracy: 0.09 - ETA: 2s - loss: 0.1320 - accuracy: 0.09 - ETA: 2s - loss: 0.1314 - accuracy: 0.09 - ETA: 2s - loss: 0.1307 - accuracy: 0.09 - ETA: 2s - loss: 0.1301 - accuracy: 0.09 - ETA: 2s - loss: 0.1295 - accuracy: 0.09 - ETA: 2s - loss: 0.1287 - accuracy: 0.09 - ETA: 2s - loss: 0.1281 - accuracy: 0.09 - ETA: 2s - loss: 0.1275 - accuracy: 0.09 - ETA: 2s - loss: 0.1269 - accuracy: 0.09 - ETA: 2s - loss: 0.1264 - accuracy: 0.09 - ETA: 2s - loss: 0.1260 - accuracy: 0.10 - ETA: 1s - loss: 0.1255 - accuracy: 0.09 - ETA: 1s - loss: 0.1250 - accuracy: 0.09 - ETA: 1s - loss: 0.1245 - accuracy: 0.09 - ETA: 1s - loss: 0.1239 - accuracy: 0.09 - ETA: 1s - loss: 0.1234 - accuracy: 0.09 - ETA: 1s - loss: 0.1229 - accuracy: 0.09 - ETA: 1s - loss: 0.1225 - accuracy: 0.09 - ETA: 1s - loss: 0.1219 - accuracy: 0.09 - ETA: 1s - loss: 0.1213 - accuracy: 0.09 - ETA: 1s - loss: 0.1208 - accuracy: 0.09 - ETA: 1s - loss: 0.1201 - accuracy: 0.09 - ETA: 1s - loss: 0.1195 - accuracy: 0.09 - ETA: 1s - loss: 0.1189 - accuracy: 0.09 - ETA: 1s - loss: 0.1185 - accuracy: 0.09 - ETA: 1s - loss: 0.1180 - accuracy: 0.09 - ETA: 1s - loss: 0.1175 - accuracy: 0.09 - ETA: 1s - loss: 0.1170 - accuracy: 0.09 - ETA: 1s - loss: 0.1165 - accuracy: 0.09 - ETA: 1s - loss: 0.1161 - accuracy: 0.09 - ETA: 1s - loss: 0.1155 - accuracy: 0.09 - ETA: 0s - loss: 0.1151 - accuracy: 0.09 - ETA: 0s - loss: 0.1146 - accuracy: 0.09 - ETA: 0s - loss: 0.1142 - accuracy: 0.09 - ETA: 0s - loss: 0.1137 - accuracy: 0.09 - ETA: 0s - loss: 0.1132 - accuracy: 0.09 - ETA: 0s - loss: 0.1128 - accuracy: 0.09 - ETA: 0s - loss: 0.1124 - accuracy: 0.09 - ETA: 0s - loss: 0.1120 - accuracy: 0.09 - ETA: 0s - loss: 0.1114 - accuracy: 0.10 - ETA: 0s - loss: 0.1110 - accuracy: 0.10 - ETA: 0s - loss: 0.1106 - accuracy: 0.10 - ETA: 0s - loss: 0.1102 - accuracy: 0.10 - ETA: 0s - loss: 0.1098 - accuracy: 0.10 - ETA: 0s - loss: 0.1094 - accuracy: 0.10 - ETA: 0s - loss: 0.1089 - accuracy: 0.10 - ETA: 0s - loss: 0.1085 - accuracy: 0.10 - ETA: 0s - loss: 0.1080 - accuracy: 0.10 - ETA: 0s - loss: 0.1077 - accuracy: 0.10 - ETA: 0s - loss: 0.1072 - accuracy: 0.10 - 5s 2ms/step - loss: 0.1068 - accuracy: 0.1001 - val_loss: 0.0762 - val_accuracy: 0.0997\n",
      "Epoch 5/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 0.0556 - accuracy: 0.15 - ETA: 3s - loss: 0.0730 - accuracy: 0.10 - ETA: 3s - loss: 0.0758 - accuracy: 0.10 - ETA: 3s - loss: 0.0741 - accuracy: 0.10 - ETA: 3s - loss: 0.0749 - accuracy: 0.10 - ETA: 3s - loss: 0.0747 - accuracy: 0.10 - ETA: 3s - loss: 0.0741 - accuracy: 0.10 - ETA: 3s - loss: 0.0745 - accuracy: 0.10 - ETA: 3s - loss: 0.0741 - accuracy: 0.10 - ETA: 3s - loss: 0.0736 - accuracy: 0.10 - ETA: 3s - loss: 0.0731 - accuracy: 0.10 - ETA: 3s - loss: 0.0730 - accuracy: 0.10 - ETA: 3s - loss: 0.0727 - accuracy: 0.10 - ETA: 3s - loss: 0.0722 - accuracy: 0.10 - ETA: 3s - loss: 0.0717 - accuracy: 0.10 - ETA: 3s - loss: 0.0717 - accuracy: 0.10 - ETA: 3s - loss: 0.0715 - accuracy: 0.10 - ETA: 3s - loss: 0.0713 - accuracy: 0.10 - ETA: 2s - loss: 0.0711 - accuracy: 0.10 - ETA: 2s - loss: 0.0708 - accuracy: 0.10 - ETA: 2s - loss: 0.0705 - accuracy: 0.10 - ETA: 2s - loss: 0.0702 - accuracy: 0.10 - ETA: 2s - loss: 0.0700 - accuracy: 0.10 - ETA: 2s - loss: 0.0697 - accuracy: 0.09 - ETA: 2s - loss: 0.0697 - accuracy: 0.10 - ETA: 2s - loss: 0.0695 - accuracy: 0.09 - ETA: 2s - loss: 0.0694 - accuracy: 0.09 - ETA: 2s - loss: 0.0691 - accuracy: 0.09 - ETA: 2s - loss: 0.0691 - accuracy: 0.09 - ETA: 2s - loss: 0.0689 - accuracy: 0.09 - ETA: 2s - loss: 0.0689 - accuracy: 0.09 - ETA: 2s - loss: 0.0687 - accuracy: 0.09 - ETA: 2s - loss: 0.0685 - accuracy: 0.09 - ETA: 2s - loss: 0.0681 - accuracy: 0.09 - ETA: 2s - loss: 0.0678 - accuracy: 0.09 - ETA: 2s - loss: 0.0676 - accuracy: 0.09 - ETA: 2s - loss: 0.0675 - accuracy: 0.09 - ETA: 2s - loss: 0.0673 - accuracy: 0.09 - ETA: 1s - loss: 0.0671 - accuracy: 0.09 - ETA: 1s - loss: 0.0669 - accuracy: 0.09 - ETA: 1s - loss: 0.0667 - accuracy: 0.09 - ETA: 1s - loss: 0.0664 - accuracy: 0.09 - ETA: 1s - loss: 0.0663 - accuracy: 0.09 - ETA: 1s - loss: 0.0661 - accuracy: 0.09 - ETA: 1s - loss: 0.0659 - accuracy: 0.09 - ETA: 1s - loss: 0.0658 - accuracy: 0.09 - ETA: 1s - loss: 0.0656 - accuracy: 0.09 - ETA: 1s - loss: 0.0654 - accuracy: 0.09 - ETA: 1s - loss: 0.0651 - accuracy: 0.09 - ETA: 1s - loss: 0.0649 - accuracy: 0.09 - ETA: 1s - loss: 0.0648 - accuracy: 0.09 - ETA: 1s - loss: 0.0646 - accuracy: 0.09 - ETA: 1s - loss: 0.0645 - accuracy: 0.09 - ETA: 1s - loss: 0.0643 - accuracy: 0.10 - ETA: 1s - loss: 0.0642 - accuracy: 0.09 - ETA: 1s - loss: 0.0641 - accuracy: 0.09 - ETA: 1s - loss: 0.0640 - accuracy: 0.09 - ETA: 1s - loss: 0.0638 - accuracy: 0.09 - ETA: 1s - loss: 0.0635 - accuracy: 0.09 - ETA: 1s - loss: 0.0634 - accuracy: 0.09 - ETA: 0s - loss: 0.0633 - accuracy: 0.09 - ETA: 0s - loss: 0.0632 - accuracy: 0.09 - ETA: 0s - loss: 0.0630 - accuracy: 0.09 - ETA: 0s - loss: 0.0628 - accuracy: 0.09 - ETA: 0s - loss: 0.0627 - accuracy: 0.09 - ETA: 0s - loss: 0.0625 - accuracy: 0.09 - ETA: 0s - loss: 0.0623 - accuracy: 0.09 - ETA: 0s - loss: 0.0621 - accuracy: 0.09 - ETA: 0s - loss: 0.0620 - accuracy: 0.09 - ETA: 0s - loss: 0.0619 - accuracy: 0.09 - ETA: 0s - loss: 0.0617 - accuracy: 0.10 - ETA: 0s - loss: 0.0615 - accuracy: 0.10 - ETA: 0s - loss: 0.0614 - accuracy: 0.10 - ETA: 0s - loss: 0.0612 - accuracy: 0.10 - ETA: 0s - loss: 0.0612 - accuracy: 0.10 - ETA: 0s - loss: 0.0611 - accuracy: 0.10 - ETA: 0s - loss: 0.0610 - accuracy: 0.10 - ETA: 0s - loss: 0.0609 - accuracy: 0.09 - ETA: 0s - loss: 0.0607 - accuracy: 0.10 - ETA: 0s - loss: 0.0606 - accuracy: 0.09 - ETA: 0s - loss: 0.0605 - accuracy: 0.09 - ETA: 0s - loss: 0.0603 - accuracy: 0.10 - 5s 2ms/step - loss: 0.0603 - accuracy: 0.1001 - val_loss: 0.0478 - val_accuracy: 0.0997\n",
      "Epoch 1/5\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 2.3769 - accuracy: 0.0000e+ - ETA: 3s - loss: 2.3544 - accuracy: 0.0000e+ - ETA: 4s - loss: 2.3467 - accuracy: 0.0000e+ - ETA: 4s - loss: 2.3360 - accuracy: 0.0000e+ - ETA: 4s - loss: 2.3248 - accuracy: 2.9762e- - ETA: 3s - loss: 2.3147 - accuracy: 6.9963e- - ETA: 3s - loss: 2.3038 - accuracy: 9.5274e- - ETA: 3s - loss: 2.2958 - accuracy: 0.0015   - ETA: 3s - loss: 2.2875 - accuracy: 0.00 - ETA: 3s - loss: 2.2795 - accuracy: 0.00 - ETA: 3s - loss: 2.2715 - accuracy: 0.00 - ETA: 3s - loss: 2.2642 - accuracy: 0.00 - ETA: 3s - loss: 2.2572 - accuracy: 0.00 - ETA: 3s - loss: 2.2495 - accuracy: 0.00 - ETA: 3s - loss: 2.2421 - accuracy: 0.00 - ETA: 3s - loss: 2.2350 - accuracy: 0.00 - ETA: 3s - loss: 2.2275 - accuracy: 0.00 - ETA: 3s - loss: 2.2199 - accuracy: 0.01 - ETA: 2s - loss: 2.2125 - accuracy: 0.01 - ETA: 2s - loss: 2.2055 - accuracy: 0.01 - ETA: 2s - loss: 2.1979 - accuracy: 0.01 - ETA: 2s - loss: 2.1902 - accuracy: 0.01 - ETA: 2s - loss: 2.1823 - accuracy: 0.02 - ETA: 2s - loss: 2.1747 - accuracy: 0.02 - ETA: 2s - loss: 2.1670 - accuracy: 0.02 - ETA: 2s - loss: 2.1592 - accuracy: 0.02 - ETA: 2s - loss: 2.1515 - accuracy: 0.03 - ETA: 2s - loss: 2.1435 - accuracy: 0.03 - ETA: 2s - loss: 2.1357 - accuracy: 0.03 - ETA: 2s - loss: 2.1278 - accuracy: 0.03 - ETA: 2s - loss: 2.1197 - accuracy: 0.03 - ETA: 2s - loss: 2.1109 - accuracy: 0.04 - ETA: 2s - loss: 2.1025 - accuracy: 0.04 - ETA: 2s - loss: 2.0937 - accuracy: 0.04 - ETA: 2s - loss: 2.0845 - accuracy: 0.04 - ETA: 2s - loss: 2.0759 - accuracy: 0.04 - ETA: 2s - loss: 2.0667 - accuracy: 0.04 - ETA: 1s - loss: 2.0575 - accuracy: 0.04 - ETA: 1s - loss: 2.0487 - accuracy: 0.05 - ETA: 1s - loss: 2.0395 - accuracy: 0.05 - ETA: 1s - loss: 2.0299 - accuracy: 0.05 - ETA: 1s - loss: 2.0202 - accuracy: 0.05 - ETA: 1s - loss: 2.0103 - accuracy: 0.05 - ETA: 1s - loss: 2.0005 - accuracy: 0.05 - ETA: 1s - loss: 1.9907 - accuracy: 0.05 - ETA: 1s - loss: 1.9810 - accuracy: 0.05 - ETA: 1s - loss: 1.9709 - accuracy: 0.06 - ETA: 1s - loss: 1.9607 - accuracy: 0.06 - ETA: 1s - loss: 1.9512 - accuracy: 0.06 - ETA: 1s - loss: 1.9421 - accuracy: 0.06 - ETA: 1s - loss: 1.9324 - accuracy: 0.06 - ETA: 1s - loss: 1.9218 - accuracy: 0.06 - ETA: 1s - loss: 1.9109 - accuracy: 0.06 - ETA: 1s - loss: 1.8999 - accuracy: 0.06 - ETA: 1s - loss: 1.8898 - accuracy: 0.06 - ETA: 1s - loss: 1.8795 - accuracy: 0.06 - ETA: 0s - loss: 1.8686 - accuracy: 0.06 - ETA: 0s - loss: 1.8580 - accuracy: 0.06 - ETA: 0s - loss: 1.8471 - accuracy: 0.06 - ETA: 0s - loss: 1.8366 - accuracy: 0.06 - ETA: 0s - loss: 1.8267 - accuracy: 0.07 - ETA: 0s - loss: 1.8159 - accuracy: 0.07 - ETA: 0s - loss: 1.8048 - accuracy: 0.07 - ETA: 0s - loss: 1.7936 - accuracy: 0.07 - ETA: 0s - loss: 1.7826 - accuracy: 0.07 - ETA: 0s - loss: 1.7714 - accuracy: 0.07 - ETA: 0s - loss: 1.7604 - accuracy: 0.07 - ETA: 0s - loss: 1.7492 - accuracy: 0.07 - ETA: 0s - loss: 1.7385 - accuracy: 0.07 - ETA: 0s - loss: 1.7275 - accuracy: 0.07 - ETA: 0s - loss: 1.7165 - accuracy: 0.07 - ETA: 0s - loss: 1.7057 - accuracy: 0.07 - ETA: 0s - loss: 1.6945 - accuracy: 0.07 - ETA: 0s - loss: 1.6833 - accuracy: 0.07 - ETA: 0s - loss: 1.6727 - accuracy: 0.07 - ETA: 0s - loss: 1.6618 - accuracy: 0.07 - 5s 2ms/step - loss: 1.6611 - accuracy: 0.0762 - val_loss: 0.8485 - val_accuracy: 0.0997\n",
      "Epoch 2/5\n",
      "2173/2188 [============================>.] - ETA: 0s - loss: 0.8403 - accuracy: 0.03 - ETA: 3s - loss: 0.8321 - accuracy: 0.08 - ETA: 3s - loss: 0.8299 - accuracy: 0.08 - ETA: 3s - loss: 0.8196 - accuracy: 0.09 - ETA: 3s - loss: 0.8107 - accuracy: 0.09 - ETA: 3s - loss: 0.8028 - accuracy: 0.09 - ETA: 3s - loss: 0.7939 - accuracy: 0.09 - ETA: 3s - loss: 0.7843 - accuracy: 0.09 - ETA: 3s - loss: 0.7781 - accuracy: 0.09 - ETA: 3s - loss: 0.7690 - accuracy: 0.09 - ETA: 3s - loss: 0.7613 - accuracy: 0.09 - ETA: 3s - loss: 0.7522 - accuracy: 0.09 - ETA: 3s - loss: 0.7446 - accuracy: 0.09 - ETA: 3s - loss: 0.7361 - accuracy: 0.09 - ETA: 3s - loss: 0.7275 - accuracy: 0.09 - ETA: 2s - loss: 0.7204 - accuracy: 0.09 - ETA: 2s - loss: 0.7129 - accuracy: 0.09 - ETA: 2s - loss: 0.7061 - accuracy: 0.10 - ETA: 2s - loss: 0.6988 - accuracy: 0.10 - ETA: 2s - loss: 0.6913 - accuracy: 0.10 - ETA: 2s - loss: 0.6846 - accuracy: 0.10 - ETA: 2s - loss: 0.6790 - accuracy: 0.10 - ETA: 2s - loss: 0.6731 - accuracy: 0.10 - ETA: 2s - loss: 0.6672 - accuracy: 0.10 - ETA: 2s - loss: 0.6611 - accuracy: 0.10 - ETA: 2s - loss: 0.6565 - accuracy: 0.10 - ETA: 2s - loss: 0.6506 - accuracy: 0.10 - ETA: 2s - loss: 0.6451 - accuracy: 0.10 - ETA: 2s - loss: 0.6389 - accuracy: 0.10 - ETA: 2s - loss: 0.6330 - accuracy: 0.10 - ETA: 2s - loss: 0.6275 - accuracy: 0.10 - ETA: 2s - loss: 0.6223 - accuracy: 0.10 - ETA: 2s - loss: 0.6172 - accuracy: 0.10 - ETA: 2s - loss: 0.6122 - accuracy: 0.10 - ETA: 2s - loss: 0.6063 - accuracy: 0.10 - ETA: 2s - loss: 0.6011 - accuracy: 0.10 - ETA: 2s - loss: 0.5951 - accuracy: 0.10 - ETA: 2s - loss: 0.5905 - accuracy: 0.10 - ETA: 2s - loss: 0.5862 - accuracy: 0.09 - ETA: 2s - loss: 0.5814 - accuracy: 0.10 - ETA: 2s - loss: 0.5766 - accuracy: 0.10 - ETA: 2s - loss: 0.5720 - accuracy: 0.10 - ETA: 1s - loss: 0.5675 - accuracy: 0.10 - ETA: 1s - loss: 0.5627 - accuracy: 0.09 - ETA: 1s - loss: 0.5584 - accuracy: 0.09 - ETA: 1s - loss: 0.5539 - accuracy: 0.09 - ETA: 1s - loss: 0.5495 - accuracy: 0.09 - ETA: 1s - loss: 0.5452 - accuracy: 0.09 - ETA: 1s - loss: 0.5409 - accuracy: 0.09 - ETA: 1s - loss: 0.5365 - accuracy: 0.09 - ETA: 1s - loss: 0.5324 - accuracy: 0.09 - ETA: 1s - loss: 0.5285 - accuracy: 0.09 - ETA: 1s - loss: 0.5243 - accuracy: 0.09 - ETA: 1s - loss: 0.5206 - accuracy: 0.10 - ETA: 1s - loss: 0.5168 - accuracy: 0.10 - ETA: 1s - loss: 0.5130 - accuracy: 0.10 - ETA: 1s - loss: 0.5090 - accuracy: 0.10 - ETA: 1s - loss: 0.5051 - accuracy: 0.10 - ETA: 1s - loss: 0.5015 - accuracy: 0.10 - ETA: 1s - loss: 0.4980 - accuracy: 0.10 - ETA: 1s - loss: 0.4945 - accuracy: 0.10 - ETA: 1s - loss: 0.4909 - accuracy: 0.10 - ETA: 1s - loss: 0.4873 - accuracy: 0.10 - ETA: 0s - loss: 0.4841 - accuracy: 0.10 - ETA: 0s - loss: 0.4807 - accuracy: 0.10 - ETA: 0s - loss: 0.4775 - accuracy: 0.10 - ETA: 0s - loss: 0.4743 - accuracy: 0.10 - ETA: 0s - loss: 0.4710 - accuracy: 0.10 - ETA: 0s - loss: 0.4677 - accuracy: 0.10 - ETA: 0s - loss: 0.4644 - accuracy: 0.10 - ETA: 0s - loss: 0.4610 - accuracy: 0.10 - ETA: 0s - loss: 0.4577 - accuracy: 0.10 - ETA: 0s - loss: 0.4545 - accuracy: 0.10 - ETA: 0s - loss: 0.4512 - accuracy: 0.10 - ETA: 0s - loss: 0.4480 - accuracy: 0.10 - ETA: 0s - loss: 0.4448 - accuracy: 0.10 - ETA: 0s - loss: 0.4419 - accuracy: 0.10 - ETA: 0s - loss: 0.4388 - accuracy: 0.10 - ETA: 0s - loss: 0.4359 - accuracy: 0.09 - ETA: 0s - loss: 0.4330 - accuracy: 0.09 - ETA: 0s - loss: 0.4301 - accuracy: 0.09 - ETA: 0s - loss: 0.4273 - accuracy: 0.10 - ETA: 0s - loss: 0.4247 - accuracy: 0.1000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-107-b0bcaccd8844>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m tuner.search(X_train, y_train,\n\u001b[0;32m      2\u001b[0m              \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m              validation_data=(X_test, y_test))\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\kerastuner\\engine\\base_tuner.py\u001b[0m in \u001b[0;36msearch\u001b[1;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_trial_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_trial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_trial_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_search_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\kerastuner\\engine\\multi_execution_tuner.py\u001b[0m in \u001b[0;36mrun_trial\u001b[1;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m             \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcopied_fit_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_values\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobjective\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirection\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'min'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1131\u001b[0m               \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m               \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1133\u001b[1;33m               return_dict=True)\n\u001b[0m\u001b[0;32m   1134\u001b[0m           \u001b[0mval_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'val_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mval\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mval_logs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[0;32m   1377\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'TraceContext'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1378\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1379\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1380\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1381\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    812\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 814\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    815\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32m~\\anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[0;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1924\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tuner.search(X_train, y_train,\n",
    "             epochs=5,\n",
    "             validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LKbr1gRg9BXs"
   },
   "source": [
    "### Stretch Goals\n",
    "- Implement Bayesian Hyper-parameter Optimization\n",
    "- Select a new dataset and apply a neural network to it.\n",
    "- Use a cloud base experiment tracking framework such as weights and biases\n",
    "- Research potential architecture ideas for this problem. Try Lenet-10 for example. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_433_Tune_Assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "U4-S1-NLP (Python3)",
   "language": "python",
   "name": "u4-s1-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "nteract": {
   "version": "0.22.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
